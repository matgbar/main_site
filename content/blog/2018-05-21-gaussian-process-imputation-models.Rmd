---
title: "Gaussian Process Imputation/Forecast Models"
author: "Matthew Barstead, Ph.D."
date: '2018-05-21'
baseurl: https://mgb-research.netlify.com/
slug: gaussian-process-imputation-models
categories: Modeling
tags:
- Bayesian
- Stan
- R
- Gaussian Process
- Forecasting
- Predictive Modeling
- R-bloggers
banner: img/banners/gp-forecast-banner.png
---

```{r, echo=F, message=F}
library(bayesplot)
library(rstan)
library(tidybayes)
library(tidyverse)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

When would it not be good to have a way to forecast the future? Honestly, I lack the imagination to provide an answer, though I'll admit there are likely edge cases I am not considering. But on average, all things being equal, it is better to know than to not know what is coming. For companies, the ability to accurately forecast events _is_ the business model. 

The simplest forecast model is one in which we use trends over time to predict future events, the assumption being that the previous trend will continue essentially as is through the forecast window. That seem like a simple forecast model, but the trick is what a given model represents as the "trend." For some data, a linear model is going to make perfect sense: 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(19571004)
x <- 0:24
b <- 2.5
y <-  2 + b * x + rnorm(length(x), sd = 4) 

ggplot(data = data.frame(x=x, y=y), 
       aes(x=x, y=y)) +
  geom_point(color = '#426EBD') +
  geom_line(color = '#426EBD') +
  stat_smooth(geom = 'line', method = 'lm', alpha = .5, lwd = 1, lty= 'dashed') +
  stat_smooth(geom = 'ribbon', method = 'lm', alpha = .15, lty= 'dashed') +
  labs(x = 'Time', y = "Revenue") +
  theme_bw() +
  theme(axis.ticks = element_blank(), 
        axis.text = element_blank())

```

For others, such a simple model is not going to be very effective.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data("AirPassengers")
air_pass_tbl <- tibble(
  pass = as.numeric(AirPassengers), 
  year = seq(1949, 1960+11/12, by=1/12)
)

ggplot(data = air_pass_tbl, 
       aes(x=year, y=pass)) +
  geom_point(color = '#426EBD') +
  geom_line(color = '#426EBD') +
  stat_smooth(geom = 'line', method = 'lm', alpha = .5, lwd = 1, lty= 'dashed') +
  stat_smooth(geom = 'ribbon', method = 'lm', alpha = .15, lty= 'dashed') +
  labs(x = 'Year', y = "Airline Passengers (in thousands)") +
  theme_bw()
```

The linear model here alone is a not a particularly good fit, though there is clearly an average change over time. However, equally clear is that there is a seasonal pattern that is increasing in its magnitude over time. The linear model as defined in the plot above does not know how to handle such a pattern. 

Let's say the prediction challenge here is to forecast airline passengers in 1960 using only airline passenger data from the preceding years. We know a simple linear change over time approach is not going to work or at least is probably not our best model - though it is better than no model since it would at least capture the upward trend. Since our only data here is a uni-dimensional time-series the question is how can we effectively model associations between monthly airline passengers across time. 

First, before we get started, let's split the data up into the training and test sets. 

```{r}
air_pass_train <- air_pass_tbl %>% 
  filter(year < 1960)

air_pass_test <- air_pass_tbl %>% 
  filter(year >= 1960)
```

I at least three, maybe even four properties of the raw data that I would like to capture in my model: 

1. Over time there is an average increase in the number of passengers.
1. There is a seasonal trend on top of the average increase that appears to be annual in nature, which would make sense for the airline industry in then (and probably now - COVID crisis notwithstanding). 
1. The effect of the seasonal trends is more pronounced over time
1. There _may_, and I stress _may_ be a slight acceleration in the, long-term increase in monthly passengers

So can I create a model that includes somehow accounts for all of these different features of the timeseries? Let's find out. I tend to like to start building simple models and move up from there. We'll start by simply creating a model based on the idea that time points closer together will have airline passenger values that are more similar than those that are farther apart? How close? How much of an effect? I'll use the squared exponential covariance kernel to help me derive these answers from the data. 

$$k(t,t') = \sigma^2 exp\Big(-\frac{(t-t')^2}{2l_1^2}\Big)$$

where $\sigma^2$ is the estimated variance accounted for by the function $k$ and $l$ is a length scale parameter that governs the decay rate. Smaller estimated values for $l$ indicate a faster decay rate in the covariance between two points as a function of time. 

Let's get the data ready: 
```{r}
m1_data <- list(
  N1 = nrow(air_pass_train), 
  N2 = nrow(air_pass_test), 
  X = air_pass_train[['year']], 
  Y = air_pass_train[['pass']], 
  Xp = air_pass_test[['year']]
)
```

The model variables are: 
 - `N1`: the number of observations in the prediction set
 - `N2`: the number of observation in the test set
 - `X`: evenly spaced interval - in this case time as measured in months
 - `Y`: passengers in a given month
 - `Xp`: the prediction window (which set of months/which time window to generate predicted passenger rates)

Now for the `Stan` code: 

```{stan, eval=FALSE, output.var='none'}
functions{
	//covariance function for main portion of the model
	matrix main_GP(
		int Nx,
		vector x,
		int Ny,
		vector y, 
		real alpha1,
		real rho1){
					matrix[Nx, Ny] Sigma;
	
					//specifying random Gaussian process that governs covariance matrix
					for(i in 1:Nx){
						for (j in 1:Ny){
							Sigma[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
						}
					}
					
					return Sigma;
				}
	//function for posterior calculations
	vector post_pred_rng(
		real a1,
		real r1, 
		real sn,
		int No,
		vector xo,
		int Np, 
		vector xp,
		vector yobs){
				matrix[No,No] Ko;
				matrix[Np,Np] Kp;
				matrix[No,Np] Kop;
				matrix[Np,No] Ko_inv_t;
				vector[Np] mu_p;
				matrix[Np,Np] Tau;
				matrix[Np,Np] L2;
				vector[Np] yp;
	
	//--------------------------------------------------------------------
	//Kernel Multiple GPs for observed data
	Ko = main_GP(No, xo, No, xo, a1, r1);
	for(n in 1:No) Ko[n,n] += sn;
		
	//--------------------------------------------------------------------
	//kernel for predicted data
	Kp = main_GP(Np, xp, Np, xp,  a1, r1);
	for(n in 1:Np) Kp[n,n] += sn;
		
	//--------------------------------------------------------------------
	//kernel for observed and predicted cross 
	Kop = main_GP(No, xo, Np, xp,  a1, r1);
	
	//--------------------------------------------------------------------
	//Algorithm 2.1 of Rassmussen and Williams 
	Ko_inv_t = Kop'/Ko;
	mu_p = Ko_inv_t*yobs;
	Tau=Kp-Ko_inv_t*Kop;
	L2 = cholesky_decompose(Tau);
	yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
	return yp;
	}
}

data { 
	int<lower=1> N1;
	int<lower=1> N2;
	vector[N1] X; 
	vector[N1] Y;
	vector[N2] Xp;
}

transformed data { 
	vector[N1] mu;
	for(n in 1:N1) mu[n] = 0;
}

parameters {
  // a1, r1, and sigma_sq cannot be negative 
	real<lower=0> a1;
	real<lower=0> r1;
	real<lower=0> sigma_sq;
}

model{ 
	matrix[N1,N1] Sigma;
	matrix[N1,N1] L_S;
	
	//using GP function from above 
	Sigma = main_GP(N1, X, N1, X,  a1, r1);
	for(n in 1:N1) Sigma[n,n] += sigma_sq;
	
	L_S = cholesky_decompose(Sigma);
	Y ~ multi_normal_cholesky(mu, L_S);
	
	// priors for parameters - effecitvely the upper half of the students t-distribution
	a1 ~ student_t(3,0,1);
	r1 ~ student_t(3,0,1);
	sigma_sq ~ student_t(3,0,1);
}

generated quantities {
	vector[N2] Ypred = post_pred_rng(a1, r1, sigma_sq, N1, X, N2, Xp, Y);
}
```

And now for the actual model. 

```{r, message=FALSE, warning=FALSE}
# Easier to note these upfront
m1_pars.to.monitor<-c('a1','r1','sigma_sq', 'Ypred')

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m1 <- stan(
  file = '../../blog_code/gp_m1.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m1_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

The model will converge, and at least on my machine with that seed there is some evidence that it had a hard time getting started. The posterior distributions seem pretty good - for the most part there is evidence of good mixing of the chains. Note that I tend to use a higher number of chains for initial analyses to help diagnose convergence issues. 

```{r}
traceplot(m1, pars=c('a1', 'r1', 'sigma_sq'))
```

Predicted values are about as good as we saw with the linear regression model. 

```{r, message=FALSE, warning=FALSE}
g_m1 <-
brms::posterior_samples(m1, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Posterior forecasts for single squared exponential covariance GP') +
    scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m1
```

Before adding another function, we should consider the fact that the distribution we care most about modeling (number of monthly passengers) is not likely normally distributed. Currently the model makes the assumption that it is, which may have contributed to some of the initial warning messages the `Stan` model returned. There are a few options, but I think a simple log-transform should work here. A little tweak of the code adding to the transformed data block: 

```{stan, eval=FALSE, output.var='none'}
transformed data { 
	vector[N1] mu;
	vector[N1] y_log;
	for(n in 1:N1){
	  mu[n] = 0;
	  y_log[n] = log(Y[n]);
	} 
}
```


```{r, message=FALSE, warning=FALSE}
# Easier to note these upfront
m2_pars.to.monitor <- m1_pars.to.monitor

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m2 <- stan(
  file = '../../blog_code/gp_m2.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m2_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

```{r}
traceplot(m2, pars=c('a1', 'r1', 'sigma_sq'))
```

```{r}
g_m2 <- 
brms::posterior_samples(m2, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Posterior forecasts for single squared exponential covariance GP with log-transformed inputs') +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

cowplot::plot_grid(
  g_m1 + 
    scale_y_continuous(breaks = c(200, 400, 600, 800)) + 
    coord_cartesian(ylim = c(195, 805)), 
  g_m2 + 
    scale_y_continuous(breaks = c(200, 400, 600, 800)) + 
    coord_cartesian(ylim = c(195, 805)), 
  labels = paste0('m', 1:2), 
  nrow = 2
)
```

Despite the greater degree of uncertainty, visual inspection of the plots leads me to think that the log-transformed approach is a little better. The lack of warning messages is also a plus. Neither of these models is particularly good. Which is why we are going to add another GP that accounts for seasonality in the time series. 

$$k(t,t')=\sigma_2^2 exp\Big(-\frac{2sin^2(\pi(t-t')*1)}{l_2^2}\Big) exp\Big(-\frac{(t-t')^2}{2l_3^2}\Big)$$
LESS HANDY-WAVY more details.... 

```{r, message=FALSE, warning=FALSE}
# Easier to note these upfront
m3_pars.to.monitor <- c(
  paste0('r', 1:3), 
  paste0('a', 1:2), 
  'sigma_sq', 
  'Ypred'
)

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m3 <- stan(
  file = '../../blog_code/gp_m3.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m3_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

```{r}
g_m3 <- 
brms::posterior_samples(m3, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Posterior forecasts for single squared exponential covariance GP with log-transformed inputs') +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m3
```

Well that is a huge improvement over the first few models. Adding in a term that addresses the obvious seasonal pattern improved model predictions immensely. My only gripe at this point is the fact that the "mean" posterior estimate tends to undershoot the observed data on average. It could just be the restricted range and the model would over predict the next few months.

```{r}
# Easier to note these upfront
m4_pars.to.monitor <- c(
  paste0('r', 1:5), 
  paste0('a', 1:3), 
  'sigma_sq', 
  'Ypred'
)

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m4 <- stan(
  file = '../../blog_code/gp_m4.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m4_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

```{r}
g_m4.1 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = str_wrap('Forecasted using a squared exponential covariance kernel, and two additional kernels, one that addressed the seasonal component in the observed time series and one that I have no idea why it works but it sort of made intuitive sense, and now I need to back track the math to make sure I am not doing something bonkers here. The fit is better though - noticeably so. And, the uncertainty intervals behave in a more satisfying manner: the farther in the future from the most recent data point used to train the parameters, the more uncertainty the model should have about its predictions.')) +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m4.1
```

Model 4 predictions extended from the observed data: 

```{r}
g_m4.2 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_train, aes(x = year, y = pass)) +
  geom_line(data = air_pass_train, aes(x = year, y = pass)) +
  labs(x = "Year", y = "Passengers (in thousands)", 
       caption = str_wrap('Model 4 - forecast: Three Gaussian process models combined')) +
  theme_bw()

g_m4.2
```

```{r}
(arima_fit<- arima(log(AirPassengers[1:132]), c(0, 1, 1),
              seasonal = list(order = c(0, 1, 1), period = 12)))
update(arima_fit, method = "CSS")

pred <- predict(arima_fit, n.ahead = 12)
tl <- pred$pred - 1.96 * pred$se
tu <- pred$pred + 1.96 * pred$se

arima_forecast <-data.frame(
  year=air_pass_test[['year']], 
  pass=exp(pred$pred), 
  UB=exp(as.numeric(tu)), 
  LB=exp(as.numeric(tl))
)

g_arima_m4 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .25) +
  geom_line(
    data = arima_forecast, 
    aes(x=year, y=pass), 
    color = RColorBrewer::brewer.pal(9, 'Reds')[7], 
    alpha = .25
    ) +
  geom_ribbon(
    data = arima_forecast, 
    aes(ymax=UB, ymin=LB), 
    fill = RColorBrewer::brewer.pal(9, 'Reds')[3], 
    alpha = .25
    ) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = str_wrap('Overlay of observed time series, ARIMA forecast (red) and, GP forecast (blue)')) +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_arima_m4
```
Gaussian process models offer a flexible framework for detecting and stacking latent change processes and can be turned into forecast models on out-of-sample observations. The secret is that the sum of any set of Gaussian processes is itself a Gaussian process: 

-AND MOST SENSIBLE FORMULA HERE. 

A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal's mean and/or variance changes over time). A common approach is to impose some stationarity on the data  (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.

The models below will make use of the preloaded `AirPassengers` data in R. The data represent the total number of monthly international airline passengers (in thousands) from 1949 to 1960. It is easy to see these data have both a non-stationary mean and a non-stationary variance. There is also a clear periodic component to these data. 