---
title: "Gaussian Process Imputation/Forecast Models"
author: "Matthew Barstead, Ph.D."
date: '2018-05-21'
baseurl: https://mgb-research.netlify.com/
slug: gaussian-process-imputation-models
categories: Modeling
tags:
- Bayesian
- Stan
- R
- Gaussian Process
- Forecasting
- Predictive Modeling
- R-bloggers
banner: img/banners/gp-forecast-banner.png
---

```{r, echo=F, message=F}
library(bayesplot)
library(rstan)
library(tidybayes)
library(tidyverse)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
load('../../DFs/gp_forecast_blog.RData')
```

(Updated: `r Sys.Date()`)

When would it not be good to have a way to forecast the future? Honestly, I lack the imagination to provide an answer, though I'll admit there are likely edge cases I am not considering. But on average, all things being equal, it is better to know than to not know what is coming. For companies, the ability to accurately forecast events _is_ the business model. 

The simplest forecast model is one in which we use trends over time to predict future events, the assumption being that the previous trend will continue essentially as is through the forecast window. That seem like a simple forecast model, but the trick is what a given model represents as the "trend." For some data, a linear model is going to make perfect sense: 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(19571004)
x <- 0:24
b <- 2.5
y <-  2 + b * x + rnorm(length(x), sd = 4) 

ggplot(data = data.frame(x=x, y=y), 
       aes(x=x, y=y)) +
  geom_point(color = '#426EBD') +
  geom_line(color = '#426EBD') +
  stat_smooth(geom = 'line', method = 'lm', alpha = .5, lwd = 1, lty= 'dashed') +
  stat_smooth(geom = 'ribbon', method = 'lm', alpha = .15, lty= 'dashed') +
  labs(x = 'Time', y = "Revenue") +
  theme_bw() +
  theme(axis.ticks = element_blank(), 
        axis.text = element_blank())

```

For others, such a simple model is not going to be very effective.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data("AirPassengers")
air_pass_tbl <- tibble(
  pass = as.numeric(AirPassengers), 
  year = seq(1949, 1960+11/12, by=1/12)
)

ggplot(data = air_pass_tbl, 
       aes(x=year, y=pass)) +
  geom_point(color = '#426EBD') +
  geom_line(color = '#426EBD') +
  stat_smooth(geom = 'line', method = 'lm', alpha = .5, lwd = 1, lty= 'dashed') +
  stat_smooth(geom = 'ribbon', method = 'lm', alpha = .15, lty= 'dashed') +
  labs(x = 'Year', y = "Airline Passengers (in thousands)") +
  theme_bw()
```

The linear model here alone is a not a particularly good fit, though there is clearly an average change over time. However, equally clear is that there is a seasonal pattern that is increasing in its magnitude over time. The linear model as defined in the plot above does not know how to handle such a pattern. 

Let's say the prediction challenge here is to forecast airline passengers in 1960 using only airline passenger data from the preceding years. We know a simple linear change over time approach is not going to work or at least is probably not our best model - though it is better than no model since it would at least capture the upward trend. Our only data here is a uni-dimensional time-series which means that our task is to effectively model associations between monthly airline passengers across time, using only what we can extract from existing patterns. 

In an ideal world, we would want to take into account other properties that would affect air travel in a forecast model. Sometimes, the data are what they are and the single time series is all we have to work from. One thing I like about a Gaussian process approach is that, using fairly well-established covariance functions, we can take a layered approach. My general approach to model building is to move from the simple to complex. Because the sum of several Gaussian processes is itself a Gaussian process, we can simply add or remove any covariance functions that do not seem to useful in improving our forecast. 

This is a hands-on approach. The model will learn the appropriate hyperparameters for the covariance function(s) from the data - provided the model is appropriate. So that part feels a little more like a machine learning model. However, which covariance functions to include, whether transformations of the data are beneficial, and how to define the covariance functions takes some trial an error. 

So let's dig into that trial and error phase. First, before we get started with any modeling, let's split the data up into the training and test sets. 

```{r, eval=FALSE}
air_pass_train <- air_pass_tbl %>% 
  filter(year < 1960)

air_pass_test <- air_pass_tbl %>% 
  filter(year >= 1960)
```

In viewing the data I see at least three, maybe even four properties of the time series that I would like to capture in my model: 

1. Over time there is an average increase in the number of passengers.
1. There is a seasonal trend on top of the average increase that appears to be annual in nature, which would make sense for the airline industry in then (and probably now - COVID crisis notwithstanding). 
1. The effect of the seasonal trends is more pronounced over time
1. There _may_, and I stress _may_ be a slight acceleration in the, long-term increase in monthly passengers

So can I create a model that somehow accounts for all of these different features of the times series? To find out we start with the simplest model - one that can address the first point. This first model will use the squared exponential covariance function (below) to bake in the idea that time points closer together will have airline passenger values that are more similar than those that are farther apart. How close? How much of an effect of time will there be on the decay in covariance? That is where the modeling comes in. We use the data to tune the hyperparameters of the squared exponential covariance function. 

$$k(t,t') = \sigma^2 exp\Big(-\frac{(t-t')^2}{2l_1^2}\Big)$$

where $\sigma^2$ is the estimated variance accounted for by the function $k$ and $l$ is a length scale parameter that governs the decay rate. So here $\sigma^2$ governs how much variance the function accounts for in the observed data and $l$ is our parameter most closely connected with the decay in the covariance between two points as a function of time. 

Let's get the data ready: 
```{r, eval=FALSE}
m1_data <- list(
  N1 = nrow(air_pass_train), 
  N2 = nrow(air_pass_test), 
  X = air_pass_train[['year']], 
  Y = air_pass_train[['pass']], 
  Xp = air_pass_test[['year']]
)
```

The model variables are: 
 - `N1`: the number of observations in the prediction set
 - `N2`: the number of observation in the test set
 - `X`: evenly spaced interval - in this case time as measured in months
 - `Y`: passengers in a given month
 - `Xp`: the prediction window (which set of months/which time window to generate predicted passenger rates)

Now for the `Stan` code that translates this. I was inspired and borrowed from Nate Lemoine's post on [Gaussian processes](https://natelemoine.com/even-faster-gaussian-processes-in-stan/) when setting this model up. I highly recommend if readers are interested in gain additional exposure to this modeling approach. 

```{stan, eval=FALSE, output.var='none'}
functions{
	//covariance function for main portion of the model
	matrix main_GP(
		int Nx,
		vector x,
		int Ny,
		vector y, 
		real alpha1,
		real rho1){
					matrix[Nx, Ny] Sigma;
	
					//specifying random Gaussian process that governs covariance matrix
					for(i in 1:Nx){
						for (j in 1:Ny){
							Sigma[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
						}
					}
					
					return Sigma;
				}
	//function for posterior calculations
	vector post_pred_rng(
		real a1,
		real r1, 
		real sn,
		int No,
		vector xo,
		int Np, 
		vector xp,
		vector yobs){
				matrix[No,No] Ko;
				matrix[Np,Np] Kp;
				matrix[No,Np] Kop;
				matrix[Np,No] Ko_inv_t;
				vector[Np] mu_p;
				matrix[Np,Np] Tau;
				matrix[Np,Np] L2;
				vector[Np] yp;
	
	//--------------------------------------------------------------------
	//Kernel Multiple GPs for observed data
	Ko = main_GP(No, xo, No, xo, a1, r1);
	for(n in 1:No) Ko[n,n] += sn;
		
	//--------------------------------------------------------------------
	//kernel for predicted data
	Kp = main_GP(Np, xp, Np, xp,  a1, r1);
	for(n in 1:Np) Kp[n,n] += sn;
		
	//--------------------------------------------------------------------
	//kernel for observed and predicted cross 
	Kop = main_GP(No, xo, Np, xp,  a1, r1);
	
	//--------------------------------------------------------------------
	//Algorithm 2.1 of Rassmussen and Williams 
	Ko_inv_t = Kop'/Ko;
	mu_p = Ko_inv_t*yobs;
	Tau=Kp-Ko_inv_t*Kop;
	L2 = cholesky_decompose(Tau);
	yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
	return yp;
	}
}

data { 
	int<lower=1> N1;
	int<lower=1> N2;
	vector[N1] X; 
	vector[N1] Y;
	vector[N2] Xp;
}

transformed data { 
	vector[N1] mu;
	for(n in 1:N1) mu[n] = 0;
}

parameters {
  // a1, r1, and sigma_sq cannot be negative 
	real<lower=0> a1;
	real<lower=0> r1;
	real<lower=0> sigma_sq;
}

model{ 
	matrix[N1,N1] Sigma;
	matrix[N1,N1] L_S;
	
	//using GP function from above 
	Sigma = main_GP(N1, X, N1, X,  a1, r1);
	for(n in 1:N1) Sigma[n,n] += sigma_sq;
	
	L_S = cholesky_decompose(Sigma);
	Y ~ multi_normal_cholesky(mu, L_S);
	
	// priors for parameters - effecitvely the upper half of the students t-distribution
	a1 ~ student_t(3,0,1);
	r1 ~ student_t(3,0,1);
	sigma_sq ~ student_t(3,0,1);
}

generated quantities {
	vector[N2] Ypred = post_pred_rng(a1, r1, sigma_sq, N1, X, N2, Xp, Y);
}
```

And now for the actual model. 

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Easier to note these upfront
m1_pars.to.monitor<-c('a1','r1','sigma_sq', 'Ypred')

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m1 <- stan(
  file = '../../blog_code/gp_m1.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m1_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

The model will converge, though there is some evidence that not all is well. Still, the traceplots below suggest the distributions of the model parameters converged reasonably well (a few spikes I don't love). I often use more chains when modeling building to help with diagnosis problems if they arise. Three chains should be sufficient for most models. 

```{r}
traceplot(m1, pars=c('a1', 'r1', 'sigma_sq'))
```

Predicted values are about as good as we saw with the linear regression model, and the credibility envelope misses the two highest values in 1960 altogether. 

```{r, message=FALSE, warning=FALSE}
g_m1 <-
brms::posterior_samples(m1, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Model 1. Single Gaussian process model on raw passenger totals.') +
    scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m1
```

Before adding another function, we should consider the fact that the distribution we care most about modeling (number of monthly passengers) is not likely normally distributed. Currently the model makes the assumption that it is, which may have contributed to some of the initial warning messages the `Stan` model returned. Semi-related is that the mean and variance of the time-series are shifting dramatically over the period examined here. 

There is more than one way to address the issues of changing scale/variance and non-normality. In this case, I am going to apply a simple log-transform on the airline passenger values. We can do this in the Stan code itself: 

```{stan, eval=FALSE, output.var='none'}
transformed data { 
	vector[N1] mu;
	vector[N1] y_log;
	for(n in 1:N1){
	  mu[n] = 0;
	  y_log[n] = log(Y[n]);
	} 
}
```

With the transform added we can re-run our model with the single sqaured exponential covariance function. 

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Model is the same in terms of parameters - just transformed the variable to log space
m2_pars.to.monitor <- m1_pars.to.monitor

m2 <- stan(
  file = '../../blog_code/gp_m2.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m2_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

There are no warnings, which is always a welcome sign, and the model seems to converge without much issue. (I am moving quickly through these modeling steps. You would definitely want to perform more posterior checks than what I am doing here.)

```{r, echo=FALSE}
traceplot(m2, pars=c('a1', 'r1', 'sigma_sq'))
```

And now for our model comparison: raw vs. log-transformed.

```{r, echo=FALSE}
g_m2 <- 
brms::posterior_samples(m2, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Model 2. A single Gaussian process on the log-transformed passenger totals.') +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

cowplot::plot_grid(
  g_m1 + 
    scale_y_continuous(breaks = c(200, 400, 600, 800)) + 
    coord_cartesian(ylim = c(195, 805)), 
  g_m2 + 
    scale_y_continuous(breaks = c(200, 400, 600, 800)) + 
    coord_cartesian(ylim = c(195, 805)), 
  nrow = 2
)
```

Despite the greater degree of uncertainty, visual inspection of the plots leads me to think that the log-transformed approach is a little better. The lack of warning messages is also a plus. Neither of these models is particularly good, and that is why we are going to add another GP that accounts for seasonality in the time series. Hopefully that can give us some curvier forecasts.

$$k(t,t')=\sigma_2^2 exp\Big(-\frac{2sin^2(\pi(t-t')*1)}{l_2^2}\Big) exp\Big(-\frac{(t-t')^2}{2l_3^2}\Big)$$

Here is what that looks like when added to the `main_GP` function in our evolving Stan code. Note that you'll also need to add the `rho` and `alpha` parameters in the appropriate locations. Raw code is available here. 

```{stan, eval=FALSE, output.var='none'}
for(i in 1:Nx){
	for(j in 1:Ny){
		K2[i, j] = alpha2*exp(-2*square(sin(pi()*fabs(x[i]-y[j])*1))/square(rho2))*
		exp(-square(x[i]-y[j])/2/square(rho3));
	}
}
```

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Now more parameters to monitor
m3_pars.to.monitor <- c(
  paste0('r', 1:3), 
  paste0('a', 1:2), 
  'sigma_sq', 
  'Ypred'
)

m3 <- stan(
  file = '../../blog_code/gp_m3.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m3_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

```{r, echo=FALSE}
g_m3 <- 
brms::posterior_samples(m3, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Model 3. The sum of two Gaussian process models.') +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m3
```

Well that is a huge improvement over the first few models. Adding in a term that addresses the obvious seasonal pattern improved model predictions immensely. My only gripe at this point is the fact that the "mean" posterior estimate tends to undershoot the observed data on average. It could just be the restricted range and the model would "over-predict" the next few months.

Returning to the initial list let us see what is still missing from the model

1. Over time there is an average increase in the number of passengers.

The linear trend seems to be effectively captured by the first Gaussian process in the model - the squared exponential covariance kernel. An oversimplified description of that particular Gaussian process is that: _as the time between two points increases, the expected covariance in values decreases_. 

1. There is a seasonal trend on top of the average increase that appears to be annual in nature, which would make sense for the airline industry in then (and probably now - COVID crisis notwithstanding).

This was addressed in `m3` when we added a periodic kernel - notably one that has its own squared exponential covariance term attached to it. The concept here is that we are stating that there is some sort of seasonal trend and we can can model the periodicity as a function of the covariance matrix using the formula above _and_ that periodic function may change over time. Essentially, the squared exponential covariance term added on the end of the periodic kernel makes it locally periodic. 

1. The effect of the seasonal trends is more pronounced over time. 
Addressed in two ways. The first is through the log-transform. The second is through the local nature of the periodic kernel. 

1. There _may_, and I stress _may_ be a slight acceleration in the, long-term increase in monthly passengers. 
One way to think about this is that the decay in the association between two scores as a function of time is going to _accelerate_. At least that is my approach in `m4`. Below I add a third Gaussian process that enforces this concept. It is a squared exponential covariance kernel - our good old decay function - multiplied by a second squared exponential term. The idea here is that there is that the decay function is not constant. The further out we get in our forecasts, the less certain we should be about our predictions and observed data should be even less predictive of values in the increasingly distant future than the simple squared exponential kernel can account for on its own. 

```{r, eval=FALSE}
# Easier to note these upfront
m4_pars.to.monitor <- c(
  paste0('r', 1:5), 
  paste0('a', 1:3), 
  'sigma_sq', 
  'Ypred'
)

#Note that I have a machine at home with 12 logical cores and 64GB of RAM
m4 <- stan(
  file = '../../blog_code/gp_m4.stan',
  data = m1_data, 
  warmup = 1000,
  iter = 2000,
  refresh=1000,
  chains = 6,
  pars = m4_pars.to.monitor,
  control = list(adapt_delta = .95, 
                 max_treedepth = 10), 
  seed = 20030414
)
```

```{r, echo=FALSE}
g_m4.1 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = 'Model 4. The sum of three Gaussian process models.') +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

g_m4.1
```

Model 4 predictions extended from the observed data: 

```{r}
g_m4.2 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .7) +
  geom_point(data = air_pass_train, aes(x = year, y = pass)) +
  geom_line(data = air_pass_train, aes(x = year, y = pass)) +
  labs(x = "Year", y = "Passengers (in thousands)", 
       caption = str_wrap('Model 4 - forecast: Three Gaussian process models combined')) +
  theme_bw()

g_m4.2
```

The predicted trend line has moved even closer to the observed, out-of-sample data with `m4`. Additionally, we see the expected behavior of our credibility interval. The farther out from the observed data we attempt to forecast, the more uncertain we are about the predictions. When thinking about models as engines that drive key decisions, being honest about how uncertain we are in forecasts is important, and too often overlooked. 

At this point I am pretty satisfied with the Gaussian process model I have assembled here. As a point of comparison we can use an ARIMA model - a very common time-series model that can model seasonality and average trends over time as well. Hopefully, my approach will compare well against the more common technique. 

Here is the model setup: 

```{r, eval = FALSE}
(arima_fit<- arima(log(AirPassengers[1:132]), c(0, 1, 1),
              seasonal = list(order = c(0, 1, 1), period = 12)))
update(arima_fit, method = "CSS")

pred <- predict(arima_fit, n.ahead = 12)
tl <- pred$pred - 1.96 * pred$se
tu <- pred$pred + 1.96 * pred$se

arima_forecast <-data.frame(
  year=air_pass_test[['year']], 
  pass=exp(pred$pred), 
  UB=exp(as.numeric(tu)), 
  LB=exp(as.numeric(tl))
)
```

And here are the results overlayed on top of the `m4` predictions and observed data.

```{r, echo=FALSE, eval=FALSE}
g_arima_m4 <- 
brms::posterior_samples(m4, 'Ypred') %>% 
  as.data.frame() %>% 
  pivot_longer(cols = starts_with('Ypred'), values_to = 'pass_log') %>% 
  mutate(
    year = rep(air_pass_test[['year']], 6000), 
    pass = exp(pass_log)
  ) %>%
  ggplot(aes(x = year, y = pass)) +
  stat_lineribbon(color = "#08519C", fill = '#426EBD', lwd = .5, .width = .95, alpha = .33) +
  geom_line(
    data = arima_forecast, 
    aes(x=year, y=pass), 
    color = RColorBrewer::brewer.pal(9, 'Reds')[7], 
    alpha = .33
    ) +
  geom_ribbon(
    data = arima_forecast, 
    aes(ymax=UB, ymin=LB), 
    fill = RColorBrewer::brewer.pal(9, 'Reds')[3], 
    alpha = .33
    ) +
  geom_point(data = air_pass_test, aes(x = year, y = pass)) +
  geom_line(data = air_pass_test, aes(x = year, y = pass)) +
  labs(x = "Month in 1960", y = "Passengers (in thousands)", 
       caption = str_wrap('Overlay of observed time series, ARIMA forecast (red) and, GP forecast (blue)')) +
  scale_x_continuous(breaks = c(1960, 1960.25, 1960.5, 1960.75), 
                     labels = c('Jan', 'Apr', 'Jul', 'Oct')) +
  theme_bw()

```

Some key differences
1. The ARIMA takes a lot less time to execute. Gaussian process models are unfortunately $O(N^3)$ which means that as the number of data points increase the computation burden increases at an exponential rate. 
1. The ARIMA model's mean forecast is similar, but slightly worse than the Gaussian process model.
1. The ARIMA model is more certain about its predictions and while uncertainty does increase over time, the increase is not as pronounced. 

```{r, echo = FALSE}
g_arima_m4
```

Summary