<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dead Reckoning Analytics and Consulting</title>
    <link>/</link>
    <description>Recent content on Dead Reckoning Analytics and Consulting</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 19 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Visualizing Variance in Multilevel Models Using the Riverplot Package</title>
      <link>/blog/2019/01/19/2019-01-19-visualizing-variance-in-multilevel-models-using-the-riverplot-package/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/01/19/2019-01-19-visualizing-variance-in-multilevel-models-using-the-riverplot-package/</guid>
      <description>Spurred on by Alex Shackman, I have been working to figure out a good way to visualize different sources of variation in momentary mood. The most common way of visually depicting variance decompositions from the sort of multilevel models we used to analyze our data is a stacked bar plot. So that seemed like a good place to start.
Figure 1. Stacked Barplot of Model Variance Decomposition
 Now, choosing a color scheme that screams “HI I’M A COLOR!</description>
    </item>
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 2 of 3)</title>
      <link>/blog/2018/12/03/2018-12-03-a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12/03/2018-12-03-a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</guid>
      <description>In the first post in this series, I described the impetus for this trek through statistical modeling, machine learning and artificial intelligence. I also provided an initial set of comparisons for three different approaches to classification: k-means, k-nearest neighbor, and latent profile analysis (model-based clustering). If you want to check those mini-walkthroughs out click here.
As a reminder, my goal here is to compare and contrast different approaches to data analysis and predictive modeling that are, in my mind, arbitrarily lumped into statistical modeling and machine learing/artificial intelligence categories.</description>
    </item>
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 1 of 3)</title>
      <link>/blog/2018/11/29/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/11/29/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai/</guid>
      <description>I was recently interviewing for a job and a recruiter asked me if I wanted to enhance aspects of my machine learning background on my resume before she passed it on for the next round of reviews. I resisted the urge to chide her in the moment by pointing out the flawed distinction between statistics and machine learning, an unnecessary admonishment that would have been to no one’s benefit. The modal outcome would have been me sounding as if I was speaking with all of the arrogance, jackassery, and superiority that a recently minted Ph.</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data Complete Workflow (Part 2 of 3)</title>
      <link>/blog/2018/09/03/2018-09-03-bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/09/03/2018-09-03-bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</guid>
      <description>Overview: This is the second post in a three-part blog series I am putting together. If you have not read the first post in this series, you may want to go back and check it out. In this post, I will focus on running and evaluating the imputation model itself, having identified the appropriate covariates that help account for missingness in the first post.
Data Brief Description: The data in question come from a study that involved a one-week ecological momentary assessment (EMA) protocol.</description>
    </item>
    
    <item>
      <title>It&#39;s Alive! First Evidence that IBI VizEdit Works</title>
      <link>/blog/2018/08/16/2018-08-16-it-s-alive-first-evidence-that-ibi-vizedit-works/</link>
      <pubDate>Thu, 16 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/08/16/2018-08-16-it-s-alive-first-evidence-that-ibi-vizedit-works/</guid>
      <description>It is official. The program I have spent the better part of a year working on, the very centerpiece of my dissertation, works. Or at least, early indicators are in, and based on 22 cases, some of which required a great deal of manual editing, the program is returning estimates in line with expectations.
Backing up, as I trip a little over my excitement, IBI VizEdit is an Rshiny application I created to help our lab process and edit heart rate data.</description>
    </item>
    
    <item>
      <title>Interaction Plots with Continuous Moderators in R</title>
      <link>/blog/2018/07/11/2018-07-12-interaction-plots-with-continuous-moderators-in-r/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07/11/2018-07-12-interaction-plots-with-continuous-moderators-in-r/</guid>
      <description>Long ago (the first half of my grad school life), I created a model for a manuscript I submitted. The paper was focused on adolescents’ appraisals of their relationships with their mothers, fathers, and best friends. Specifically, I wanted to test whether the association between different motivations for social withdrawal (i.e., removing oneself from social activities and interactions) and internalizing symptoms varied as a function of perceived support in any one (or all three) of these relationships.</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data: Complete Work Flow - Part 1 of 3</title>
      <link>/blog/2018/07/05/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07/05/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow/</guid>
      <description>Overview: This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for missingness. The third and final post will be a walkthrough of the final models and their interpretation - including a comparison of the same models using listwise deletion (which is bad unless missingness is small or definitely, 100% completely at random).</description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>/blog/2018/06/07/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06/07/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. Having already collected data (i.e., having a fixed sample size), the goal of the power analysis was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (\(\alpha = .05\)) for a secondary research question that was not part of the original proposal (we added collection of a set of variables partway through the data collection period).</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>/blog/2018/05/21/2018-05-21-gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05/21/2018-05-21-gaussian-process-imputation-models/</guid>
      <description>A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/</guid>
      <description>Here to help I derive a great deal of satisfaction from helping others achieve their data and analysis goals. Please reach out if you have questions about any of open-source products I have developed, or if you are interested in retaining my consulting services for a custom project.</description>
    </item>
    
    <item>
      <title>Dashboards &amp; Reports</title>
      <link>/dashboards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/dashboards/</guid>
      <description>A Picture Graph is Worth a Thousand Words Spreadsheets It is hard to understate the value of effective data visualization. It also hard to overstate how easy it is to be tricked (or to trick ourselves) with poorly designed graphics that may not be communicating the true properties of the data. In our work at Dead Reckoning graphical representations of data primarily perform at least one of the following roles: exploratory analysis, data summary, or data-based decision-making.</description>
    </item>
    
    <item>
      <title>Data Pipelines</title>
      <link>/pipelines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/pipelines/</guid>
      <description>Drilling for Data: The New Black Gold In the age of Big Data (an overused but accurate term), few resources contain more inherent value that the information populating servers and hard drives around the world. Six of the top seven companies in the world (as measured by overall market value) are technology companies, and make no mistake that one enormous competitive advantage these companies enjoy is the ability to obtain, store, and ultimately navigate literal oceans of data.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/faq/</guid>
      <description>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.
1. WHAT TO DO IF I HAVE STILL NOT RECEIVED THE ORDER? Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante.</description>
    </item>
    
    <item>
      <title>Meaning Behind the Name Dead Reckoning</title>
      <link>/background/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/background/</guid>
      <description>Dead Reckoning: A Framework for Data Science Before satellites could accurately estimate the global position of an object within a few meters, sailors needed a way to determine where they were in a vast, remote, and otherwise unremarkable seascape. Their answer to the problem was data science, an early version of the practice to be sure, but data science nonetheless. They did not have R or Python scripts running code in the background, but they did develop specialized computers.</description>
    </item>
    
    <item>
      <title>Predictive Analytics</title>
      <link>/predictive-analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/predictive-analytics/</guid>
      <description>Predictive Analytics and Data-Driven Decision-Making For data science nerds like myself, the modeling step is often the most fun, in part because it is represents the most tangible value-add deliverable in the data science life cycle. It is also the point at which some certainty regarding decision opportunities begins to emerge. Lastly, it is the stage at which all of the hard work setting up pipelines, cleaning and transforming data, generating features, and performing exploratory analyses begins to pay off.</description>
    </item>
    
  </channel>
</rss>