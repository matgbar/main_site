<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matt Barstead&#39;s Research, Blog, &amp; Repository on Matt Barstead&#39;s Research, Blog, &amp; Repository</title>
    <link>https://mgb-research.netlify.com/</link>
    <description>Recent content in Matt Barstead&#39;s Research, Blog, &amp; Repository on Matt Barstead&#39;s Research, Blog, &amp; Repository</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Matthew Barstead</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Interaction Plots with Continuous Moderators in R</title>
      <link>https://mgb-research.netlify.com/post/interaction-plots-with-continuous-moderators-in-r/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/interaction-plots-with-continuous-moderators-in-r/</guid>
      <description>&lt;p&gt;Long ago (the first half of my grad school life), I created a model for a manuscript I submitted. The paper was focused on adolescents’ appraisals of their relationships with their mothers, fathers, and best friends. Specifically, I wanted to test whether the association between different motivations for social withdrawal (i.e., removing oneself from social activities and interactions) and internalizing symptoms varied as a function of perceived support in any one (or all three) of these relationships.&lt;/p&gt;
&lt;p&gt;It is and was a modest study, with some flaws (notably the fact I only had self-report measures from the adolescents). If you want more context for the rest of this post, you can read the paper &lt;a href=&#34;https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/&#34;&gt;here&lt;/a&gt;. These data come from the &lt;em&gt;Friendship Project&lt;/em&gt; and were collected in the early-to-mid 2000s, a fact that I wish I had included in the manuscript in retrospect. My blog is as good a place as any to call out my past transparency shortcomings I suppose.&lt;/p&gt;
&lt;p&gt;To understand the data and models here is some additional information:&lt;br /&gt;
1. I had grade 8 measures of relationship quality, shyness, preference for solitude, and anxiety/depression 2. I had grade 9 measures of anxiety and depression 3. The analyses were based on a saturated path model in &lt;code&gt;lavaan&lt;/code&gt; - an R package for creating and testing structural equation models.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;lavaan&lt;/code&gt; (&lt;strong&gt;la&lt;/strong&gt;tent &lt;strong&gt;va&lt;/strong&gt;riable &lt;strong&gt;an&lt;/strong&gt;alysis), I created three latent variables that represented outcomes of interest in grade 9: anxiety, depression, and general negative affect (I refer to this as dispositional negativity in the manuscript). To start, here is the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod8&amp;lt;-&amp;#39;
int=~cdi9ngmd+cdi9inpr+cdi9inef+cdi9anhe+cdi9ngse+masc9tr+masc9sma+masc9per+masc9ac+masc9hr+masc9pf
dep=~cdi9ngmd+cdi9inpr+cdi9inef+cdi9anhe+cdi9ngse
anx=~masc9tr+masc9sma+masc9per+masc9ac+masc9hr+masc9pf
int~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt
dep~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt+frsptXpfs+frsptXshy
anx~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt
#Covariances
int~~0*dep
int~~0*anx
anx~~0*dep
#Added covariances 
masc9hr ~~ masc9pf
cdi9inpr ~~ masc9per
cdi9inef ~~  masc9ac 
&amp;#39;

fit8&amp;lt;-sem(mod8, data=dat, std.lv = T, estimator = &amp;#39;MLR&amp;#39;)
summary(fit8, fit.measures=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there is a whole script file of assumption checking and model evaluation that goes along with this (that is how I got to model 8). I have just included the final model here for simplicity’s sake.&lt;/p&gt;
&lt;p&gt;One thing that frustrates me when we report on interactions between variables is that we often pick static, potentially arbitrary values to probe simple slopes. A common pair of values is +/- 1 &lt;em&gt;SD&lt;/em&gt;. Sometimes researchers will also examine simple slopes at the mean of the sample as well. Probing simple slopes at static values also influences how researchers present their findings visually. Typically we get either a pair of bars or lines to use to help us better evaluate the nature of the detected interaction.&lt;/p&gt;
&lt;p&gt;Here is where my gripe starts to eek in. Why are we probing simple slopes at static (often arbitrary) values? When our moderators are continuous it seems to me the better approach is to evaluate and interpret the effect of the moderator on the association between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; along the entire continuum of plausible moderator values.&lt;/p&gt;
&lt;p&gt;To do this, I need to extract some information from my model first. I’ll need the point estimates and their variances invovled in defining the moderation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parm&amp;lt;-as.data.frame(parameterEstimates(fit))
parm[,1:3]

COV&amp;lt;-vcov(fit)
COVd&amp;lt;-as.data.frame(COV)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay so now that I have the information, I can grab the relevant values as follows. (You don’t need to grab things from the model using R objects - you can type in values manually here instead should you wish).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&amp;lt;-parm[26,4]#slope for SHY
b3&amp;lt;-parm[28,4]#slope for SHY x Maternal Support
s11&amp;lt;-COV[26,26]#variance for SHY
s13&amp;lt;-COV[26,28]#covariance for SHY and SHY x Maternal Support
s33&amp;lt;-COV[28,28]#variance for SHY x MS Parameter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then need to create a vector of values that I am going to use for plotting. Since the long-standing convention has been to use standardized values of the moderator for probing simple slopes, I keep with that tradition in the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd1&amp;lt;-c(-2, -1.75, -1.5, -1.25, -1, -.75, -.5, -.25, 0,
       .25,.5,.75,1,1.25,1.5,1.75,2)

sd2&amp;lt;-sd1*sd(data$c.mospt)

#Formula for standard error of simple slopes can be found in most regression textbooks 
se&amp;lt;-sqrt(s11+2*(sd2)*s13+(sd2)*(sd2)*s33)
se

b1&amp;lt;-b1+b3*(sd2)

b1
UB&amp;lt;-b1+se*qnorm(.975)
LB&amp;lt;-b1+se*qnorm(.025)

jpeg(&amp;quot;SHYxMS predicting DN.jpeg&amp;quot;, res=300, width = 9, height=6, units = &amp;quot;in&amp;quot;)
plot(b1~sd1, type=&amp;quot;n&amp;quot;, ylim=c(-2, 2), xlim=c(-2, 2), 
     xlab=&amp;quot;Standardized Maternal Support&amp;quot;, ylab=&amp;quot;Shyness Slope&amp;quot;, 
     main=&amp;quot;Relation betweeen Shyness and Dispositional Negativity 
     as a Function of Maternal Support&amp;quot;, 
     cex.lab=1.5, cex.main=1.9)
polygon(c(rev(sd1), sd1), c(rev(UB), LB), col=&amp;quot;grey80&amp;quot;, border=NA)
lines(b1~sd1, type=&amp;quot;l&amp;quot;, lwd=4)
lines(UB~sd1, lty=&amp;quot;dashed&amp;quot;, col=&amp;quot;red&amp;quot;, lwd=2)
lines(LB~sd1, lty=&amp;quot;dashed&amp;quot;, col=&amp;quot;red&amp;quot;, lwd=2)
abline(a=0, b=0, lty=&amp;quot;dotted&amp;quot;, col=&amp;quot;black&amp;quot;, lwd=3)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And voila! To me, the greatest value of this approach is that I can see exactly where 0 is included in the 95% confidence band and where the value falls outside the confidence band.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;img/Barstead_etal_Jora2017_fig4.jpg&#34; alt=&#34;My picture&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;My picture&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Note that you may have to change the y-axis scaling to get the plot to display correctly. Otherwise, you now never have to ever ever plot the interaction between two continuous variables using a pair of static variables.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generalization of an early intervention for inhibited preschoolers to the classroom setting</title>
      <link>https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Withdrawing from the peer group</title>
      <link>https://mgb-research.netlify.com/publication/rubin_etal_2018_handbook/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/rubin_etal_2018_handbook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>&lt;p&gt;Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. Having already collected data (i.e., having a fixed sample size), the goal of the power analysis was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;) for a secondary research question that was not part of the original proposal (we added collection of a set of variables partway through the data collection period).&lt;/p&gt;
&lt;p&gt;We collected measures of these variables at three time points, approximately evenly spaced apart, and, for the purposes of these analyses, I decided to treat the data as if they were collected at precisely the same equally spaced interval for all participants. Though this is not technically true, it is sufficiently true for the purposes of these analyses. Modifying the code to take into account the actual difference in time between individual assessments is entirely possible and potentially important depending on your measures and design.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; In short, I need a reasonable population model for my data. Ideally, this model is grounded in both theory and empirical findings.&lt;/p&gt;
&lt;p&gt;To create a reasonable population model, even for a relatively simple analysis (the present working example qualifies as such), I need to think through what I know about the treatment effects and the target population. For instance, we know that data in the present case were obatined from a selected sample of children, who were eligible to participate if their scores on a &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1046/j.1467-8624.2003.00645.x&#34;&gt;measure of early childhood anxiety risk&lt;/a&gt; exceeded the 85th percentile.&lt;/p&gt;
&lt;p&gt;This knowledge provides useful information when considering population priors. For instance, I should expect the obtained sample to be drawn from a population elevated in anxiety and other, related, symptoms of emotional and/or behavioral maladaptation at time 1.&lt;/p&gt;
&lt;p&gt;This is useful until I need to consider what I mean by “elevated,” at least insofar as how I will define it numerically in the model. To address this definitional issue, it is helpful to adopt a scale (sometimes a standardized scale is a particularly good option given the easy and obvious interpretation of scores).&lt;/p&gt;
&lt;p&gt;For the present set of analyses, I am going to attempt to place my outcome measure on a pseudo-&lt;em&gt;T&lt;/em&gt;-score (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\mu \approx 50, \sigma \approx 10\)&lt;/span&gt;), that is approximately normally distributed in the population (note that I am referring to a clinical &lt;em&gt;T&lt;/em&gt;-score not a &lt;em&gt;t&lt;/em&gt; distribution). In a Bayesian sense, I am setting a prior; namely that I believe the obtained sample was randomly drawn from a population of children with elevated scores on a measure of maladaptation.&lt;/p&gt;
&lt;p&gt;So far I have only settled on a starting point (i.e., the intercept), but I have a number of parameters I need to consider specfying, and most importantly a number of parameters about which I am somewhat uncertain. To see what other parameters I need to consider, it is perhaps useful to review the simple linear growth model (specified below using &lt;a href=&#34;https://books.google.com/books/about/Hierarchical_Linear_Models.html?id=uyCV0CNGDLQC&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;Raudenbush &amp;amp; Bryk’s&lt;/a&gt; notation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Level 1 Equation:&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y_{ti} = \pi_{0i} + \pi_{1i}(Time) + e_{ti}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The level 1 equation includes the coefficient &lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt; which represents the average predicted value for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(Time=0\)&lt;/span&gt;. For this reason researchers typically center their intercepts at a meaningful value. In the present analyses &lt;span class=&#34;math inline&#34;&gt;\(Time\)&lt;/span&gt; will coded such that &lt;span class=&#34;math inline&#34;&gt;\(T_1 = 0, T_2 = 1,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_3 = 2\)&lt;/span&gt;. With this specification, the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt;}) represents the predicted value for an outcome (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;) prior to the start of the intervention (note that I am also including a random effect in this model specification - more on that below).&lt;/p&gt;
&lt;p&gt;Average change over time is represented in the model by &lt;span class=&#34;math inline&#34;&gt;\(\pi_{1i}\)&lt;/span&gt;. Any one case, however, likely deviates to some degree from the average model of change. These deviations are sometimes referred to as random effects. In the context of a growth model, they represent the degree to an individual case’s slope (or intercept) deviates from the estimated average. Using Raudenbush &amp;amp; Bryk’s (2002) notation, these random effects are defined below, where the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;’s represent the fixed effects and the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;’s represent individual deviations from the estimated fixed effects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Level 2 Equations:&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\pi_{0i} = \beta_{00} + r_{0i}
\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\pi_{1i} = \beta_{10} + r_{1i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a quick toy example, let’s see what putting priors into practice actually means. Say I have 10 cases measured repeatedly over 5 equally spaced time points. Over the course of the entire data collection window, I expect that I will see an average decrease of -2.5 units in the outcome measure. Additionally, I expect that while most participants will exhibit negaive change overall, it is possible that some cases will experience positive change - I want to make sure that my priors allow for this possibility in practice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(5499)

N&amp;lt;-10
Time.vals&amp;lt;-rep(0:4, times=N)  #5 time points
ID&amp;lt;-rep(1:N, each=5)
pred.y&amp;lt;-vector()
Time&amp;lt;-0:4

#Hyperparamters - fixed effects for slope and intercept
b_00&amp;lt;-50
b_10&amp;lt;--0.5
  for(n in 1:N){
    #Level 1 error
    e_ti&amp;lt;-rnorm(5, 0, 3)
    
    #Level 2 sources of error
    r_0i&amp;lt;-rnorm(1, 0, 5)
    r_1i&amp;lt;-rnorm(1, 0, 1.5)
    
    #Level 1 coefficients 
    pi_0i&amp;lt;-b_00+r_0i
    pi_1i&amp;lt;-b_10+r_1i
    
    #Outcome incoporating level 1 error
    pred.y&amp;lt;-c(pred.y, pi_0i+pi_1i*Time+e_ti)
  }


DF.temp&amp;lt;-data.frame(pred.y, ID, Time.vals)
DF.temp$ID&amp;lt;-as.factor(DF.temp$ID) #a little easier to work with a grouping factor here. 
g1&amp;lt;-ggplot(data=DF.temp)+
  geom_line(aes(x=Time.vals, y=pred.y, group = ID, color=ID), lty=&amp;#39;dashed&amp;#39;, alpha=.75)+
  geom_smooth(aes(x=Time.vals, y=pred.y, group=ID, color=ID), method = &amp;#39;lm&amp;#39;, se=F)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/samp_growth-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the plot above, let’s see if I got what I wanted (note the dashed lines are the raw data for each case). While most cases did in fact decline on average, there are two cases for which scores on the outcome measure increased. Of course, I would need to inspect a larger number of randomly generated datasets (according to my population model) to know for sure. I recommend plotting 10-15 randomly generated data sets and ensuring that the average change and individual heterogeneity of change approximate your expectations.&lt;/p&gt;
&lt;p&gt;Whenever conducting a power analysis, to the degree possible, it is important to ensure that the assumptions you are building into that analysis are meaningfully grounded in some way - typically tethering a rationale to theory, empirical reports, or both.&lt;/p&gt;
&lt;p&gt;Though true, this framework encourages thinking at the level of the overall effect. We often turn to meta-analyses or other reviews that have attempt to describe or define the population-level effect. Less often do these sorts of reviews speak to individual variability from mean estimates.&lt;/p&gt;
&lt;p&gt;The net result is that we sometimes struggle to define our expectations about variability. This is where plotting the data is so valuable in my mind. I can quickly see whether or not individual trajectories are departing wildly from what is reasonable. If so, I can tweak certain aspects of the model and re-inspect until I am satisfied with the results.&lt;/p&gt;
&lt;p&gt;We always stress plotting your real data. Same goes for the made-up stuff too.&lt;/p&gt;
&lt;p&gt;Once I have sufficiently tuned up my population model, the next trick is to simulate a sufficiently large number of data sets to estimate power for my proposed model of change. Briefly, I will note that I am not really addressing the possibility that there is a meaningful correlation between individual random effects. I’ll save that for a future post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; Now that I have something of a framework created, I can apply it to my problem in a more direct fashion. I should state at the outset that this simulation is going to take some time. That is because I have chosen to evaluate significance using coverage of &lt;code&gt;0&lt;/code&gt; by a 95% boostrapped confidence interval. I tend to prefer this over say a p-value generated by the &lt;code&gt;lmer()&lt;/code&gt; function using the {lmerTest} library. This may be overkill, but it is the way I would typically assess whether coefficients in the model meaningfully differ from 0, so it is the approach I will be using to assess power as well.&lt;/p&gt;
&lt;p&gt;My goal with the analysis presented below is to assess the power of the model to detect significant negative linear change with the expectation that the overall effect in the population is relatively small (I’ll be using &lt;em&gt;Cohen’s&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as a guide for evaluating effect size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The libraries used
library(lme4)
library(merTools)
library(boot)
library(sjstats)
library(ggplot2)

#Specifying fixed effects for model
b_00&amp;lt;-65        #Fixed intercept: Average starting point on a pseudo-t-score scale)
b_10&amp;lt;--1.5      #Fixed slope: Change per unit of time 
Time&amp;lt;-0:2       #Vector of equally-spaced intervals

#Setting up some empty vectors for the simulation to fill in
#------------------------------------------------------------------------------------
#Intercept vectors
b00_Est&amp;lt;-vector()
b00_boot_se&amp;lt;-vector()
b00_boot_LB&amp;lt;-vector()
b00_boot_UB&amp;lt;-vector()
b00_var&amp;lt;-vector()

#Slope vectors
b10_Est&amp;lt;-vector()
b10_boot_se&amp;lt;-vector()
b10_boot_LB&amp;lt;-vector()
b10_boot_UB&amp;lt;-vector()
b10_var&amp;lt;-vector()

#Capturing variability in Y at multiple levels and overall
ICC.vals&amp;lt;-vector()
sd.y&amp;lt;-vector()
CohensD&amp;lt;-vector()
#------------------------------------------------------------------------------------
#Select number of simulations &amp;amp; Sample size
n.sims&amp;lt;-2500  #number of total simulations to run - recommend &amp;gt; 5,000
N&amp;lt;-120          #Sample size 

for(s in 1:n.sims){
  #browser()
  Time.vals&amp;lt;-rep(0:2, times=N)
  IDs&amp;lt;-rep(1:N, each=3)
  pred.y&amp;lt;-vector()
  for(n in 1:N){
    #Level 1 error
    e_ti&amp;lt;-rnorm(3, 0, 5)
    
    #Level 2 sources of error
    r_0i&amp;lt;-rnorm(1, 0, 5)
    r_1i&amp;lt;-rnorm(1, 0, 2.5)
    
    #Level 1 coefficients 
    pi_0i&amp;lt;-b_00+r_0i
    pi_1i&amp;lt;-b_10+r_1i
    
    #Outcome incoporating level 1 error
    pred.y&amp;lt;-c(pred.y, pi_0i+pi_1i*Time+e_ti)
  }
  
  DF&amp;lt;-data.frame(ID=IDs, 
                 Time=Time.vals, 
                 Y=pred.y)
  
  fit.null&amp;lt;-lme4::lmer(Y~1+(1|ID), DF)
  ICC.vals&amp;lt;-c(ICC.vals, as.numeric(sjstats::icc(fit.null)))
  sd.y&amp;lt;-c(sd.y, sd(pred.y))
  CohensD&amp;lt;-c(CohensD, effsize::cohen.d(c(DF$Y[DF$Time==2], DF$Y[DF$Time==0]), f=rep(c(&amp;#39;T3&amp;#39;, &amp;#39;T1&amp;#39;), each=120))$estimate)
  fit.ucgm&amp;lt;-lme4::lmer(Y~1+Time + (1+Time|ID), data=DF)
  
  boot.ucgm&amp;lt;-bootMer(fit.ucgm, FUN=fixef, type = &amp;#39;parametric&amp;#39;,
                     nsim=1000, parallel = &amp;#39;multicore&amp;#39;, ncpus=12)
  
  #obtaining CIs for intercept
  b00_Est&amp;lt;-c(b00_Est, mean(boot.ucgm$t[,1]))
  b00_boot_se&amp;lt;-c(b00_boot_se, sd(boot.ucgm$t[,1]))
  b00_boot_LB&amp;lt;-c(b00_boot_LB, b00_Est[s]+qt(.975, N-1)*b00_boot_se[s])
  b00_boot_UB&amp;lt;-c(b00_boot_UB, b00_Est[s]+qt(.025, N-1)*b00_boot_se[s])

  #obtaining CIs for time slope
  b10_Est&amp;lt;-c(b10_Est, mean(boot.ucgm$t[,2]))
  b10_boot_se&amp;lt;-c(b10_boot_se, sd(boot.ucgm$t[,2]))
  b10_boot_LB&amp;lt;-c(b10_boot_LB, b10_Est[s]+qt(.975, N-1)*b10_boot_se[s])
  b10_boot_UB&amp;lt;-c(b10_boot_UB, b10_Est[s]+qt(.025, N-1)*b10_boot_se[s])

  #Obtaining estimates of variability in slope and intercept
  b00_var&amp;lt;-c(b00_var, as.numeric(VarCorr(fit.ucgm)$ID[1,1]))
  b10_var&amp;lt;-c(b10_var, as.numeric(VarCorr(fit.ucgm)$ID[2,2]))
  print(paste(s, &amp;#39;out of&amp;#39;, n.sims, &amp;#39;simulations&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the simulation has finished, it is time to combine the output and plot. I cannot stress enough how important it is to plot things. Again, I am looking to see whether or not this model has returned reasonable estimates.&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;Pro tip&lt;/em&gt;: run your whole simulation code with a much smaller number of simulations to start - say 30 - inspect the results, then run the full simulation code if everything checks out.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_small&amp;lt;-data.frame(b00_Est=b00_Est, 
                      b00_se=b00_boot_se,
                      b00_var=b00_var, 
                      b00_boot_LB = b10_boot_LB,
                      b00_boot_UB = b10_boot_UB,
                      b10_Est=b10_Est, 
                      b10_se=b10_boot_se, 
                      b10_var=b10_var,
                      b10_boot_LB = b10_boot_LB, 
                      b10_boot_UB = b10_boot_UB,
                      sd.y=sd.y, 
                      CohensD=CohensD,
                      ICC=ICC.vals)

#plotting distributions returned from the simulations
#Selecting only certain columns related to fixed effects 
bayesplot::mcmc_dens(dat_small[,c(1:2,6:7)])+
  ggtitle(&amp;#39;Variances of Slope Estimates - Small Effect Model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/pow_sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about some of the basic properties of the data? Was the average linear effect relatively small (which was the goal of this analysis after all)? What about the intra-class correlations returned by each data set; were they reasonable given the design and expectations based on similar data sets?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::mcmc_dens(dat_small[,12:13])+
  ggtitle(&amp;#39;Model Diagnostics - Small Effect Model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/plot_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I would say yes to both. The maximum a posteriori estimate (MAPE) for standardized average change is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(dat_small$CohensD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.3826507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat_small$CohensD, c(.025, .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       2.5%      97.5% 
## -0.5780296 -0.1765590&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which to me seems entirely reasonable to classify as a “small” effect power analysis, with the added bonus that you can clearly see this approach reflects uncertainty about the magnitude of change observed in any one sample.&lt;/p&gt;
&lt;p&gt;So how do I get power? It is simple. I am interested in the power to detect negative change at &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;, given the population model I created, in a sample of 120 subjects. Okay so not that simple. Since I saved the upper boundaries of the 95% CIs for my estimates I could add up all of the times the upper boundary for a given sample was less than 0 (an indication that a significant effect was detected), divide that total by the total number of simulations and voila, I’d have a calculation of power.&lt;/p&gt;
&lt;p&gt;Using a binary variable it is even easier…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig&amp;lt;-ifelse(dat_small$b10_boot_UB&amp;lt;0, 1, 0)
mean(sig)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(dat_small$b10_boot_UB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6303266&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the maximum upper boundary for model-based 95%-bootstrapped CIs for the fixed slope was negative - which is why I get a probability of 1 using the &lt;code&gt;mean(sig)&lt;/code&gt; call. If interested I could use this information to start working my way down iteratively to a minimum effect size detectable at a rate of 80%, given the population model and a sample size of 120.&lt;/p&gt;
&lt;p&gt;Or… Instead of doing all of that trial and error work, I could create a program to do it, one that has its code written more efficiently, and one that does a little more parallelizing. All for a future post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</guid>
      <description>&lt;p&gt;A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.&lt;/p&gt;
&lt;p&gt;The models below will make use of the preloaded &lt;code&gt;AirPassengers&lt;/code&gt; data in R. The data represent the total number of monthly international airline passengers (in thousands) from 1949 to 1960. It is easy to see these data have both a non-stationary mean and a non-stationary variance. There is also a clear periodic component to these data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;#39;AirPassengers&amp;#39;)
plot(AirPassengers)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/AirPassengers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a toy problem, I am going to focus on the application of a Gaussian process model to forecasting future monthly passengers. This is not the only way one could try to solve this prediction problem. I offer it as a means of understanding the potential power that exists in using these sorts of models for prediction and imputation problems involving univariate time series data.&lt;/p&gt;
&lt;p&gt;A few notes about Gaussian process models. To start they are a class of Bayesian models. There are a few &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; packages that allow researchers to use this modeling approach. I have become something of a &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; convert recently, but it is not the only option out there.&lt;/p&gt;
&lt;p&gt;The authoritative text on Gaussian process models was arguably published by &lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/RW.pdf&#34;&gt;Rasumssen &amp;amp; Williams in 2006&lt;/a&gt;, but only recently have computing power and programming languages allowed for a deeper tapping of this methodology’s strengths. For anyone interested in learning more about these models I highly recommend the Rasmussen &amp;amp; Williams (2006) text as a starting point.&lt;/p&gt;
&lt;p&gt;It is worth pointing out that, because Guassian process models rely on Bayesian estimation, parameters either need to be fixed or given a prior distribution. I like that Bayesian analyses really make you think about your priors. It is the statistical equivalent of eating your veggies. You may not always enjoy it, but it will do you good in the long run. Strategies for choosing priors are beyond the purpose of this post. If interested, the following &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;page on the Stan GitHub repo&lt;/a&gt; provides a brief, but reasonable overview as a starting point.&lt;/p&gt;
&lt;p&gt;First, the data need a bit of prepping to be fed into a Stan program.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Obtaining a numeric vector for time. Maintaining the units of measure at 1 = 1 year
Year&amp;lt;-seq(1949, 1960+11/12, by=1/12)

#converting time-series to a vector
Pass&amp;lt;-as.vector(AirPassengers)

#identifiying number of data points for the &amp;quot;training&amp;quot; data
N1&amp;lt;-length(Year)

#specifying 2-year prediction window. 
year.fore2&amp;lt;-seq(1961, 1962+11/12, by=1/12)
N2&amp;lt;-length(year.fore2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data prepped, I will run the first of two models. The first model relies solely on the squared exponential covariance function (plus error) to define the underlying Gaussian process. The squared exponential function takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;) = \sigma^2 exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_1^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the estimated variance accounted for by the function &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a length scale parameter that governs the decay rate. Smaller estimated values for &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; indicate a faster decay rate in the covariance between two points as a function of time.&lt;/p&gt;
&lt;p&gt;This model, along with its forecasting function are defined in &lt;code&gt;Stan&lt;/code&gt; code below (Adapted from &lt;a href=&#34;http://natelemoine.com/fast-gaussian-process-models-in-stan/&#34;&gt;Nate Lemoine’s code&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real rho1){
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            Sigma[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real r1, 
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, r1);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, r1);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, r1);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; r1;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X,  a1, r1);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ student_t(3,0,1);
    sigma_sq ~ student_t(3,0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, r1, sigma_sq, N1, X, N2, Xp, Y);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model takes just over a minute to run. For those of you who are computational gearheads, here is the hardware I am working with (with a total of 64GB of RAM):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmarkme::get_cpu()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $vendor_id
## [1] &amp;quot;GenuineIntel&amp;quot;
## 
## $model_name
## [1] &amp;quot;Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz&amp;quot;
## 
## $no_of_cores
## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick view of summary stats good convergence of estimates across the 6 chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(&amp;#39;a1&amp;#39;,&amp;#39;r1&amp;#39;,&amp;#39;sigma_sq&amp;#39;, &amp;#39;Ypred&amp;#39;)
summary(fit.stan1, pars=pars.to.monitor[-4])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean      se_mean           sd        2.5%          25%
## a1        2.064974195 2.117451e-02 1.2312158202 0.739681295  1.272435874
## r1       22.681133287 1.252456e-01 9.0005034579 9.102436133 16.045254783
## sigma_sq  0.003559372 5.819530e-06 0.0004251947 0.002818252  0.003254155
##                   50%          75%        97.5%    n_eff      Rhat
## a1        1.759559393  2.505894675  5.176821352 3380.974 1.0010353
## r1       21.367822936 28.012894270 42.692355288 5164.270 0.9996089
## sigma_sq  0.003524852  0.003823097  0.004468209 5338.262 1.0002208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and traceplots demonstrate good mixing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan1, pars=c(&amp;#39;a1&amp;#39;, &amp;#39;r1&amp;#39;, &amp;#39;sigma_sq&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the real question is how did the model do?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not all that great of a prediction to be honest. In this case, the function essentially reduces to a linear regression as there is no place for the periodic nature of the data to be explicitly modeled. This is where the flexibility of Gaussian process models starts to shine as any Gaussian process can be re-expressed as a the sum of an infinite number of Gaussian processes. Here we will add a covariance function that incorporates periodicity. In this case, a period is approximately one year.&lt;/p&gt;
&lt;p&gt;The new periodic covariance function is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;)=\sigma_2^2 exp\Big(-\frac{2sin^2(\pi(t-t&amp;#39;)*1)}{l_2^2}\Big) exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_3^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The inclusion of the squared exponential function here simply reduces the ability of the annual features of the data to explain covariation as the interval between two points grows. Here is the &lt;code&gt;Stan&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real alpha2,
        real rho1,
        real rho2,
        real rho3){
                    matrix[Nx, Ny] K1;
                    matrix[Nx, Ny] K2;
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            K1[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    //specifying random Gaussian process incorporates heart rate
                    for(i in 1:Nx){
                        for(j in 1:Ny){
                            K2[i, j] = alpha2*exp(-2*square(sin(pi()*fabs(x[i]-y[j])*1))/square(rho2))*
                            exp(-square(x[i]-y[j])/2/square(rho3));
                        }
                    }
                        
                    Sigma = K1+K2;
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real a2,
        real r1, 
        real r2,
        real r3,
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, a2, r1, r2, r3);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, a2, r1, r2,  r3);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, a2, r1, r2, r3);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; a2;
    real&amp;lt;lower=15&amp;gt; r1;      //Set after some preliminary modeling
    real&amp;lt;lower=0&amp;gt; r2;
    real&amp;lt;lower=0&amp;gt; r3;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X, a1, a2, r1, r2, r3);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ normal(2.06,1.23);     //Taken from the first model
    a2 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ normal(22.68,9.005);   //Taken from the first model
    r2 ~ student_t(3,0,1);
    r3 ~ student_t(3,0,1);  
    sigma_sq ~ normal(0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, a2, r1, r2, r3, sigma_sq, N1, X, N2, Xp, Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model took about 6 minutes to run.&lt;/p&gt;
&lt;p&gt;Unfortunately, the sampling algorithm that generates the Bayesian estimates is not parallelizable. Until &lt;code&gt;Stan&lt;/code&gt; and &lt;code&gt;rstan&lt;/code&gt; can run using graphics chips’ architecture (which has been buzzed about around the &lt;a href=&#34;http://discourse.mc-stan.org/t/stan-on-the-gpu/326&#34;&gt;Stan ether&lt;/a&gt;), model run time is going to be the biggest downside. Still, 6 minutes is not that long to wait if the model performs well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(paste0(&amp;#39;a&amp;#39;, 1:2), paste0(&amp;#39;r&amp;#39;, 1:3), &amp;#39;Ypred&amp;#39;)
summary(fit.stan2, pars=pars.to.monitor[-6])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean      se_mean          sd        2.5%          25%
## a1  2.54983942 0.0134372272  0.92993236  1.03551274  1.842231419
## a2  0.01028192 0.0007652084  0.02499935  0.00195483  0.003809465
## r1 29.72788580 0.0860626598  5.77288905 19.05829543 25.751944510
## r2  0.71435573 0.0024883370  0.12979765  0.49982165  0.627554970
## r3 21.16693790 0.7029880494 10.71983939  2.13531511 14.836950073
##             50%        75%       97.5%     n_eff     Rhat
## a1  2.464693706  3.1470023  4.60109271 4789.4253 1.001067
## a2  0.005887629  0.0100722  0.04229047 1067.3281 1.004514
## r1 29.510979184 33.4589313 41.57756671 4499.4237 1.000575
## r2  0.700328139  0.7850996  1.00378065 2720.9169 1.000556
## r3 19.220095144 25.4484115 45.91201691  232.5309 1.026833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan2, pars=pars.to.monitor[-6])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The forecast looks to be in line with what I might expect based on trends leading up to 1962. The results are similar to those obtained using a different forecasting technique (i.e., an ARIMA model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- arima(log10(AirPassengers), c(0, 1, 1),
              seasonal = list(order = c(0, 1, 1), period = 12)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12))
## 
## Coefficients:
##           ma1     sma1
##       -0.4018  -0.5569
## s.e.   0.0896   0.0731
## 
## sigma^2 estimated as 0.0002543:  log likelihood = 353.96,  aic = -701.92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update(fit, method = &amp;quot;CSS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12), method = &amp;quot;CSS&amp;quot;)
## 
## Coefficients:
##           ma1     sma1
##       -0.3772  -0.5724
## s.e.   0.0883   0.0704
## 
## sigma^2 estimated as 0.0002619:  part log likelihood = 354.32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred &amp;lt;- predict(fit, n.ahead = 24)
tl &amp;lt;- pred$pred - 1.96 * pred$se
tu &amp;lt;- pred$pred + 1.96 * pred$se

ARIMA.for&amp;lt;-data.frame(Year=year.fore2, Passengers=10^pred$pred, UB=10^as.numeric(tu), LB=10^as.numeric(tl))

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;blue&amp;#39;)+
  geom_ribbon(data=ARIMA.for, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;red&amp;#39;)+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers, color=&amp;#39;blue&amp;#39;), lwd=1.25)+
  geom_line(data=ARIMA.for, aes(x=Year, y=Passengers, color=&amp;#39;red&amp;#39;), lwd=1.25)+
  coord_cartesian(xlim=c(1960, 1963.25), ylim= c(275,1000))+
  scale_color_manual(name=&amp;#39;Forecast Model&amp;#39;,
                     values = c(&amp;#39;blue&amp;#39;, &amp;#39;red&amp;#39;), 
                     labels = c(&amp;#39;Gaussian Process&amp;#39;, &amp;#39;ARIMA&amp;#39;))
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt; The two models make fairly similar predictions for 1961 (shaded regions represent respective 95% intervals). In 1962, the ARIMA model is a little more bullish than the Gaussian process model on airline passengers.&lt;/p&gt;
&lt;p&gt;Still, it is impossible to know which of these models is better, a methodological question I may tackle in greater detail when I have some time. The answer is almost certainly “it depends.” For now, the main takeaway is that Gaussian process models may represent a useful approach to the age-old problems of forecasting and imputation, a fact I plan to exploit in some of my signal processing work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dispositional negativity in the wild: Social environment governs momentary emotional experience</title>
      <link>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpersonal predictors of stress generation: Is there a super factor?</title>
      <link>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neuroticism and conscientiousness as moderators of the relation between social withdrawal and internalizing problems in adolescence</title>
      <link>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shyness, preference for solitude and adolescent internalizing: The roles of maternal, paternal, and best-friend support</title>
      <link>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Callous Unemotional Meta-Analysis</title>
      <link>https://mgb-research.netlify.com/project/cu-meta/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/cu-meta/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Callous-unemotional (CU) traits are central to our understanding of antisocial behavior, which is harmful, financially costly to society, and hard to treat. Theoretical and diagnostic models define CU traits based on a lack of empathy, guilt, and prosociality. However, there has been no comprehensive quantitative synthesis of results to support this claim, nor any examination of whether associations differ by age, gender, sample type, or informant. Given its centrality to our understanding and treatment of antisocial behavior, the time is ripe for a critical examination of the meaning of the CU traits construct, including associations with its purported underlying features of low empathy, guilt, and prosociality, and potential demographic and methodological moderators of these associations. To address this gap in the literature, we conducted a systematic search of the literature on CU traits and identified 82 effect sizes from 51 studies (&lt;em&gt;N&lt;/em&gt; = 30,475) that quantified the magnitude of the association between CU traits and either empathy, prosociality, or guilt. The results revealed statistically significant, modest-to-moderate negative correlations between CU traits and empathy (&lt;em&gt;k&lt;/em&gt; = 25, $\rho$  = -.21), CU traits and guilt (&lt;em&gt;k&lt;/em&gt; = 3, $\rho$ = -.18), and CU traits and prosociality (&lt;em&gt;k&lt;/em&gt; = 16, $\rho$ = -.28). The magnitude of the negative association between CU traits and prosociality was stronger among younger children. In addition, the negative correlations between CU traits and both prosociality and cognitive empathy were stronger when the informant was a parent or teacher. We discuss how these findings can inform theory, conceptualization, and measurement of CU traits across the lifespan, as well as implications for models of antisocial behavior and moral development.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation Models</title>
      <link>https://mgb-research.netlify.com/project/gp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/gp/</guid>
      <description>&lt;p&gt;I have become increasingly interested in the use of Gaussian process models as a tool for forecasting and imputation with univariate time series. See my &lt;a href=&#34;https://mgb-research.netlify.com/post/gaussian-process-imputation-models/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; on the topic for more information and a toy forecasting problem.&lt;/p&gt;

&lt;p&gt;I am currently working on a study examining the effectiveness of these models in imputing heart rate data that has been severely corrupted by artifact - to the point that there is not &amp;ldquo;enough&amp;rdquo; true signal remaining to do much of anything with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBI VizEdit</title>
      <link>https://mgb-research.netlify.com/project/ibi-vizedit/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/ibi-vizedit/</guid>
      <description>&lt;p&gt;IBI VizEdit is a program built using RShiny. It is designed to assist in the manual editing of inter-beat interval files that are derived from photoplethysmogram (PPG) recordings. Unlike the electrocardiogram signal (EKG or ECG), PPG signals are characterized by a slow-moving waveform, which presents a different set of challenges when the true signal becomes corrupted by motion artefacts and other sources of noise.&lt;/p&gt;

&lt;p&gt;Though increasingly popular due to their ease of use, most heart rate editing software that exists to date was designed and optimized for the detection and editing of inter-beat interval files derived from ECG signals. IBI VizEdit provides a new suite of tools for researchers who find themselves working with messy PPG files.&lt;/p&gt;

&lt;p&gt;Please note that IBI VizEdit is beta software. It has not been fully tested, and there are likely numerous bugs and opportunities to optimize code and performance. Any and all feedback is welcome.&lt;/p&gt;

&lt;p&gt;As of right now, IBI VizEdit is only supported for use on Windows 7/8/10 and Linux (Ubuntu 16.04 in particular).&lt;/p&gt;

&lt;p&gt;Please cite as:&lt;/p&gt;

&lt;p&gt;Barstead, M. G. (2018). IBI VizEdit v.1.2-beta: An RShiny Application [Computer software]. University of Maryland. doi: 10.5281/zenodo.1209474&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Individual Risk for Anxiety and Depression</title>
      <link>https://mgb-research.netlify.com/project/dn_research/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/dn_research/</guid>
      <description>&lt;p&gt;Cell phones are ubiquitous on college campuses, annoying many an instructor who looks back at a classroom filled with young adults enamored with their black mirrors (and yes I am still very impressed with the underlying meaning of the hit Netflix series&amp;rsquo; title). As others have in the past &lt;a href=&#34;https://psyc.umd.edu/facultyprofile/Shackman/Alexander&#34; target=&#34;_blank&#34;&gt;Dr. Alex Shackman&lt;/a&gt;, sought to use the constant presence of cell phones to better understand the daily lives of individuals.&lt;/p&gt;

&lt;p&gt;Using college students as subjects is often decried as laziness and viewed somewhat skeptically as investigators reaching out to a subject pool that is readily available to them. There is definitely some merit to these concerns; however, in the present line of work, university students are an ideal group to study. For one, the transition to college can be stressful, particularly for individuals at risk for mental health disorders such as anxiety and depression. Furthermore, understanding the risks in this population is timely as we continue to see &lt;a href=&#34;https://adaa.org/living-with-anxiety/college-students#&#34; target=&#34;_blank&#34;&gt;increases in mental health problems&lt;/a&gt; among this group. Finally, young adulthood is a period when many will experience their first major bouts of anxiety and depression (though of course many will struggle with symptoms earlier in adolescence as well) and may therefore represent an important period to intervene, preventing subsequent maladaptive cascades related to mood disorders.&lt;/p&gt;

&lt;p&gt;Successfully negotiating the college transition has important consequences for future outcomes including gaining full-time employment, marriage, and individuals&amp;rsquo; long-term mental and physical health. As part of a broader program of research evaluating the neurobiological factors that predict social, emotional, and academic success during this period Dr. Shackman designed a series of studies using ecological momentary assessments. The jargony term boils down to participants receiving multiple daily surveys (delivered via cell phone) querying about their recent activities, their social surroundings, and their current mood.&lt;/p&gt;

&lt;p&gt;The central premise motivating this work is that if we can better understand the daily lives of individuals at risk for the development of psychopathology, we can gain some insight about how to prevent it. We have published some &lt;a href=&#34;https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/&#34; target=&#34;_blank&#34;&gt;initial work&lt;/a&gt; with these data demonstrating that interacting with close others may be especially beneficial for college students at increased risk for internalizing disorders. We are currently in the process of replicating and extending this work, examining more nuanced features of momentary affective responses to daily events.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preventing Anxiety in Early Childhood</title>
      <link>https://mgb-research.netlify.com/project/turtlemd/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/turtlemd/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://education.umd.edu/directory/kenneth-rubin&#34; target=&#34;_blank&#34;&gt;Dr. Ken Rubin&lt;/a&gt; has spent the majority of his career studying the causes, correlates, and consequences of childhood anxiety and social withdrawal. As a result of an ongoing collaboration with &lt;a href=&#34;https://psyc.umd.edu/facultyprofile/Chronis-Tuscano/Andrea&#34; target=&#34;_blank&#34;&gt;Dr. Andrea Chronis-Tuscano&lt;/a&gt;, a clinical psychology professor and researcher, and in consultation with &lt;a href=&#34;https://carleton.ca/psychology/people/robert-coplan/&#34; target=&#34;_blank&#34;&gt;Dr. Rob Coplan&lt;/a&gt;, this career has recently been translated into the creation of an early intervention program. The intervention, named the &lt;em&gt;Turtle Program&lt;/em&gt;, simultaneously targets maladaptive parenting behaviors known to maintain and exacerbate childhood anxiety in addition to anxious and inhibited children&amp;rsquo;s social skills and emotion regulation deficits.&lt;/p&gt;

&lt;p&gt;Pilot work comparing the treatment against a waitlist control group demonstrated promising results insofar as &lt;a href=&#34;http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039043&#34; target=&#34;_blank&#34;&gt;reducing anxiety symptoms&lt;/a&gt; and &lt;a href=&#34;https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/&#34; target=&#34;_blank&#34;&gt;improving adaptive social behaviors&lt;/a&gt;. As of June 2018, we have completed initial data collection for the follow-up investigation comparing the &lt;em&gt;Turtle Program&lt;/em&gt; against an established parent psychoeducation intervention for shy and inhibited children (known as &lt;a href=&#34;https://coollittlekids.org.au/login&#34; target=&#34;_blank&#34;&gt;Cool Little Kids&lt;/a&gt;). While we continued to gather information about children and parents a year after participating in one of the two interventions, we have just started to compile our initial findings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Withdrawal Meta-Analysis</title>
      <link>https://mgb-research.netlify.com/project/sw-meta/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/sw-meta/</guid>
      <description>&lt;p&gt;Are the terms anxious withdrawal, anxious solitude, behavioral inhibition, shyness-sensitivity, interchangeable? Are peer, parent, teacher, and self-perceptions of social withdrawal measuring the same thing? To date, researchers have relied on narrative arguments to build a case for or againts correspondence among the terms and measurement approaches used by developmental scientists interested in these and related constructs. I want to let the data speak for itself, and I am conducting a meta-analysis to do just that.&lt;/p&gt;

&lt;p&gt;I conducted a preliminary analysis using data from 101 studies. I have since gone back and completed a more comprehensive record search and am in the process of updating my models. For a early preview of the results, check out the pre-print here on my &lt;a href=&#34;https://www.researchgate.net/publication/324746193_Psychosocial_Correlates_of_Social_Withdrawal_and_Its_Many_Variants_A_Quantitative_Synthesis_of_Research_Spanning_Four_Decades&#34; target=&#34;_blank&#34;&gt;ResearchGate page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
