<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matt Barstead&#39;s Research, Blog, &amp; Repository on Matt Barstead&#39;s Research, Blog, &amp; Repository</title>
    <link>https://mgb-research.netlify.com/</link>
    <description>Recent content in Matt Barstead&#39;s Research, Blog, &amp; Repository on Matt Barstead&#39;s Research, Blog, &amp; Repository</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Matthew Barstead</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>&lt;p&gt;Recently I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. The power analysis was needed as part of a grant application. Having already collected data on the sample, the goal was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;) for a secondary research question - which was not part of the original proposal (we added collection of a set of variables part way through the data collection period).&lt;/p&gt;
&lt;p&gt;We collected measures of these variables at three time points approximately evenly spaced apart, and for the purposes of these analyses, I decided to treat the data as if it were collected at precisely the same equally spaced interval for all participants. Though this is not technically true of the data, it is sufficiently true for the purposes of these analyses. Modifying the code to take into account the actual distance in time between assessments is entirely possible and potentially important depending on your measures and design.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; In short, I need a reasonable population model for my data. To create a reasonable population model, I need to think through what I know about the treatment effects and the population targeted for recruitment. For instance, we know that data were obatined from a selected sample of children, who were eligible to participate if their scores on a &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1046/j.1467-8624.2003.00645.x&#34;&gt;measure of early childhood anxiety risk&lt;/a&gt; exceeded the 85th percentile.
This provides useful information when considering prior information about the sample. I should expect members of the population to be drawn from a population elevated in anxiety and other, related, symptoms of emotional and/or behavioral maladaptation. For the present set of analyses, I am going to attempt to place my outcome measure on a pseudo-&lt;em&gt;T&lt;/em&gt;-score (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\mu \approx 50, \sigma \approx 10\)&lt;/span&gt;), that is approximately normally distributed in the population. In a Bayesian sense, I am setting a prior; namely that I believe the obtained sample was randomly drawn from a population of children with elevated scores on a measure of maladaptation.&lt;/p&gt;
&lt;p&gt;So far I have only settled on a starting point (i.e., the intercept), but I have a number of parameters I need to consider specfying, and most importantly a number of parameters about which I am somewhat uncertain. To see what other parameters I need to consider, it is perhaps useful to review the simple linear growth model (specified below using &lt;a href=&#34;https://books.google.com/books/about/Hierarchical_Linear_Models.html?id=uyCV0CNGDLQC&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;Raudenbush &amp;amp; Bryk’s&lt;/a&gt; notation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Level 1 Equation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{ti} = \pi_{0i} + \pi_{1i}(Time) + e_{ti}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The level 1 equation includes the coefficient &lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt; which represents the average predicted value for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(Time=0\)&lt;/span&gt;. For this reason researchers typically center their intercepts at a meaningful value. In the present analyses Time will coded such that &lt;span class=&#34;math inline&#34;&gt;\(T_1 = 0, T_2 = 1,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_3 = 2\)&lt;/span&gt;. With this specification, the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt;}) represents the predicted value for a generic outcome (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;) prior to the start of the intervention (note that I am also including a random effect in this model specification - more on that below).&lt;/p&gt;
&lt;p&gt;Average change over time is represented in the model by &lt;span class=&#34;math inline&#34;&gt;\(\pi_{1i}\)&lt;/span&gt;. Any one case, however, likely deviates to some degree from the average model of change. These deviations are sometimes referred to as random effects. In the context of a growth model, they represent the degree to an individual case’s slope (or intercept) deviates from the estimated average.&lt;/p&gt;
&lt;p&gt;As a quick toy example, let’s see what putting priors into practice actually means. Say I have 10 subjects measured repeatedly over 5 equally spaced time points. Over the course of the entire data collection window, I expect that I will see an average decrease of -2.5. Additionally, I expect that while most participants will exhibit negaive change overall, it is possible that a small group will experience positive change - I want to make sure that my priors allow for this possibility in practice.&lt;/p&gt;
&lt;p&gt;Plotting an initial sample is an important step in this process as it allows me to make sure that the simulated cases reasonably reflect plausible values of change. Are starting values reasonable and appropriate? What about individual variability around each fitted model of change? Are any changes too drastic to be reasonable? Do any values wander out of the boundaries of allowable scores? I can go back and adjust the amount of error variance or the fixed parameters to bring the resulting data more in line with my expectations as needed.&lt;/p&gt;
&lt;p&gt;Once I have sufficiently tuned up my population model, the next trick is to simulate a sufficiently large number of data sets to estimate power for my proposed model of change. Briefly, I will note that I am not really addressing the possibility that there is a meaningful correlation between individual random effects. I’ll save that for a future post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; Now that I have something of a framework created I can apply it to my problem in a more direct fashion. I should state at the outset that this simulation is going to take some time. That is because I have chosen to evaluate significance using coverage of &lt;code&gt;0&lt;/code&gt; by a 95% boostrapped confidence interval. I tend to prefer this over say a p-value generated by the &lt;code&gt;lmer()&lt;/code&gt; function using the {lmerTest} library. This may be overkill, but it is the way I would typically assess whether coefficients in the model meaningfully differ from 0, so it is the approach I will be using to assess power as well.&lt;/p&gt;
&lt;p&gt;My goal with the analysis presented below is to assess the power of the model to detect significant negative linear change under the conditions I expect to find if I had the ability to resample the population a near-infinite number of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The libraries used
library(lme4)
library(merTools)
library(boot)
library(sjstats)
library(ggplot2)

#Specifying fixed effects for model
b_00&amp;lt;-65        #Fixed intercept: Average starting point on a pseudo-t-score scale)
b_10&amp;lt;--1.5      #Fixed slope: Change per unit of time 
Time&amp;lt;-0:2       #Vector of equally-spaced intervals

#Setting up some empty vectors for the simulation to fill in
#------------------------------------------------------------------------------------
#Intercept vectors
b00_Est&amp;lt;-vector()
b00_boot_se&amp;lt;-vector()
b00_boot_LB&amp;lt;-vector()
b00_boot_UB&amp;lt;-vector()
b00_var&amp;lt;-vector()

#Slope vectors
b10_Est&amp;lt;-vector()
b10_boot_se&amp;lt;-vector()
b10_boot_LB&amp;lt;-vector()
b10_boot_UB&amp;lt;-vector()
b10_var&amp;lt;-vector()

#Capturing variability in Y at multiple levels and overall
ICC.vals&amp;lt;-vector()
sd.y&amp;lt;-vector()
CohensD&amp;lt;-vector()
#------------------------------------------------------------------------------------
#Select number of simulations &amp;amp; Sample size
n.sims&amp;lt;-10000   #number of total simulations to run - recommend &amp;gt; 5,000
N&amp;lt;-120          #Sample size 

for(s in 1:n.sims){
  #browser()
  Time.vals&amp;lt;-rep(0:2, times=N)
  IDs&amp;lt;-rep(1:N, each=3)
  pred.y&amp;lt;-vector()
  for(n in 1:N){
    #Level 1 error
    e_ti&amp;lt;-rnorm(3, 0, 5)
    
    #Level 2 sources of error
    r_0i&amp;lt;-rnorm(1, 0, 5)
    r_1i&amp;lt;-rnorm(1, 0, 2.5)
    
    #Level 1 coefficients 
    pi_0i&amp;lt;-b_00+r_0i
    pi_1i&amp;lt;-b_10+r_1i
    
    #Outcome incoporating level 1 error
    pred.y&amp;lt;-c(pred.y, pi_0i+pi_1i*Time+e_ti)
  }
  
  DF&amp;lt;-data.frame(ID=IDs, 
                 Time=Time.vals, 
                 Y=pred.y)
  
  fit.null&amp;lt;-lme4::lmer(Y~1+(1|ID), DF)
  ICC.vals&amp;lt;-c(ICC.vals, as.numeric(sjstats::icc(fit.null)))
  sd.y&amp;lt;-c(sd.y, sd(pred.y))
  CohensD&amp;lt;-c(CohensD, effsize::cohen.d(c(DF$Y[DF$Time==2], DF$Y[DF$Time==0]), f=rep(c(&amp;#39;T3&amp;#39;, &amp;#39;T1&amp;#39;), each=120))$estimate)
  fit.ucgm&amp;lt;-lme4::lmer(Y~1+Time + (1+Time|ID), data=DF)
  
  boot.ucgm&amp;lt;-bootMer(fit.ucgm, FUN=fixef, type = &amp;#39;parametric&amp;#39;,
                     nsim=1000, parallel = &amp;#39;multicore&amp;#39;, ncpus=12)
  
  #obtaining CIs for intercept
  b00_Est&amp;lt;-c(b00_Est, mean(boot.ucgm$t[,1]))
  b00_boot_se&amp;lt;-c(b00_boot_se, sd(boot.ucgm$t[,1]))
  b00_boot_LB&amp;lt;-c(b00_boot_LB, b00_Est[s]+qt(.975, N-1)*b00_boot_se[s])
  b00_boot_UB&amp;lt;-c(b00_boot_UB, b00_Est[s]+qt(.025, N-1)*b00_boot_se[s])

  #obtaining CIs for time slope
  b10_Est&amp;lt;-c(b10_Est, mean(boot.ucgm$t[,2]))
  b10_boot_se&amp;lt;-c(b10_boot_se, sd(boot.ucgm$t[,2]))
  b10_boot_LB&amp;lt;-c(b10_boot_LB, b10_Est[s]+qt(.975, N-1)*b10_boot_se[s])
  b10_boot_UB&amp;lt;-c(b10_boot_UB, b10_Est[s]+qt(.025, N-1)*b10_boot_se[s])

  #Obtaining estimates of variability in slope and intercept
  b00_var&amp;lt;-c(b00_var, as.numeric(VarCorr(fit.ucgm)$ID[1,1]))
  b10_var&amp;lt;-c(b10_var, as.numeric(VarCorr(fit.ucgm)$ID[2,2]))
  print(paste(s, &amp;#39;out of&amp;#39;, n.sims, &amp;#39;simulations&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I combine and plot the results to get a sense of what sort of power I am going to have at my fixed sample size of 120.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_small&amp;lt;-data.frame(Effect=rep(&amp;#39;Small&amp;#39;, n.sims), 
                         b00_Est=b00_Est, 
                         b00_se=b00_boot_se,
                         b10_Est=b10_Est, 
                         b10_se=b10_boot_se, 
                         b00_var=b00_var, 
                         b10_var=b10_var, 
                         b00_sig=b00_sig, 
                         b10_sig=b10_sig, 
                         sd.y=sd.y, 
                         CohensD=CohensD,
                         ICC=ICC.vals)

mean(dat_small$sd.y)
mean(dat_small$CohensD)
mean(dat_small$ICC.vals)
dat_small$b10_pow.sig&amp;lt;-pt(abs(dat_small$b10_Est/dat_small$b10_se), lower.tail = F, df=N-1)*2
length(dat_small$b10_pow.sig[dat_small$b10_pow.sig&amp;lt;.05])/n.sims

g1&amp;lt;-ggplot(data=DF, aes(x=Time, y=Y, group=ID))+
  geom_line(alpha=.5)
g1

bayesplot::mcmc_areas(dat_small[,6:7], prob=.95)+
  ggtitle(&amp;#39;Variances for Fixed Effects Estimates - Small Effect Model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</guid>
      <description>&lt;p&gt;A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.&lt;/p&gt;
&lt;p&gt;The models below will make use of the preloaded &lt;code&gt;AirPassengers&lt;/code&gt; data in R. The data represent the total number of monthly international airline passengers (in thousands) from 1949 to 1960. It is easy to see these data have both a non-stationary mean and a non-stationary variance. There is also a clear periodic component to these data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;#39;AirPassengers&amp;#39;)
plot(AirPassengers)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/AirPassengers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a toy problem, I am going to focus on the application of a Gaussian process model to forecasting future monthly passengers. This is not the only way one could try to solve this prediction problem. I offer it as a means of understanding the potential power that exists in using these sorts of models for prediction and imputation problems involving univariate time series data.&lt;/p&gt;
&lt;p&gt;A few notes about Gaussian process models. To start they are a class of Bayesian models. There are a few &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; packages that allow researchers to use this modeling approach. I have become something of a &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; convert recently, but it is not the only option out there.&lt;/p&gt;
&lt;p&gt;The authoritative text on Gaussian process models was arguably published by &lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/RW.pdf&#34;&gt;Rasumssen &amp;amp; Williams in 2006&lt;/a&gt;, but only recently have computing power and programming languages allowed for a deeper tapping of this methodology’s strengths. For anyone interested in learning more about these models I highly recommend the Rasmussen &amp;amp; Williams (2006) text as a starting point.&lt;/p&gt;
&lt;p&gt;It is worth pointing out that, because Guassian process models rely on Bayesian estimation, parameters either need to be fixed or given a prior distribution. I like that Bayesian analyses really make you think about your priors. It is the statistical equivalent of eating your veggies. You may not always enjoy it, but it will do you good in the long run. Strategies for choosing priors are beyond the purpose of this post. If interested, the following &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;page on the Stan GitHub repo&lt;/a&gt; provides a brief, but reasonable overview as a starting point.&lt;/p&gt;
&lt;p&gt;First, the data need a bit of prepping to be fed into a Stan program.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Obtaining a numeric vector for time. Maintaining the units of measure at 1 = 1 year
Year&amp;lt;-seq(1949, 1960+11/12, by=1/12)

#converting time-series to a vector
Pass&amp;lt;-as.vector(AirPassengers)

#identifiying number of data points for the &amp;quot;training&amp;quot; data
N1&amp;lt;-length(Year)

#specifying 2-year prediction window. 
year.fore2&amp;lt;-seq(1961, 1962+11/12, by=1/12)
N2&amp;lt;-length(year.fore2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data prepped, I will run the first of two models. The first model relies solely on the squared exponential covariance function (plus error) to define the underlying Gaussian process. The squared exponential function takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;) = \sigma^2 exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_1^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the estimated variance accounted for by the function &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a length scale parameter that governs the decay rate. Smaller estimated values for &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; indicate a faster decay rate in the covariance between two points as a function of time.&lt;/p&gt;
&lt;p&gt;This model, along with its forecasting function are defined in &lt;code&gt;Stan&lt;/code&gt; code below (Adapted from &lt;a href=&#34;http://natelemoine.com/fast-gaussian-process-models-in-stan/&#34;&gt;Nate Lemoine’s code&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real rho1){
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            Sigma[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real r1, 
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, r1);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, r1);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, r1);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; r1;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X,  a1, r1);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ student_t(3,0,1);
    sigma_sq ~ student_t(3,0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, r1, sigma_sq, N1, X, N2, Xp, Y);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model takes just over a minute to run. For those of you who are computational gearheads, here is the hardware I am working with (with a total of 64GB of RAM):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmarkme::get_cpu()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $vendor_id
## [1] &amp;quot;GenuineIntel&amp;quot;
## 
## $model_name
## [1] &amp;quot;Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz&amp;quot;
## 
## $no_of_cores
## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick view of summary stats good convergence of estimates across the 6 chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(&amp;#39;a1&amp;#39;,&amp;#39;r1&amp;#39;,&amp;#39;sigma_sq&amp;#39;, &amp;#39;Ypred&amp;#39;)
summary(fit.stan1, pars=pars.to.monitor[-4])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean      se_mean           sd        2.5%          25%
## a1        2.064974195 2.117451e-02 1.2312158202 0.739681295  1.272435874
## r1       22.681133287 1.252456e-01 9.0005034579 9.102436133 16.045254783
## sigma_sq  0.003559372 5.819530e-06 0.0004251947 0.002818252  0.003254155
##                   50%          75%        97.5%    n_eff      Rhat
## a1        1.759559393  2.505894675  5.176821352 3380.974 1.0010353
## r1       21.367822936 28.012894270 42.692355288 5164.270 0.9996089
## sigma_sq  0.003524852  0.003823097  0.004468209 5338.262 1.0002208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and traceplots demonstrate good mixing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan1, pars=c(&amp;#39;a1&amp;#39;, &amp;#39;r1&amp;#39;, &amp;#39;sigma_sq&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the real question is how did the model do?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not all that great of a prediction to be honest. In this case, the function essentially reduces to a linear regression as there is no place for the periodic nature of the data to be explicitly modeled. This is where the flexibility of Gaussian process models starts to shine as any Gaussian process can be re-expressed as a the sum of an infinite number of Gaussian processes. Here we will add a covariance function that incorporates periodicity. In this case, a period is approximately one year.&lt;/p&gt;
&lt;p&gt;The new periodic covariance function is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;)=\sigma_2^2 exp\Big(-\frac{2sin^2(\pi(t-t&amp;#39;)*1)}{l_2^2}\Big) exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_3^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The inclusion of the squared exponential function here simply reduces the ability of the annual features of the data to explain covariation as the interval between two points grows. Here is the &lt;code&gt;Stan&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real alpha2,
        real rho1,
        real rho2,
        real rho3){
                    matrix[Nx, Ny] K1;
                    matrix[Nx, Ny] K2;
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            K1[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    //specifying random Gaussian process incorporates heart rate
                    for(i in 1:Nx){
                        for(j in 1:Ny){
                            K2[i, j] = alpha2*exp(-2*square(sin(pi()*fabs(x[i]-y[j])*1))/square(rho2))*
                            exp(-square(x[i]-y[j])/2/square(rho3));
                        }
                    }
                        
                    Sigma = K1+K2;
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real a2,
        real r1, 
        real r2,
        real r3,
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, a2, r1, r2, r3);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, a2, r1, r2,  r3);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, a2, r1, r2, r3);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; a2;
    real&amp;lt;lower=15&amp;gt; r1;      //Set after some preliminary modeling
    real&amp;lt;lower=0&amp;gt; r2;
    real&amp;lt;lower=0&amp;gt; r3;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X, a1, a2, r1, r2, r3);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ normal(2.06,1.23);     //Taken from the first model
    a2 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ normal(22.68,9.005);   //Taken from the first model
    r2 ~ student_t(3,0,1);
    r3 ~ student_t(3,0,1);  
    sigma_sq ~ normal(0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, a2, r1, r2, r3, sigma_sq, N1, X, N2, Xp, Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model took about 6 minutes to run.&lt;/p&gt;
&lt;p&gt;Unfortunately, the sampling algorithm that generates the Bayesian estimates is not parallelizable. Until &lt;code&gt;Stan&lt;/code&gt; and &lt;code&gt;rstan&lt;/code&gt; can run using graphics chips’ architecture (which has been buzzed about around the &lt;a href=&#34;http://discourse.mc-stan.org/t/stan-on-the-gpu/326&#34;&gt;Stan ether&lt;/a&gt;), model run time is going to be the biggest downside. Still, 6 minutes is not that long to wait if the model performs well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(paste0(&amp;#39;a&amp;#39;, 1:2), paste0(&amp;#39;r&amp;#39;, 1:3), &amp;#39;Ypred&amp;#39;)
summary(fit.stan2, pars=pars.to.monitor[-6])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean      se_mean          sd        2.5%          25%
## a1  2.54983942 0.0134372272  0.92993236  1.03551274  1.842231419
## a2  0.01028192 0.0007652084  0.02499935  0.00195483  0.003809465
## r1 29.72788580 0.0860626598  5.77288905 19.05829543 25.751944510
## r2  0.71435573 0.0024883370  0.12979765  0.49982165  0.627554970
## r3 21.16693790 0.7029880494 10.71983939  2.13531511 14.836950073
##             50%        75%       97.5%     n_eff     Rhat
## a1  2.464693706  3.1470023  4.60109271 4789.4253 1.001067
## a2  0.005887629  0.0100722  0.04229047 1067.3281 1.004514
## r1 29.510979184 33.4589313 41.57756671 4499.4237 1.000575
## r2  0.700328139  0.7850996  1.00378065 2720.9169 1.000556
## r3 19.220095144 25.4484115 45.91201691  232.5309 1.026833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan2, pars=pars.to.monitor[-6])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The forecast looks to be in line with what I might expect based on trends leading up to 1962. The results are similar to those obtained using a different forecasting technique (i.e., an ARIMA model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- arima(log10(AirPassengers), c(0, 1, 1),
              seasonal = list(order = c(0, 1, 1), period = 12)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12))
## 
## Coefficients:
##           ma1     sma1
##       -0.4018  -0.5569
## s.e.   0.0896   0.0731
## 
## sigma^2 estimated as 0.0002543:  log likelihood = 353.96,  aic = -701.92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update(fit, method = &amp;quot;CSS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12), method = &amp;quot;CSS&amp;quot;)
## 
## Coefficients:
##           ma1     sma1
##       -0.3772  -0.5724
## s.e.   0.0883   0.0704
## 
## sigma^2 estimated as 0.0002619:  part log likelihood = 354.32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred &amp;lt;- predict(fit, n.ahead = 24)
tl &amp;lt;- pred$pred - 1.96 * pred$se
tu &amp;lt;- pred$pred + 1.96 * pred$se

ARIMA.for&amp;lt;-data.frame(Year=year.fore2, Passengers=10^pred$pred, UB=10^as.numeric(tu), LB=10^as.numeric(tl))

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;blue&amp;#39;)+
  geom_ribbon(data=ARIMA.for, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;red&amp;#39;)+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers, color=&amp;#39;blue&amp;#39;), lwd=1.25)+
  geom_line(data=ARIMA.for, aes(x=Year, y=Passengers, color=&amp;#39;red&amp;#39;), lwd=1.25)+
  coord_cartesian(xlim=c(1960, 1963.25), ylim= c(275,1000))+
  scale_color_manual(name=&amp;#39;Forecast Model&amp;#39;,
                     values = c(&amp;#39;blue&amp;#39;, &amp;#39;red&amp;#39;), 
                     labels = c(&amp;#39;Gaussian Process&amp;#39;, &amp;#39;ARIMA&amp;#39;))
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
The two models make fairly similar predictions for 1961 (shaded regions represent respective 95% intervals). In 1962, the ARIMA model is a little more bullish than the Gaussian process model on airline passengers.&lt;/p&gt;
&lt;p&gt;Still, it is impossible to know which of these models is better, a methodological question I may tackle in greater detail when I have some time. The answer is almost certainly “it depends.” For now, the main takeaway is that Gaussian process models may represent a useful approach to the age-old problems of forecasting and imputation, a fact I plan to exploit in some of my signal processing work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dispositional negativity in the wild: Social environment governs momentary emotional experience</title>
      <link>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpersonal predictors of stress generation: Is there a super factor?</title>
      <link>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neuroticism and conscientiousness as moderators of the relation between social withdrawal and internalizing problems in adolescence</title>
      <link>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shyness, preference for solitude and adolescent internalizing: The roles of maternal, paternal, and best-friend support</title>
      <link>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Callous Unemotional Meta-Analysis</title>
      <link>https://mgb-research.netlify.com/project/cu-meta/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/cu-meta/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Callous-unemotional (CU) traits are central to our understanding of antisocial behavior, which is harmful, financially costly to society, and hard to treat. Theoretical and diagnostic models define CU traits based on a lack of empathy, guilt, and prosociality. However, there has been no comprehensive quantitative synthesis of results to support this claim, nor any examination of whether associations differ by age, gender, sample type, or informant. Given its centrality to our understanding and treatment of antisocial behavior, the time is ripe for a critical examination of the meaning of the CU traits construct, including associations with its purported underlying features of low empathy, guilt, and prosociality, and potential demographic and methodological moderators of these associations. To address this gap in the literature, we conducted a systematic search of the literature on CU traits and identified 82 effect sizes from 51 studies (&lt;em&gt;N&lt;/em&gt; = 30,475) that quantified the magnitude of the association between CU traits and either empathy, prosociality, or guilt. The results revealed statistically significant, modest-to-moderate negative correlations between CU traits and empathy (&lt;em&gt;k&lt;/em&gt; = 25, $\rho$  = -.21), CU traits and guilt (&lt;em&gt;k&lt;/em&gt; = 3, $\rho$ = -.18), and CU traits and prosociality (&lt;em&gt;k&lt;/em&gt; = 16, $\rho$ = -.28). The magnitude of the negative association between CU traits and prosociality was stronger among younger children. In addition, the negative correlations between CU traits and both prosociality and cognitive empathy were stronger when the informant was a parent or teacher. We discuss how these findings can inform theory, conceptualization, and measurement of CU traits across the lifespan, as well as implications for models of antisocial behavior and moral development.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation Models</title>
      <link>https://mgb-research.netlify.com/project/gp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/gp/</guid>
      <description>&lt;p&gt;I have become increasingly interested in the use of Gaussian process models as a tool for forecasting and imputation with univariate time series. See my &lt;a href=&#34;https://mgb-research.netlify.com/post/gaussian-process-imputation-models/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; on the topic for more information and a toy forecasting problem.&lt;/p&gt;

&lt;p&gt;I am currently working on a study examining the effectiveness of these models in imputing heart rate data that has been severely corrupted by artifact - to the point that there is not &amp;ldquo;enough&amp;rdquo; true signal remaining to do much of anything with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBI VizEdit</title>
      <link>https://mgb-research.netlify.com/project/ibi-vizedit/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/ibi-vizedit/</guid>
      <description>&lt;p&gt;IBI VizEdit is a program built using RShiny. It is designed to assist in the manual editing of inter-beat interval files that are derived from photoplethysmogram (PPG) recordings. Unlike the electrocardiogram signal (EKG or ECG), PPG signals are characterized by a slow-moving waveform, which presents a different set of challenges when the true signal becomes corrupted by motion artefacts and other sources of noise.&lt;/p&gt;

&lt;p&gt;Though increasingly popular due to their ease of use, most heart rate editing software that exists to date was designed and optimized for the detection and editing of inter-beat interval files derived from ECG signals. IBI VizEdit provides a new suite of tools for researchers who find themselves working with messy PPG files.&lt;/p&gt;

&lt;p&gt;Please note that IBI VizEdit is beta software. It has not been fully tested, and there are likely numerous bugs and opportunities to optimize code and performance. Any and all feedback is welcome.&lt;/p&gt;

&lt;p&gt;As of right now, IBI VizEdit is only supported for use on Windows 7/8/10 and Linux (Ubuntu 16.04 in particular).&lt;/p&gt;

&lt;p&gt;Please cite as:&lt;/p&gt;

&lt;p&gt;Barstead, M. G. (2018). IBI VizEdit v.1.2-beta: An RShiny Application [Computer software]. University of Maryland. doi: 10.5281/zenodo.1209474&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Individual Risk for Anxiety and Depression</title>
      <link>https://mgb-research.netlify.com/project/dn_research/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/dn_research/</guid>
      <description>&lt;p&gt;Cell phones are ubiquitous on college campuses, annoying many an instructor who looks back at a classroom filled with young adults enamored with their black mirrors (and yes I am still very impressed with the underlying meaning of the hit Netflix series&amp;rsquo; title). As others have in the past &lt;a href=&#34;https://psyc.umd.edu/facultyprofile/Shackman/Alexander&#34; target=&#34;_blank&#34;&gt;Dr. Alex Shackman&lt;/a&gt;, sought to use the constant presence of cell phones to better understand the daily lives of individuals.&lt;/p&gt;

&lt;p&gt;Using college students as subjects is often decried as laziness and viewed somewhat skeptically as investigators reaching out to a subject pool that is readily available to them. There is definitely some merit to these concerns; however, in the present line of work, university students are an ideal group to study. For one, the transition to college can be stressful, particularly for individuals at risk for mental health disorders such as anxiety and depression. Furthermore, understanding the risks in this population is timely as we continue to see &lt;a href=&#34;https://adaa.org/living-with-anxiety/college-students#&#34; target=&#34;_blank&#34;&gt;increases in mental health problems&lt;/a&gt; among this group. Finally, young adulthood is a period when many will experience their first major bouts of anxiety and depression (though of course many will struggle with symptoms earlier in adolescence as well) and may therefore represent an important period to intervene, preventing subsequent maladaptive cascades related to mood disorders.&lt;/p&gt;

&lt;p&gt;Successfully negotiating the college transition has important consequences for future outcomes including gaining full-time employment, marriage, and individuals&amp;rsquo; long-term mental and physical health. As part of a broader program of research evaluating the neurobiological factors that predict social, emotional, and academic success during this period Dr. Shackman designed a series of studies using ecological momentary assessments. The jargony term boils down to participants receiving multiple daily surveys (delivered via cell phone) querying about their recent activities, their social surroundings, and their current mood.&lt;/p&gt;

&lt;p&gt;The central premise motivating this work is that if we can better understand the daily lives of individuals at risk for the development of psychopathology, we can gain some insight about how to prevent it. We have published some &lt;a href=&#34;https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/&#34; target=&#34;_blank&#34;&gt;initial work&lt;/a&gt; with these data demonstrating that interacting with close others may be especially beneficial for college students at increased risk for internalizing disorders. We are currently in the process of replicating and extending this work, examining more nuanced features of momentary affective responses to daily events.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preventing Anxiety in Early Childhood</title>
      <link>https://mgb-research.netlify.com/project/turtlemd/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/turtlemd/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://education.umd.edu/directory/kenneth-rubin&#34; target=&#34;_blank&#34;&gt;Dr. Ken Rubin&lt;/a&gt; has spent the majority of his career studying the causes, correlates, and consequences of childhood anxiety and social withdrawal. As a result of an ongoing collaboration with &lt;a href=&#34;https://psyc.umd.edu/facultyprofile/Chronis-Tuscano/Andrea&#34; target=&#34;_blank&#34;&gt;Dr. Andrea Chronis-Tuscano&lt;/a&gt;, a clinical psychology professor and researcher, and in consultation with &lt;a href=&#34;https://carleton.ca/psychology/people/robert-coplan/&#34; target=&#34;_blank&#34;&gt;Dr. Rob Coplan&lt;/a&gt;, this career has recently been translated into the creation of an early intervention program. The intervention, named the &lt;em&gt;Turtle Program&lt;/em&gt;, simultaneously targets maladaptive parenting behaviors known to maintain and exacerbate childhood anxiety in addition to anxious and inhibited children&amp;rsquo;s social skills and emotion regulation deficits.&lt;/p&gt;

&lt;p&gt;Pilot work comparing the treatment against a waitlist control group demonstrated promising results insofar as &lt;a href=&#34;http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039043&#34; target=&#34;_blank&#34;&gt;reducing anxiety symptoms&lt;/a&gt; and &lt;a href=&#34;https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/&#34; target=&#34;_blank&#34;&gt;improving adaptive social behaviors&lt;/a&gt;. As of June 2018, we have completed initial data collection for the follow-up investigation comparing the &lt;em&gt;Turtle Program&lt;/em&gt; against an established parent psychoeducation intervention for shy and inhibited children (known as &lt;a href=&#34;https://coollittlekids.org.au/login&#34; target=&#34;_blank&#34;&gt;Cool Little Kids&lt;/a&gt;). While we continued to gather information about children and parents a year after participating in one of the two interventions, we have just started to compile our initial findings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Withdrawal Meta-Analysis</title>
      <link>https://mgb-research.netlify.com/project/sw-meta/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/project/sw-meta/</guid>
      <description>&lt;p&gt;Are the terms anxious withdrawal, anxious solitude, behavioral inhibition, shyness-sensitivity, interchangeable? Are peer, parent, teacher, and self-perceptions of social withdrawal measuring the same thing? To date, researchers have relied on narrative arguments to build a case for or againts correspondence among the terms and measurement approaches used by developmental scientists interested in these and related constructs. I want to let the data speak for itself, and I am conducting a meta-analysis to do just that.&lt;/p&gt;

&lt;p&gt;I conducted a preliminary analysis using data from 101 studies. I have since gone back and completed a more comprehensive record search and am in the process of updating my models. For a early preview of the results, check out the pre-print here on my &lt;a href=&#34;https://www.researchgate.net/publication/324746193_Psychosocial_Correlates_of_Social_Withdrawal_and_Its_Many_Variants_A_Quantitative_Synthesis_of_Research_Spanning_Four_Decades&#34; target=&#34;_blank&#34;&gt;ResearchGate page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
