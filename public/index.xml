<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dead Reckoning Analytics and Consulting</title>
    <link>/</link>
    <description>Recent content on Dead Reckoning Analytics and Consulting</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data: Complete Work Flow - Part 1 of 3</title>
      <link>/blog/2018/07/05/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07/05/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow/</guid>
      <description>Overview: This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for missingness. The third and final post will be a walkthrough of the final models and their interpretation - including a comparison of the same models using listwise deletion (which is bad unless missingness is small or definitely, 100% completely at random).</description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>/blog/2018/06/07/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06/07/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. Having already collected data (i.e., having a fixed sample size), the goal of the power analysis was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (\(\alpha = .05\)) for a secondary research question that was not part of the original proposal (we added collection of a set of variables partway through the data collection period).</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>/blog/2018/05/21/2018-05-21-gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05/21/2018-05-21-gaussian-process-imputation-models/</guid>
      <description>A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signalâ€™s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/</guid>
      <description>Here to help I derive a great deal of satisfaction from helping others achieve their data and analysis goals. Please reach out if you have questions about any of open-source products I have developed, or if you are interested in retaining my consulting services for a custom project.</description>
    </item>
    
    <item>
      <title>Dashboards &amp; Reports</title>
      <link>/dashboards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/dashboards/</guid>
      <description>A Picture Graph is Worth a Thousand Words Spreadsheets It is hard to understate the value of effective data visualization. It also hard to overstate how easy it is to be tricked (or to trick ourselves) with poorly designed graphics that may not be communicating the true properties of the data. In our work at Dead Reckoning graphical representations of data primarily perform at least one of the following roles: exploratory analysis, data summary, or data-based decision-making.</description>
    </item>
    
    <item>
      <title>Data Pipelines</title>
      <link>/pipelines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/pipelines/</guid>
      <description>Drilling for Data: The New Black Gold In the age of Big Data (an overused but accurate term), few resources contain more inherent value that the information populating servers and hard drives around the world. Six of the top seven companies in the world (as measured by overall market value) are technology companies, and make no mistake that one enormous competitive advantage these companies enjoy is the ability to obtain, store, and ultimately navigate literal oceans of data.</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/faq/</guid>
      <description>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.
1. WHAT TO DO IF I HAVE STILL NOT RECEIVED THE ORDER? Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante.</description>
    </item>
    
    <item>
      <title>Meaning Behind the Name Dead Reckoning</title>
      <link>/background/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/background/</guid>
      <description>Dead Reckoning: A Framework for Data Science Before satellites could accurately estimate the global position of an object within a few meters, sailors needed a way to determine where they were in a vast, remote, and otherwise unremarkable seascape. Their answer to the problem was data science, an early version of the practice to be sure, but data science nonetheless. They did not have R or Python scripts running code in the background, but they did develop specialized computers.</description>
    </item>
    
    <item>
      <title>Predictive Analytics</title>
      <link>/predictive-analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/predictive-analytics/</guid>
      <description>Predictive Analytics and Data-Driven Decision-Making For data science nerds like myself, the modeling step is often the most fun, in part because it is represents the most tangible value-add deliverable in the data science life cycle. It is also the point at which some certainty regarding decision opportunities begins to emerge. Lastly, it is the stage at which all of the hard work setting up pipelines, cleaning and transforming data, generating features, and performing exploratory analyses begins to pay off.</description>
    </item>
    
  </channel>
</rss>