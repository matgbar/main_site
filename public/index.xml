<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog on Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog</title>
    <link>https://mgb-research.netlify.com/</link>
    <description>Recent content in Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog on Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Matthew Barstead</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 2 of 3)</title>
      <link>https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</guid>
      <description>&lt;p&gt;In the first post in this series, I described the impetus for this trek through statistical modeling, machine learning and artificial intelligence. I also provided an initial set of comparisons for three different approaches to classification: &lt;em&gt;k&lt;/em&gt;-means, &lt;em&gt;k&lt;/em&gt;-nearest neighbor, and latent profile analysis (model-based clustering). If you want to check those mini-walkthroughs out &lt;a href=&#34;https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a reminder, my goal here is to compare and contrast different approaches to data analysis and predictive modeling that are, in my mind, arbitrarily lumped into statistical modeling and machine learing/artificial intelligence categories. Though there may be some good reasons for dichotomizing statistics and machine learning, it has been my experience that each camp that favors one conceptualization over the other (i.e., statisticians vs. data scientists) holds unnecessary comtempt for the other group. On my shelf are textbooks that refer to linear regression models as supervised machine learning techniques &lt;span class=&#34;citation&#34;&gt;(e.g., Zumel and Mount 2014)&lt;/span&gt; and statistical models &lt;span class=&#34;citation&#34;&gt;(e.g., Gelman and Hill 2007)&lt;/span&gt;. I promise that the ordinary least squares solution, regardless of how you categorize the technique, is going to return the same result given the same model and same data each time.&lt;/p&gt;
&lt;p&gt;To remain consistent with the &lt;a href=&#34;https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/&#34;&gt;first post&lt;/a&gt; in this series, I am going to continue with the &lt;code&gt;iris&lt;/code&gt; data set for now. The data set contains four measurements of plant anatomy: petal length, petal width, sepal length, and sepal width. These measures were collected on 50 specimens from three species for a total of 150 observations. The goal will be the same, continue to assess the accuracy of different classification techniques in predicting class membership using these data.&lt;/p&gt;
&lt;p&gt;As opposed to the first post, each of the techniques I will employ in this post allow for classification of new data that a trained model has yet to “see.” The ability to predict out-of-sample data allows for a more applied series of testing scenarios. Specifically, I’ll evaluate each technique/model in terms of prediction accuracy using a &lt;em&gt;k&lt;/em&gt;-fold cross validation technique.&lt;/p&gt;
&lt;p&gt;The models covered in this post will include: naive Bayes classification, discriminant analysis, and multinomial logistic regression. The goal of each model is essentially the same - develop a model (or a series of mathematical rules if you prefer) that maximally predicts observed data and can be used to predict out of sample data (i.e., generalize to a population if you prefer).&lt;/p&gt;
&lt;div id=&#34;naive-bayes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Bayesian models have at their root a simple mathematical truism about the behavior of the probability of independent events. Bayes Theorem elegantly lays this truism out:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; given the value of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is equal to the probability of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; times the probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; divided by the probability of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;. For those of you who know your way around a cross-tabulated table, these statements may seem pretty obvious. Let’s use some made up data to make it clearer though. Let’s model cancer risk as a function of smoking. I’ll pretend I was able to randomly sample 500 individuals from a population in which smokers are approximately 4 times more likely to receive a cancer diagnosis during their lifetime than non-smokers (&lt;em&gt;Note&lt;/em&gt; I am just making these data up; I have no idea what appropriate base-rates should be). I am going to assume &lt;span class=&#34;math inline&#34;&gt;\(P(C|S)=.48\)&lt;/span&gt; (that is the probability of cancer given smoking) and &lt;span class=&#34;math inline&#34;&gt;\(P(C|NS)=.19\)&lt;/span&gt; (that is the probability of cancer given not-smoking).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;P_C_sm&amp;lt;-.48
P_NC_sm&amp;lt;-1-P_C_sm
odds_C_sm&amp;lt;-P_C_sm/P_NC_sm

P_C_nsm&amp;lt;-.19
P_NC_nsm&amp;lt;-1-P_C_nsm
odds_C_nsm&amp;lt;-P_C_nsm/P_NC_nsm

odds_C_sm/odds_C_nsm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.935223&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An odds ratio of 3.94 would seem to confirm my fictional probabilities result in the scenario I wanted (i.e., smokers are 4x as likely to receive a cancer diagnosis). Now it is time to generate a random sample of each subpopulation. Let’s assume that smoking rates are relatively low in this population, say 15%. Since I am randomly sampling, I would expect that my final sample yields something close to this rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(143)
Smoker&amp;lt;-sample(c(&amp;#39;Smoker&amp;#39;, &amp;#39;Non-Smoker&amp;#39;), 
               prob=c(.15, .85), 
               size=500, 
               replace = TRUE)

Cancer&amp;lt;-vector()
for(i in 1:length(Smoker)){
  if(Smoker[i]==&amp;#39;Smoker&amp;#39;){
    Cancer[i]&amp;lt;-sample(c(&amp;#39;Cancer&amp;#39;, &amp;#39;No Cancer&amp;#39;),
                      prob = c(P_C_sm, P_NC_sm), 
                      size = 1)
  }
  else{
    Cancer[i]&amp;lt;-sample(c(&amp;#39;Cancer&amp;#39;, &amp;#39;No Cancer&amp;#39;),
                      prob = c(P_C_nsm, P_NC_nsm), 
                      size = 1)
  }
}

DF&amp;lt;-data.frame(Smoker=Smoker, 
               Cancer=Cancer)
table(DF)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Cancer
## Smoker       Cancer No Cancer
##   Non-Smoker     80       343
##   Smoker         33        44&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So I now have a simple data set and I can, using Bayes Theorem calculating a probability of having cancer given the fact that an individual is a smoker &lt;span class=&#34;math inline&#34;&gt;\(P(C|S)\)&lt;/span&gt;. We’ll just need some numbers to plug in. First some easy values, &lt;span class=&#34;math inline&#34;&gt;\(P(C)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(S)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(S) = \frac{33+44}{500} = .154\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(C) = \frac{80+33}{500} = .226\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, we need to get the probability of smoking, given cancer or &lt;span class=&#34;math inline&#34;&gt;\(P(S|C)\)&lt;/span&gt;, which is just the same as saying, what is the probability of being a smoker, solely among individuals with cancer.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(S|C) = \frac{33}{80+33}=.292\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So now plugging this information into Bayes Theorem we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(C|S) = \frac{P(S|C)P(C)}{P(S)} = \frac{.292*.226}{.154}=.429\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the basic machinery of all Bayesian models, the naive Bayes classifier included. Accoring to these calculations, even smokers would not be more likely to be diagnosed with cancer relative to other smokers, so using a naive Bayes classifier would not likely provide much additional predictive utility in this case. However, if we had additional predictors, we could model the outcome as a function of a a series of conditional joint probabilites, making finer grade distinctions about the posterior probability of our outcome variable.&lt;/p&gt;
&lt;p&gt;With this brief discussion of Bayes Theorem and probability behind us, we can introduce Naive Bayes classifiers, which can handle mutinomial outcome variables (i.e., multiple categories) and continuous predictors or categorical predictors. The continuous predictors work best when approximately normal in their distribution as the probabilities used in the model are derived from the probability density function of the normal distrbution. These models calculate the conditional probability that a given case is in&lt;/p&gt;
&lt;p&gt;To compare the performance of the Naive Bayes classifier in accurately predicting out-of-sample cases, I’ll be using a &lt;em&gt;k&lt;/em&gt;-fold cross validation technique in which I will hold out a randomly selected 20% of the cases from the &lt;code&gt;iris&lt;/code&gt; data set as a testing sample, train the model on the remaining 80% and aggregate the prediction accuracy across on the testing and training set on 100 trials. With a small data set, this is a more effective technique to evaluate and compare model prediction performance than a single test/train split that could easily over- or underestimate out-of-sample prediction accuracy due to random chance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
set.seed(321)
k.fold&amp;lt;-100
library(e1071)

#Setting up a series of vectors for tracking accuracy
Start&amp;lt;-Sys.time()
Accuracy&amp;lt;-vector()
Model&amp;lt;-vector()
Pred_type&amp;lt;-vector()
for(i in 1:k.fold){
  #Split data into training/testing set
  smpl_size&amp;lt;-floor(.8*nrow(iris))
  ind &amp;lt;- sample(seq_len(nrow(iris)), size = smpl_size)
  train&amp;lt;-iris[ind, ]
  test&amp;lt;-iris[-ind, ]
  
  #Train the model and obtain accuracy based on observed data
  NB_train&amp;lt;-naiveBayes(Species~., data=train)
  NB_train_pred&amp;lt;-predict(NB_train, newdata = train)
  tab_train&amp;lt;-table(NB_train_pred, train$Species)
  Train_acc&amp;lt;-sum(diag(tab_train))/nrow(train)
  Accuracy&amp;lt;-c(Accuracy, Train_acc)
  Model&amp;lt;-c(Model, &amp;#39;Naive Bayes&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Training Set&amp;#39;)
  
  #Test model and get out of sample accuracy
  NB_test_pred&amp;lt;-predict(NB_train, newdata = test)
  tab_test&amp;lt;-table(NB_test_pred, test$Species)
  Test_acc&amp;lt;-sum(diag(tab_test))/nrow(test)
  Accuracy&amp;lt;-c(Accuracy, Test_acc)
  Model&amp;lt;-c(Model, &amp;#39;Naive Bayes&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Testing Set&amp;#39;)
}
round(Sys.time()-Start, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 1.67 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So not too long of a wait time to run through the cross-validation at 100 folds on my personal laptop.&lt;/p&gt;
&lt;p&gt;Now, for the overall results of the model in terms of accurate predictions. I always prefer to show, my data whenever possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NB_KFold&amp;lt;-data.frame(Accuracy = Accuracy, 
                     Model = Model, 
                     Pred_type = Pred_type)

g1&amp;lt;-ggplot(data=NB_KFold, aes(x=Pred_type, y=Accuracy))+
  geom_bar(aes(fill=Pred_type), 
           alpha=.5, 
           stat=&amp;#39;summary&amp;#39;, 
           fun.y=&amp;#39;mean&amp;#39;)+
  geom_point(aes(color=Pred_type), 
             position = position_jitter(w = 0.05, h = 0))+
  coord_cartesian(ylim=c(.70, 1))+
  guides(fill=guide_legend(title=&amp;quot;&amp;quot;), 
         color=guide_legend(title = &amp;quot;&amp;quot;))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 1, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(NB_KFold$Accuracy[Pred_type==&amp;#39;Testing Set&amp;#39;]), 
                                         digits = 4)))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 2, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(NB_KFold$Accuracy[Pred_type==&amp;#39;Training Set&amp;#39;]), 
                                         digits = 4)))+
  theme_bw()

g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-12-03-a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3_files/figure-html/NB4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, there is a slight dropoff in the accuracy of the model predictions when tested against out-of-sample data, but it really is slight. Prediction accuracy does show considerable range across the testing sets, however. Had I randomly selected one of the testing data sets that performs just above 80%, I would have likely come to a very different conclusion about the effectivenes of the model in predicting “unobserved” data. The amount of variability in accurate prediction performance across the different training/testing runs is a secondary property of the models to bear in mind when making comparisons related to model effectiveness.&lt;/p&gt;
&lt;p&gt;For completeness and to allow a more direct comparison with the techniques described in &lt;a href=&#34;https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/&#34;&gt;the previous post in this series&lt;/a&gt;, below are the prediction results and confusin matrix for the entire &lt;code&gt;iris&lt;/code&gt; sample using a Naive Bayes classifier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NB_full&amp;lt;-naiveBayes(Species~., data = iris)
NB_full_pred&amp;lt;-predict(NB_full, newdata=iris)
table(NB_full_pred, iris$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
## NB_full_pred setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         47         3
##   virginica       0          3        47&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt; the number of errors presented in the confusion matrix above is still higher than the classification model presented &lt;a href=&#34;https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/&#34;&gt;at the end of the previous post&lt;/a&gt; (6 misclassified versus only 3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-discriminant-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;For individuals familiar with support vector machines, a linear discriminant analysis can be thought of as a special restrictive case of SVMs. Using a combination of linear functions, the model attempts to maximally separate the observed data into its constituent classifications or groupings. The discriminant function or functions represent linear combinations of the predictor variables, and as such have some similarities to principal components analyses as well. However, linear discriminant analyses are trying to categorize &lt;em&gt;cases&lt;/em&gt; into groups, whereas principal components analyses are attempting to form clusters of &lt;em&gt;variables&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Implementing a discriminant analysis is fairly straightforward. Borrowing from the code above, I’ll use the same cross-validation technique to assess model prediction performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
set.seed(321)
k.fold&amp;lt;-100
library(MASS)

#Setting up a series of vectors for tracking accuracy
Start&amp;lt;-Sys.time()
Accuracy&amp;lt;-vector()
Model&amp;lt;-vector()
Pred_type&amp;lt;-vector()
for(i in 1:k.fold){
  #Split data into training/testing set
  smpl_size&amp;lt;-floor(.8*nrow(iris))
  ind &amp;lt;- sample(seq_len(nrow(iris)), size = smpl_size)
  train&amp;lt;-iris[ind, ]
  test&amp;lt;-iris[-ind, ]
  
  #Train the model and obtain accuracy based on observed data
  LDA_train&amp;lt;-lda(Species~., data=train)
  #note the inclusion of $class here - the way an lda object is stored
  LDA_train_pred&amp;lt;-predict(LDA_train, newdata = train)$class
  tab_train&amp;lt;-table(LDA_train_pred, train$Species)
  Train_acc&amp;lt;-sum(diag(tab_train))/nrow(train)
  Accuracy&amp;lt;-c(Accuracy, Train_acc)
  Model&amp;lt;-c(Model, &amp;#39;LDA&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Training Set&amp;#39;)
  
  #Test model and get out of sample accuracy
  LDA_test_pred&amp;lt;-predict(LDA_train, newdata = test)$class
  tab_test&amp;lt;-table(LDA_test_pred, test$Species)
  Test_acc&amp;lt;-sum(diag(tab_test))/nrow(test)
  Accuracy&amp;lt;-c(Accuracy, Test_acc)
  Model&amp;lt;-c(Model, &amp;#39;LDA&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Testing Set&amp;#39;)
}
round(Sys.time()-Start, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.35 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, the the total run-time is not that long for the 100 cross-validation runs. Now let’s plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LDA_KFold&amp;lt;-data.frame(Accuracy = Accuracy, 
                     Model = Model, 
                     Pred_type = Pred_type)

g1&amp;lt;-ggplot(data=LDA_KFold, aes(x=Pred_type, y=Accuracy))+
  geom_bar(aes(fill=Pred_type), 
           alpha=.5, 
           stat=&amp;#39;summary&amp;#39;, 
           fun.y=&amp;#39;mean&amp;#39;)+
  geom_point(aes(color=Pred_type), 
             position = position_jitter(w = 0.05, h = 0))+
  coord_cartesian(ylim=c(.70, 1))+
  guides(fill=guide_legend(title=&amp;quot;&amp;quot;), 
         color=guide_legend(title = &amp;quot;&amp;quot;))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 1, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(LDA_KFold$Accuracy[Pred_type==&amp;#39;Testing Set&amp;#39;]), 
                                         digits = 4)))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 2, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(LDA_KFold$Accuracy[Pred_type==&amp;#39;Training Set&amp;#39;]), 
                                         digits = 4)))+
  theme_bw()

g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-12-03-a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3_files/figure-html/LDA2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, whether you believe this model is a statistical model or a machine learning model, it is clear that it outperforms the Naive Bayes classifier, making more accurate in- and out-of-sample predictions in the current data set. The variance in prediction accuracy is also much smaller with this modeling approach. As with the Naive Bayes model, I included, the complete in-sample confusion matrix using a linear discriminant analysis below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LDA_full&amp;lt;-lda(Species~., data=iris)
LDA_full_pred&amp;lt;-predict(LDA_full)$class
table(LDA_full_pred, iris$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              
## LDA_full_pred setosa versicolor virginica
##    setosa         50          0         0
##    versicolor      0         48         1
##    virginica       0          2        49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern of errors is slightly different than the mixture-model discriminant analysis at the end of the &lt;a href=&#34;https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/&#34;&gt;first post&lt;/a&gt;, but the overall in-sample results are the same. Each techniqued resulted in a total of 3 misclassifications.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multinomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multinomial logistic regression&lt;/h2&gt;
&lt;p&gt;I will be switching up the data sets in the next post so that I can compare some more traditional machine learning/artifical intelligence approaches to basic statistical models, and when I do, I will use a binomial logistic regression as one of the models. I only say that to highlight the fact that I am presenting a multinomial logistic regression (which is essentially a series of binomial logistic regressions) before I provide information about logistic regression foundations, which I will detail more in the next post (I’ll add a link here once it is complete).&lt;/p&gt;
&lt;p&gt;For now, know that a multinomial logistic regression is an extension of binomial or binary logistic regression. These models attempt to predict the log odds of an event occurring (also known as the logits).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log\Big(\frac{P(y)}{1-P(y)}\Big)=\beta_0+\beta_1X_1+\beta_2X_2+...\beta_pX_p+\epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This link function is one of several that form the basis of what are referred to as generalized linear models. This class of models allows analysts to predict, via linear combinations of predictors (as is the case with linear regression), outcomes that do not result in normal distributions of model-based residuals. Normality of residuals is a statistical property assumed by ordinary least squares regression, which works through minimizing the total error.&lt;/p&gt;
&lt;p&gt;Using a special case of a generalized linear model, the multinomial logistic regression, let’s go ahead and attempt to predict &lt;code&gt;Species&lt;/code&gt; as we have done so many times before. A quick note, the function I am using to create a logistic regression model comes from the &lt;code&gt;nnet&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
set.seed(321)
k.fold&amp;lt;-100
library(nnet)

#Setting up a series of vectors for tracking accuracy
Start&amp;lt;-Sys.time()
Accuracy&amp;lt;-vector()
Model&amp;lt;-vector()
Pred_type&amp;lt;-vector()
for(i in 1:k.fold){
  #Split data into training/testing set
  smpl_size&amp;lt;-floor(.8*nrow(iris))
  ind &amp;lt;- sample(seq_len(nrow(iris)), size = smpl_size)
  train&amp;lt;-iris[ind, ]
  test&amp;lt;-iris[-ind, ]
  
  #Train the model and obtain accuracy based on observed data
  MLR_train&amp;lt;-multinom(Species~., data=train, trace=FALSE)
  #note the inclusion of $class here - the way an lda object is stored
  MLR_train_pred&amp;lt;-predict(MLR_train, newdata = train)
  tab_train&amp;lt;-table(MLR_train_pred, train$Species)
  Train_acc&amp;lt;-sum(diag(tab_train))/nrow(train)
  Accuracy&amp;lt;-c(Accuracy, Train_acc)
  Model&amp;lt;-c(Model, &amp;#39;MLR&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Training Set&amp;#39;)
  
  #Test model and get out of sample accuracy
  MLR_test_pred&amp;lt;-predict(MLR_train, newdata = test)
  tab_test&amp;lt;-table(MLR_test_pred, test$Species)
  Test_acc&amp;lt;-sum(diag(tab_test))/nrow(test)
  Accuracy&amp;lt;-c(Accuracy, Test_acc)
  Model&amp;lt;-c(Model, &amp;#39;MLR&amp;#39;)
  Pred_type&amp;lt;-c(Pred_type, &amp;#39;Testing Set&amp;#39;)
}
round(Sys.time()-Start, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.77 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again a fairly quick run time (by the way I keep saying this because I am used to waiting a few hours for multilevel Bayesian regression models with correlated random effects to coverge. Waiting a second or two to get aggregated results is nothing).&lt;/p&gt;
&lt;p&gt;Let’s see the prediction results…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MLR_KFold&amp;lt;-data.frame(Accuracy = Accuracy, 
                     Model = Model, 
                     Pred_type = Pred_type)

g1&amp;lt;-ggplot(data=MLR_KFold, aes(x=Pred_type, y=Accuracy))+
  geom_bar(aes(fill=Pred_type), 
           alpha=.5, 
           stat=&amp;#39;summary&amp;#39;, 
           fun.y=&amp;#39;mean&amp;#39;)+
  geom_point(aes(color=Pred_type), 
             position = position_jitter(w = 0.05, h = 0))+
  coord_cartesian(ylim=c(.70, 1))+
  guides(fill=guide_legend(title=&amp;quot;&amp;quot;), 
         color=guide_legend(title = &amp;quot;&amp;quot;))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 1, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(MLR_KFold$Accuracy[Pred_type==&amp;#39;Testing Set&amp;#39;]), 
                                         digits = 4)))+
  annotate(geom=&amp;#39;text&amp;#39;, x = 2, y = .75, 
           label = paste(&amp;#39;Mean =&amp;#39;, 
                         round(mean(MLR_KFold$Accuracy[Pred_type==&amp;#39;Training Set&amp;#39;]), 
                                         digits = 4)))+
  theme_bw()

g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-12-03-a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3_files/figure-html/MLR2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, compared to the Naive Bayes model, the multinomial logistic regression performs better in-sample. However, there is a larger dropoff in this model’s in-sample versus out-of-sample predictive accuracy. Additionally, the results indicate that cross-validated variability in predictive accuracy is about as variable as the Naive Bayes model. If I were considering these three models for a prediction task related to these data, so far the linear discriminant model is the best.&lt;/p&gt;
&lt;p&gt;The next post will delve into additional modeling approaches using a new dataset. Where possible these same models covered using the &lt;code&gt;iris&lt;/code&gt; data set will also be applied to compare their predictive performance under the same set of data conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Gelman2007&#34;&gt;
&lt;p&gt;Gelman, Andrew, and Jennifer Hill. 2007. &lt;em&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models&lt;/em&gt;. Cambridge Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Zumel2014&#34;&gt;
&lt;p&gt;Zumel, Nina, and John Mount. 2014. &lt;em&gt;Practical Data Science with R&lt;/em&gt;. Manning Publications.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 1 of 3)</title>
      <link>https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/</guid>
      <description>&lt;p&gt;I was recently interviewing for a job and a recruiter asked me if I wanted to enhance aspects of my machine learning background on my resume before she passed it on for the next round of reviews. I resisted the urge to chide her in the moment by pointing out the flawed distinction between statistics and machine learning, an unnecessary admonishment that would have been to no one’s benefit. The modal outcome would have been me sounding as if I was speaking with all of the arrogance, jackassery, and superiority that a recently minted Ph.D. could bring to bear in a social exchange. Not a good look when you are hunting for a job.&lt;/p&gt;
&lt;p&gt;I bristle at the distinction between statistics and machine learning, in part, because, &lt;em&gt;statistical modeling&lt;/em&gt; &lt;strong&gt;is&lt;/strong&gt; on my resume. Links to statistical programs that I have created and to this very site are on there as well. References to published work with statistical models I have built and iterated through can also be found on the document. But, I make no overt statements about machine learning algorithms or their use in my work. I would be less annoyed if it were not the case that machine learning is just plain sexier than statistics these days. Machine learning is a buzz word that encapsulates all the amazing possibilities that exist for implementing cutting-edge, data-driven solutions to real-world problems. Statistics is the ugly cousin of machine learning that just so happens to make machine learning possible in the first place!&lt;/p&gt;
&lt;p&gt;Some might argue that the &lt;a href=&#34;https://mgb-research.netlify.com/post/gaussian-process-imputation-models/&#34;&gt;Gaussian process models&lt;/a&gt; I work with are machine learning models &lt;span class=&#34;citation&#34;&gt;(Rasmussen and Williams 2006)&lt;/span&gt;. To me, they are mathematical expressions (i.e., models) that are used to explain a covariance matrix (sounds pretty statistics-y right?). I use a Bayesian estimation approach, in which a program, guided by certain rejection rules explores values of parameters until it converges on a likely distribution given the data and prior assumptions about the distribution of the parameters (sounds sort of machine learning-y right?). I could have just as easily specified the same model in a maximum likelihood estimation framework, in which, the most likely estimates for the population values of the parameters would be determined given the data by minimizing the loglikelihood using a different sort of algorithm.&lt;/p&gt;
&lt;p&gt;If you fell asleep a little bit when reading those last two sentences, I don’t blame you. The point is that these models are statistical models &lt;em&gt;and&lt;/em&gt; they are machine learning models. In the end, all statistical models have some sort of algorithm that guides decisions about parameter estimates and/or predicted values. The data go into the model, the machine (i.e., computer) identifies some set of likely parameter or predicted values based on the rules for the given model, and the end user gets a bunch of output they can use to make sense of the raw data fed into the program in the first place.&lt;/p&gt;
&lt;p&gt;I am not the first person to struggle with the relevance of distinguishing between statistical models and machine learning models &lt;span class=&#34;citation&#34;&gt;(Bzdok, Altman, and Krzywinksi 2018; O’Neil and Schutt 2014)&lt;/span&gt;. Honestly, it seems to come down to a distinction between the terms commonly employed in a given field, with computer science and data science folks often preferring machine learning over statistical modeling &lt;span class=&#34;citation&#34;&gt;(O’Neil and Schutt 2014)&lt;/span&gt;. I do want to avoid being too glib, though, in my blurring of the lines between these two categories of data analysis, as there may be some value in separating the development of models to explain phenomena (statistical models) and the creation of data-based predictions about phenomena while remaining relatively agnostic about the underlying data-generating process (machine learning models).&lt;/p&gt;
&lt;p&gt;So with this bubbling annoyance as my backdrop, I decided I should go ahead and apply different machine learning and statistical modeling approaches to a similar set of data problems. With the remainder of this post I plan to use publicly available data sets to walk through a few different implementations of statistical and machine learning models, comparing their outputs and touching briefly on meaningful differences and potential applications. I’ll briefly review implementations of &lt;em&gt;k&lt;/em&gt;-means, &lt;em&gt;k&lt;/em&gt; nearest neighbor, mixture modeling (latent profile analysis), discriminant analysis, naive Bayes classification, logistic regression, random forest, and a simple neural network.&lt;/p&gt;
&lt;p&gt;I’ll be tackling the first three models/techniques in this post with future posts in this series tackling the remaining approaches.&lt;/p&gt;
&lt;div id=&#34;k-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;K&lt;/em&gt;-Means&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;k&lt;/em&gt;-means algorithm is often referred to as an unsupervised learning model. The algorithm allows users to specify &lt;em&gt;k&lt;/em&gt; number of groups that maximally separate continuous data into groups in (multi)dimensional space by minimizing within-cluster variance. Another way of thinking about this technique is that it classifies each case based on its total distance from a group mean or centroid (a multivariate mean). Like a lot of modeling approaches, &lt;em&gt;k&lt;/em&gt;-means is finding a solution that minimizes an error term, in this case the within-group distance from a prototypical point (i.e., the mean).&lt;/p&gt;
&lt;p&gt;Specifying the number of clusters, &lt;em&gt;k&lt;/em&gt;, can be a little tricky, but there are some reasonable ways to do so. One is to use the “elbow” in a plot of the within-cluster sum of squares (WSS; a measure of within-cluster variance). At a certain number of clusters, the reduction in WSS begins to level out, suggesting you start to get lower explanatory returns for increased cluster complexity.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;iris&lt;/code&gt; data set, we’ll work through an example. The data set contains four measurements of plant anatomy: petal length, petal width, sepal length, and sepal width. These measures were collected on 50 specimens from three species for a total of 150 observations.&lt;/p&gt;
&lt;p&gt;First we are going to load in the data and then we will standardize the measures of plant anatomy so that all measures are on the same scale. I’ll be using a distribution with a mean of 0 and unit variance (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}=0, sd_y=1\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
iris[,1:4]&amp;lt;-scale(iris[,1:4]) #standarizes columns of a matrix, default is mean = 0, sd = 1
psych::describe(iris)   #making sure I get the results I expect&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              vars   n mean   sd median trimmed  mad   min  max range  skew
## Sepal.Length    1 150    0 1.00  -0.05   -0.04 1.25 -1.86 2.48  4.35  0.31
## Sepal.Width     2 150    0 1.00  -0.13   -0.03 1.02 -2.43 3.08  5.51  0.31
## Petal.Length    3 150    0 1.00   0.34    0.00 1.05 -1.56 1.78  3.34 -0.27
## Petal.Width     4 150    0 1.00   0.13   -0.02 1.36 -1.44 1.71  3.15 -0.10
## Species*        5 150    2 0.82   2.00    2.00 1.48  1.00 3.00  2.00  0.00
##              kurtosis   se
## Sepal.Length    -0.61 0.08
## Sepal.Width      0.14 0.08
## Petal.Length    -1.42 0.08
## Petal.Width     -1.36 0.08
## Species*        -1.52 0.07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have the data standardized, it is time to run the &lt;em&gt;k&lt;/em&gt;-means algorithm. Knowing that there are 3 species in here, I am going to iterate from 1 cluster to 6 clusters. I expect going into this analyis that the optimal solution, identified visually as the “elbow” in the plot below shoud be based on 3 clusters, given that there are in fact three species. Most use cases for &lt;em&gt;k&lt;/em&gt;-means are situations in which a grouping variable is not known in advance. &lt;em&gt;Note:&lt;/em&gt; The “elbow” approach is imperfect and other options exist for determining the number of clusters to specify.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))
for (i in 2:6) wss[i] &amp;lt;- sum(kmeans(iris[,1:4], centers=i)$withinss)
DF_kmeans&amp;lt;-data.frame(Cluster=1:6, 
                      WSS=wss)
g1&amp;lt;-ggplot(data=DF_kmeans, aes(x=Cluster, y=WSS))+
  geom_point(size=3)+
  geom_line()+
  geom_hline(data=DF_kmeans, 
             yintercept = (wss[3]+wss[4])/2, 
             color=&amp;#39;red&amp;#39;, 
             lty=&amp;#39;dashed&amp;#39;)+
  theme_bw()
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/kmeans1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is clearly a big dropoff from 1 to 2 clusters here, but we can really see that at 4 clusters the line starts to “flatten” out. Using a standard visual approach then, we would retain the number of clusters prior to the leveling out, which conforms to our expectations that 3 clusters should exist.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_kmeans &amp;lt;- kmeans(iris[,1:4], 3) #Saving the three-cluster solution
#Storing the cluster solution with the data
iris_kmeans &amp;lt;- data.frame(iris, fit_kmeans$cluster) 
colnames(iris_kmeans)[length(iris_kmeans)]&amp;lt;-&amp;#39;Cluster&amp;#39;
tab&amp;lt;-table(iris_kmeans$Cluster, iris_kmeans$Species)
iris_kmeans$Cluster&amp;lt;-as.character(iris_kmeans$Cluster)
#need to adjust results (which will randomly assign cluster values) to align with corresponding species
iris_kmeans$Cluster[iris_kmeans$Cluster==rownames(tab)[tab[,2]==max(tab[,2])]]&amp;lt;-&amp;#39;Versicolor Cluster&amp;#39;
iris_kmeans$Cluster[iris_kmeans$Cluster==rownames(tab)[tab[,3]==max(tab[,3])]]&amp;lt;-&amp;#39;Virginica Cluster&amp;#39;
iris_kmeans$Cluster[iris_kmeans$Cluster==rownames(tab)[tab[,1]==max(tab[,1])]]&amp;lt;-&amp;#39;Setosa Cluster&amp;#39;
table(iris_kmeans$Cluster, iris_kmeans$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     
##                      setosa versicolor virginica
##   Setosa Cluster         50          0         0
##   Versicolor Cluster      0         39        14
##   Virginica Cluster       0         11        36&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the algorithm correctly clustered all of the setosa specimens, but had some difficulty differentiating bewteen the virginica and versicolor specimens. We can see this overlap when plotting the first two discriminant functions against one another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotcluster(iris[,1:4],fit_kmeans$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/kmeans4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The raw data also indicate that veriscolor and virginica may be harder to disentangle from each other based on the available anatomical measures than they are to separate from setosa specimens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1&amp;lt;-ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

g2&amp;lt;-ggplot(data=iris, aes(x=Sepal.Length, y=Petal.Length, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

g3&amp;lt;-ggplot(data=iris, aes(x=Sepal.Length, y=Petal.Width, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

g4&amp;lt;-ggplot(data=iris, aes(x=Sepal.Width, y=Petal.Width, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

g5&amp;lt;-ggplot(data=iris, aes(x=Sepal.Width, y=Petal.Length, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

g6&amp;lt;-ggplot(data=iris, aes(x=Petal.Length, y=Petal.Width, group=Species, color=Species))+
  geom_point()+
  stat_ellipse(alpha=.25)+
  theme_bw()

cowplot::plot_grid(g1, g2, g3, g4, g5, g6, nrow=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/kmeans5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At least in bivariate space, it really seems as though setosa is very different than the other two specimens on these different measures of plant anatomy.&lt;/p&gt;
&lt;p&gt;To close, it is worth highlighting that &lt;em&gt;k&lt;/em&gt;-means can be used to identify clusters within observed data, based on minimizing distances from cluster means. There are no coefficients or weights produced by this model, so it cannot make predictions. Absent foreknowledge of the exiting groupings, it is impossible to evaluate its performance relative to a meaningful benchmark. It is a good exploratory technique and can have value as a pre-processing step in many situations, but beyond that, other methods will have to be used.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;K&lt;/em&gt; Nearest Neighbor&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;k&lt;/em&gt; nearest neighbor (often abbreviated as knn) algorithm is conceptually similar to the &lt;em&gt;k&lt;/em&gt;-means approach, but operates more on a case-by-case basis. For each observation in the dataset, the algorithm finds a subset of the most similar cases in terms of squared distance (again a way of minimizing differences/errors/residuals, whatever term you like the best) from the target case. The number of cases used is set by &lt;em&gt;k&lt;/em&gt;. Then, the nearest of the &lt;em&gt;k&lt;/em&gt; neighbors is used to predict the classification of the target observation.&lt;/p&gt;
&lt;p&gt;As opposed to &lt;em&gt;k&lt;/em&gt;-means, knn can be used to predict classification based on new data, by finding &lt;em&gt;k&lt;/em&gt; neighbors and using the nearest neighbor to classify the new case. The trick here is finding the optimal &lt;em&gt;k&lt;/em&gt; for classification, which is going to vary from data set to data set. We can use a cross-validation approach though to identify the optimal &lt;em&gt;k&lt;/em&gt;. I am using the &lt;code&gt;train&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package in R to perform this analysis. So for those following along at home, you’ll have to make sure you have &lt;code&gt;caret&lt;/code&gt; and its dependencies installed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_knn&amp;lt;-train(Species~., 
               data = iris, 
               method=&amp;#39;knn&amp;#39;, 
               trControl = trainControl(method=&amp;#39;repeatedcv&amp;#39;, 
                                        number = 15, 
                                        repeats = 50, 
                                        classProbs = TRUE, 
                                        summaryFunction = multiClassSummary),
               metric = &amp;#39;AUC&amp;#39;,
               tuneLength = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info =
## trainInfo, : There were missing values in resampled performance measures.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit_knn)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/knn-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So if we look at this plot, we see that the area under the curve (AUC) is highest at &lt;em&gt;k&lt;/em&gt;=9 (values for AUC closer to 1 represent a better performing knn model). Other relevant performance statistics can be viewed by simply running &lt;code&gt;fit_knn&lt;/code&gt; in your console.&lt;/p&gt;
&lt;p&gt;Next, we will want to see how the model performed in predicting the data. As an aside, with larger datasets it is possible to mitigate against overfitting by holding out a proportion of the data for testing model performance. My goal in this post is to compare the performance of statistical vs. machine learning techniques in correctly identifying patterns in the underlying data. As such, I will be keeping the entire &lt;code&gt;iris&lt;/code&gt; data set in tact so that supervised and unsupervised models can be evaluated on similar grounds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred&amp;lt;-predict(fit_knn)
table(pred, iris$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
## pred         setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         48         4
##   virginica       0          2        46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compared to the &lt;em&gt;k&lt;/em&gt;-means approach, we can see that the knn model, using 9 nearest neighborhoods, performs relatively well at separating versicolor and virginica specimens. Recall that &lt;em&gt;k&lt;/em&gt;-means struggled making this distinction due to the two species’ overlap in distributions on several of the measures. A key added benefit of knn over &lt;em&gt;k&lt;/em&gt;-means is that I can feed this model new data, and receive a species prediction, which is not possible with &lt;em&gt;k&lt;/em&gt;-means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Note I chose values that are more consistent with the versicolor
newdata&amp;lt;-data.frame(Sepal.Length = .07, 
                    Sepal.Width = -.95, 
                    Petal.Length = .11, 
                    Petal.Width =.59)

pred_new&amp;lt;-predict(fit_knn, newdata = newdata)
pred_new&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] versicolor
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can also get the estimated probability the new specimen included in the data frame above belongs to each of the groups. There are many reasons to want to consider group membership from models like these in a probabilistic fashion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Note I chose values that are more consistent with the versicolor
newdata&amp;lt;-data.frame(Sepal.Length = .07, 
                    Sepal.Width = -.95, 
                    Petal.Length = .11, 
                    Petal.Width =.59)

pred_new&amp;lt;-predict(fit_knn, newdata = newdata, type=&amp;#39;prob&amp;#39;)
pred_new&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   setosa versicolor virginica
## 1      0  0.8888889 0.1111111&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mixture-modeling-latent-profile-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mixture Modeling (Latent Profile Analysis)&lt;/h2&gt;
&lt;p&gt;Our departure into mixture modeling represents the first “full-blown” foray into a statistical model. I’ll start with the univariate case, as that tends to help people make sense of what a mixture model is trying to do. Taking a step back before displaying the univariate example it is worth asking, what value these models hold? Well, sometimes when researchers obtain a sample of observations, it is possible that they may have sampled multiple, separate populations. These populations in turn, have their own univariate (or multivariate) distributions on target scores of interest. Mixture models offer a means of both a) identifying the maximally likely number of populations (i.e., separate distributions that were sampled) and b) probabilistically determining each case’s membership in the sampled, but potentially unmeasured latent populations.&lt;/p&gt;
&lt;p&gt;With that, welcome to the world of maximum likelihood estimation!! For you stats folks out there, you will recognize the approach I am using here is frequentist (the package I use relies on the EM algorithm). That is not to say that there are not Bayesian extensions of these sorts of models, though.&lt;/p&gt;
&lt;p&gt;In a practical sense, there are two problems that a mixture modeling approach has to solve. The first is, estimating the appropriate number of latent classes or subgroups that best account for the observed distribution(s) of scores. This dilemma is similar to the problem facing an analyst using &lt;em&gt;k&lt;/em&gt;-means to identify subgroups. However, there are a number of additional tools, beyond simply examining reductions in within-group sums of squares (WSS) the analyst can use to identify the optimal number of classes in a mixture modeling framework.&lt;/p&gt;
&lt;p&gt;After negotiating this first step, the next challenge is to estimate the probability that a given case belongs to one class or the other. In a mixture model, the mean and the variances (and covariances in a multivariate classification problem) are used to create a posterior probability for each observed data point, and each case is then classified based on its highest posterior probability.&lt;/p&gt;
&lt;p&gt;Below, as promised, is a simple demonstration of a “mixed” univariate distribution. Imagine that I measured a random sample of men and women on height. Let’s say that men in the population I sampled had an average height of 70 inches with a standard deviation of 4 inches (i.e., &lt;span class=&#34;math inline&#34;&gt;\(N\sim(\mu_M=70, \sigma_M=4)\)&lt;/span&gt;). By comparison, women in the population sampled had a mean height of 65 inches with a standard deviation of 3.5 inches (i.e., &lt;span class=&#34;math inline&#34;&gt;\(N\sim(\mu_F=65, \sigma_F=3.5)\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DF_mix_uni&amp;lt;-data.frame(Gender = c(rep(&amp;#39;Male&amp;#39;, 100),
                                  rep(&amp;#39;Female&amp;#39;,100)), 
                       Height = c(rnorm(100, 70, 4), 
                                  rnorm(100, 65,3.5))
                       )

g1&amp;lt;-ggplot()+
  geom_density(data=DF_mix_uni, 
               aes(x=Height, fill=&amp;#39;Overall&amp;#39;, color=&amp;#39;Overall&amp;#39;), alpha=.25)+
  geom_density(data = DF_mix_uni, 
               aes(x=Height, group=Gender, fill=Gender, color=Gender), alpha=.25)+
  guides(color=FALSE)+
  scale_fill_discrete(name=&amp;#39;&amp;#39;)+
  theme_bw()
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/LPA1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using this plot, we can see how the two different distributions for men and women “average” out to the distribution for the overall sample. This of course becomes more complicated when we measure subgroups on mutliple dimensions and the distributions in question enter a multivariate space.&lt;/p&gt;
&lt;p&gt;Still working with the &lt;code&gt;iris&lt;/code&gt; data set, I’ll walk through a model-based clustering attempt to identify subpopulations of plant specimens given their anatomical measurements alone. As was the case with &lt;em&gt;k&lt;/em&gt;-means, this effort at classification has no traditional depedent variable and is often considered an exploratory technique in statistical language or an unsupervised technique in machine learning speak.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;mclust&lt;/code&gt; package provides some nice built-in features for a problem like this, though as with many things in R, it is far from the only way to perform an analysis like this. First, I need to figure out the optimal number of latent classes I should extract in my final classification model. The &lt;code&gt;mclust&lt;/code&gt; package allows the user to constrain certain properties of the multivariate distributions across groups to be equal in the model. Or alternatively, the analyst can choose to let these vary freely across possible groupings. Unless I have strong practical or theoretical reasons for assuming equivalency on these properties, I tend to the let all vary (i.e., I include &lt;code&gt;&#39;VVV&#39;&lt;/code&gt; in the model names). For illustrative purposes, I will have &lt;code&gt;mclust&lt;/code&gt; default to showing model fit under all possible patterns of distributional constraints.&lt;/p&gt;
&lt;p&gt;For step 1, I need a way to evaluate model fit somehow. Employing this modeling approach, I will actually end up weighing multiple sources of information in determining the optimal number of clusters, starting with Bayesian Information Criteria (BIC), which is a statsitic that can be used to compare multiple models. The BIC score penalizes models for added complexity that results in little improvement in the data-model fit. This feature helps ensure that more parsimonious models that perform well are chosen over more complex models with similar data-model fit. In the &lt;code&gt;mclust&lt;/code&gt; formulation of BIC, values closer to 0 indicate better fit (&lt;em&gt;Note&lt;/em&gt; this is not always the case across modeling approaches inside or outside R).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BIC&amp;lt;-mclust::mclustBIC(iris[,1:4])
plot(BIC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/LPA2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(BIC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best BIC values:
##              VVV,2       VVV,3       VEV,3
## BIC      -790.6956 -797.518958 -797.548282
## BIC diff    0.0000   -6.823349   -6.852673&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative to BIC is the Integrated Complete-data Likelihood (ICL) criterion. It is also a measure of data-model fit, but it does not assume that the overall distribution is necessarily Gaussian (i.e., normal) &lt;span class=&#34;citation&#34;&gt;(Baudry 2015)&lt;/span&gt;. I understand this is getting a little technical, but the fact is that model-based clustering approaches are exploratory and require consideration of converging lines of evidence. The BIC value is based on some assumptions about the data that are not made with the ICL criterion. With &lt;em&gt;k&lt;/em&gt;-means we only had to worry about minimizing WSS. With these more complex models, there are multiple ways of assessing the data-model fit as a function of the number of classes extracted. (&lt;em&gt;Note&lt;/em&gt; you need to have the &lt;code&gt;mclust&lt;/code&gt; loaded in the environment for the &lt;code&gt;mclustICL&lt;/code&gt; function to work properly).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ICL&amp;lt;-mclust::mclustICL(data=iris[,1:4])
plot(ICL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/LPA3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ICL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best ICL values:
##              VVV,2      VVV,3      VEV,2
## ICL      -790.6969 -800.73908 -802.56763
## ICL diff    0.0000  -10.04221  -11.87076&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far, the results are fairly similar across both measures of model fit I have used. Each indicate that the 2-class ‘VVV’ solution (distributional properties are free to vary across possible latent groups) is the best, simplest model for the observed data. It is worth noting that the difference between the &lt;code&gt;VVV,2&lt;/code&gt; and &lt;code&gt;VVV,3&lt;/code&gt; models is not all that large though. As a final step in determining the optimal number of latent groups, I will perform a bootstrapped likelihood ratio test. Without getting into the details, this technique tests whether inclusion of an additional class (i.e., &lt;span class=&#34;math inline&#34;&gt;\(k + 1\)&lt;/span&gt;) improves model fit above and beyond a simpler model (with &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; classes).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt; for high-dimensional datasets this could take a while.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LRT&amp;lt;-mclust::mclustBootstrapLRT(data=iris[,1:4], modelName = &amp;#39;VVV&amp;#39;, nboot=1000)
LRT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------- 
## Bootstrap sequential LRT for the number of mixture components 
## ------------------------------------------------------------- 
## Model        = VVV 
## Replications = 1000 
##               LRTS bootstrap p-value
## 1 vs 2   331.11985       0.000999001
## 2 vs 3    68.33618       0.000999001
## 3 vs 4    25.40075       0.667332667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay so in reviewing these results, we see that the likelihood ratio test indicates that going from 1 to 2 classes improves model fit significantly (i.e, &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;, a commonly used cutoff). Going from 2 classes to 3 classes further improves model fit, but going from 3 classes to 4 classes does not.&lt;/p&gt;
&lt;p&gt;The conclusion here, based on all of this exploratory work, is that we can go ahead and conclude that 3-classes is likely the optimal number to extract with no equality constraints placed on the multivariate distributions across groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_mix&amp;lt;-Mclust(iris[,1:4], G=3, modelNames = &amp;#39;VVV&amp;#39;)
summary(fit_mix, parameters=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model
## with 3 components: 
## 
##  log.likelihood   n df      BIC       ICL
##       -288.5255 150 44 -797.519 -800.7391
## 
## Clustering table:
##  1  2  3 
## 50 45 55 
## 
## Mixing probabilities:
##         1         2         3 
## 0.3333333 0.2995796 0.3670871 
## 
## Means:
##                    [,1]        [,2]       [,3]
## Sepal.Length -1.0111914  0.08690767  0.8472868
## Sepal.Width   0.8504137 -0.64115633 -0.2489706
## Petal.Length -1.3006301  0.25163727  0.9756758
## Petal.Width  -1.2507035  0.12842627  1.0308924
## 
## Variances:
## [,,1]
##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length   0.17757788  0.26939586  0.010964687 0.016039717
## Sepal.Width    0.26939586  0.74121713  0.014899264 0.027426477
## Petal.Length   0.01096469  0.01489926  0.009484392 0.004420409
## Petal.Width    0.01603972  0.02742648  0.004420409 0.018733017
## [,,2]
##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length   0.40154260   0.2684011    0.1263996  0.08625955
## Sepal.Width    0.26840105   0.4875691    0.1184208  0.12941216
## Petal.Length   0.12639958   0.1184208    0.0644659  0.04540180
## Petal.Width    0.08625955   0.1294122    0.0454018  0.05516462
## [,,3]
##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length   0.56449298   0.2554746   0.20706544  0.09739655
## Sepal.Width    0.25547457   0.5808469   0.10942673  0.16842603
## Petal.Length   0.20706544   0.1094267   0.10505359  0.05511877
## Petal.Width    0.09739655   0.1684260   0.05511877  0.14733331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By including the &lt;code&gt;parameters=TRUE&lt;/code&gt; argument, I can see the means and variances for each of the three clusters returned. The &lt;code&gt;mclust&lt;/code&gt; package also has some nice exploratory features built in that can be used to evaluate the properties of the clusters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit_mix, what=&amp;#39;classification&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-11-29-a-rose-by-any-other-name-statistics-machine-learning-and-ai_files/figure-html/LPA7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And, I can also compare the class results against the observed data (because this is something known in this instance - which may not always be the case when using a mixture model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_mix&amp;lt;-data.frame(iris, fit_mix$classification)
colnames(iris_mix)[length(iris_mix)]&amp;lt;-&amp;#39;Classification&amp;#39;
iris_mix$Classification&amp;lt;-as.character(iris_mix$Classification)
table(iris_mix$Classification, iris_mix$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     setosa versicolor virginica
##   1     50          0         0
##   2      0         45         0
##   3      0          5        50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is slightly better in its overall classification than the knn approach (only 5 errors vs 6), and both performed better than the &lt;em&gt;k&lt;/em&gt;-means algorithm.&lt;/p&gt;
&lt;p&gt;This concludes the exploratory approach to classifying with this data set. However, the &lt;code&gt;mclust&lt;/code&gt; package also allows for a more predictive modeling approach when there is a known class (as is the case in the present data set). You can even include priors (they have to be conjugate priors, which means that there are mathematical solutions to the resulting integrations - ignore if this makes no sense to you and stick with the defaults).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class&amp;lt;-iris$Species
fit_mix2&amp;lt;-MclustDA(data=iris[,1:4], class = class, modelNames = &amp;#39;VVV&amp;#39;)
summary(fit_mix2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------ 
## Gaussian finite mixture model for classification 
## ------------------------------------------------ 
## 
## MclustDA model summary: 
## 
##  log.likelihood   n df       BIC
##       -291.2597 150 42 -792.9662
##             
## Classes       n Model G
##   setosa     50   XXX 1
##   versicolor 50   XXX 1
##   virginica  50   XXX 1
## 
## Training classification summary:
## 
##             Predicted
## Class        setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         48         2
##   virginica       0          1        49
## 
## Training error = 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this modeling approach we are now down to the smallest error rate so far, only three cases were mis-classified. What is more, this approach, as opposed to the exploratory results can be used to predict the classification of new data. It can also be used to evaluate a trained dataset on a new, observed data set to ameliorate problems with overfitting to an obtained sample (i.e., using a training/testing approach to model performance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Note I chose values that are more consistent with virginica
newdata&amp;lt;-data.frame(Sepal.Length = .75, 
                    Sepal.Width = -.19, 
                    Petal.Length = .76, 
                    Petal.Width =.84)

pred_new&amp;lt;-predict(fit_mix2, newdata = newdata)
pred_new&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $classification
## [1] virginica
## Levels: setosa versicolor virginica
## 
## $z
##             setosa versicolor virginica
## [1,] 2.479729e-124 0.07448784 0.9255122&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there you have it. Three models/techniques/algorithms that are all trying to accomplish similar classification/grouping tasks. The knn and the mixture modeling approaches clearly demonstrated the best performance in this, admittedly simple, toy problem of classifying plant specimens. The next installment will take a deeper dive into traditional discriminant analysis as well as Naive Bayes classifiers. The former are often considered to be a part of the statistical modeling family; the latter are more frequently used these days in the data science/machine learning world. Each, again end up doing very similar things.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-baudry2015&#34;&gt;
&lt;p&gt;Baudry, Jean-Patrick. 2015. “Estimation and Model Selection for Model-Based Clustering with the Conditional Classification Likelihood.” &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt; 9 (1): 1041–77.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Bzdoketal2018&#34;&gt;
&lt;p&gt;Bzdok, Danilo, Naomi Altman, and Martin Krzywinksi. 2018. “Statistics Versus Machine Learning.” &lt;em&gt;Nature Methods&lt;/em&gt; 15: 233–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ONeilSchutt2014&#34;&gt;
&lt;p&gt;O’Neil, Cathy, and Rachel Schutt. 2014. &lt;em&gt;Doing Data Science&lt;/em&gt;. O’Reilly Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-RasmussenWilliams2006&#34;&gt;
&lt;p&gt;Rasmussen, C.E., and K.I. Williams. 2006. &lt;em&gt;Gaussian Processes for Machine Learning&lt;/em&gt;. The MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data Complete Workflow (Part 2 of 3)</title>
      <link>https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</guid>
      <description>&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview:&lt;/h1&gt;
&lt;p&gt;This is the second post in a three-part blog series I am putting together. If you have not read the first post in this series, you may want to go back and &lt;a href=&#34;https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/&#34;&gt;check it out&lt;/a&gt;. In this post, I will focus on running and evaluating the imputation model itself, having identified the appropriate covariates that help account for missingness in the first post.&lt;/p&gt;
&lt;div id=&#34;data-brief-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Brief Description:&lt;/h2&gt;
&lt;p&gt;The data in question come from a study that involved a one-week ecological momentary assessment (EMA) protocol. For seven consecutive days, participants (&lt;em&gt;N&lt;/em&gt;=127) responded to 10 prompts delivered at pseudo-random times. The timing of EMA probes was built around class schedules during the day (hence pseudo-random). More detail about the sample and procedures can be found &lt;a href=&#34;https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imputation-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imputation Options&lt;/h2&gt;
&lt;p&gt;There are several imputation packages available that aid researchers imputing data in R. Perhaps the two most popular are the &lt;code&gt;mice&lt;/code&gt; package &lt;span class=&#34;citation&#34;&gt;(van Buuren and Groothuis-Oudshoorn 2011)&lt;/span&gt; and the &lt;code&gt;Amelia&lt;/code&gt; package &lt;span class=&#34;citation&#34;&gt;(Honaker, King, and Blackwell 2011)&lt;/span&gt;. When the data in question has a nested structure (e.g., students nested within classrooms, patients nested within clinics, observations nested within individuals, etc.), the &lt;code&gt;pan&lt;/code&gt; package &lt;span class=&#34;citation&#34;&gt;(Zhao and Schafer 2018)&lt;/span&gt; can be used.&lt;/p&gt;
&lt;p&gt;In this case, I will be using the &lt;code&gt;mitml&lt;/code&gt; package &lt;span class=&#34;citation&#34;&gt;(Grund, Robitzsch, and Luedtke 2018)&lt;/span&gt;, which is a wraparound package that depends on the &lt;code&gt;pan&lt;/code&gt; package. Alternative approaches using the &lt;code&gt;pan&lt;/code&gt; algorithm via &lt;code&gt;mice&lt;/code&gt; can be found &lt;a href=&#34;http://www.gerkovink.com/miceVignettes/Multi_level/Multi_level_data.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imputation-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imputation Model&lt;/h2&gt;
&lt;p&gt;To setup the imputation model, I need to specify the variables that need to be imputed (in front of the &lt;code&gt;~&lt;/code&gt;) and the complete variables, along with the random effects (after the &lt;code&gt;~&lt;/code&gt;). Exploratory analyses covered in the &lt;a href=&#34;https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/&#34;&gt;first post&lt;/a&gt; indicated that BFI Openess scores and BFI Conscientiousness scores both predicted missingness. As is common practice, I will also include dispositional negativity scores, which serve as the primary individual-level predictor in my final model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fml&amp;lt;- c.Worst + c.Best + NegAff + PosAff  ~ 
  1 + c.DN + BFI_C + BFI_O + (1|ID)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the formula setup, I can now impute the data. There have been a number of recommendations out there regarding how many datasets you need to impute to ensure that you get stable results. I recommend reading a pair of papers out there to those who think you can always get away with around 10 datasets when imputing &lt;span class=&#34;citation&#34;&gt;(Bodner 2008; Graham, Olchowski, and Gilreath 2007)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Based on the structure of my data and the total missingness and recommendations made by &lt;span class=&#34;citation&#34;&gt;Graham, Olchowski, and Gilreath (2007)&lt;/span&gt;, I chose to impute 20 data sets. I also have a relatively lengthy burn-in period so I can ensure convergence of the posterior distributions from which values are drawn. In setting &lt;code&gt;n.iter = 50&lt;/code&gt; I have further made the choice to separate my draws from the MCMC chains to help mitigate any autocorrelation that could arise in imputed data sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp&amp;lt;-panImpute(dat.study1, 
               formula=fml, 
               n.burn=100, 
               n.iter = 50, 
               m=20, 
               seed = 0716)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extracting Datasets&lt;/h2&gt;
&lt;p&gt;The next step is to pull out the imputed data and start examining whether or not the imputed data sets make sense. First, we can examine convergence of the imputation model. If you were to run &lt;code&gt;plot(imp, print=&#39;beta&#39;)&lt;/code&gt; you would be able review convergence for all aspects of the imputation model. Doing so would also reveal that some parameters did not fully converge and the number of iterations between draws likely needs to be increased. To address these problems, I begin by re-running the imputation model with a longer burn-in phase and greater distance between draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp&amp;lt;-panImpute(dat.study1, 
               formula=fml, 
               n.burn=10000, 
               n.iter = 5000, 
               m=20, 
               seed = 0716)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having satisfied myself that there are no lingering convergence issues I can create some initial plots. First, I need to re-structure the data to make it a bit easier to plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat&amp;lt;-mitmlComplete(imp)

dat.long&amp;lt;-data.frame()
for(i in 1:20){
  dat.temp&amp;lt;-dat[[i]]
  dat.temp$IMP&amp;lt;-rep(i, length(dat.temp[,1]))
  dat.long&amp;lt;-rbind(dat.long, dat.temp)
}

dat.study1$IMP&amp;lt;-rep(0, length(dat.study1[,1]))
dat.long&amp;lt;-rbind(dat.study1, dat.long)
dat.long$Orig&amp;lt;-ifelse(dat.long$IMP&amp;lt;1, &amp;#39;Original&amp;#39;, &amp;#39;Imputed&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay now we can plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1&amp;lt;-ggplot()+
  geom_freqpoly(data = dat.long, aes(x=NegAff, group=IMP, color=Orig), stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ggtitle(&amp;#39;Momentary Negative Affect&amp;#39;)+
  ylab(&amp;#39;Density&amp;#39;)+
  xlab(&amp;#39;&amp;#39;)

g2&amp;lt;-ggplot()+
  geom_freqpoly(data = dat.long, aes(x=PosAff, group=IMP, color=Orig), stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ggtitle(&amp;#39;Momentary Positive Affect&amp;#39;)+
  ylab(&amp;#39;Density&amp;#39;)+
  xlab(&amp;#39;&amp;#39;)

g3&amp;lt;-ggplot()+
  geom_freqpoly(data = dat.long, aes(x=c.Worst, group=IMP, color=Orig), stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ggtitle(&amp;#39;Worst Event&amp;#39;)+
  ylab(&amp;#39;Density&amp;#39;)+
  xlab(&amp;#39;&amp;#39;)

g4&amp;lt;-ggplot()+
  geom_freqpoly(data = dat.long, aes(x=c.Best, group=IMP, color=Orig), stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ggtitle(&amp;#39;Best Event&amp;#39;)+
  ylab(&amp;#39;Density&amp;#39;)+
  xlab(&amp;#39;&amp;#39;)

print(cowplot::plot_grid(g1, g2, g3, g4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-09-03-bayesian-multilevel-model-with-missing-data-complete-workflow-part-2_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The good news is that the imputed values largely follow the same distribution as the original values. Having assessed convergence and now the actual imputation results, I feel pretty good about the results. I am now ready to move on to the actual analysis, which will be reviewed in the final post in this series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Bodner2008&#34;&gt;
&lt;p&gt;Bodner, Todd E. 2008. “What improves with increased missing data imputations?” &lt;em&gt;Structural Equation Modeling&lt;/em&gt; 15 (4): 651–75. &lt;a href=&#34;https://doi.org/10.1080/10705510802339072&#34;&gt;https://doi.org/10.1080/10705510802339072&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Graham2007&#34;&gt;
&lt;p&gt;Graham, John W., Allison E. Olchowski, and Tamika D. Gilreath. 2007. “How many imputations are really needed? Some practical clarifications of multiple imputation theory.” &lt;em&gt;Prevention Science&lt;/em&gt; 8 (3): 206–13. &lt;a href=&#34;https://doi.org/10.1007/s11121-007-0070-9&#34;&gt;https://doi.org/10.1007/s11121-007-0070-9&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mitml_2018&#34;&gt;
&lt;p&gt;Grund, Simon, Alexander Robitzsch, and Oliver Luedtke. 2018. &lt;em&gt;Mitml: Tools for Multiple Imputation in Multilevel Modeling&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=mitml&#34;&gt;https://CRAN.R-project.org/package=mitml&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Amelia_2011&#34;&gt;
&lt;p&gt;Honaker, James, Gary King, and Matthew Blackwell. 2011. “Amelia II: A Program for Missing Data.” &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 45 (7): 1–47. &lt;a href=&#34;http://www.jstatsoft.org/v45/i07/&#34;&gt;http://www.jstatsoft.org/v45/i07/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mice_2011&#34;&gt;
&lt;p&gt;van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in R.” &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 45 (3): 1–67. &lt;a href=&#34;https://www.jstatsoft.org/v45/i03/&#34;&gt;https://www.jstatsoft.org/v45/i03/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pan_2018&#34;&gt;
&lt;p&gt;Zhao, Jing Hua, and Joseph L. Schafer. 2018. &lt;em&gt;Pan: Multiple Imputation for Multivariate Panel or Clustered Data&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s Alive! First Evidence that IBI VizEdit Works</title>
      <link>https://mgb-research.netlify.com/post/it-s-alive-first-evidence-that-ibi-vizedit-works/</link>
      <pubDate>Thu, 16 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/it-s-alive-first-evidence-that-ibi-vizedit-works/</guid>
      <description>&lt;p&gt;It is official. The program I have spent the better part of a year working on, the very centerpiece of my dissertation, works. Or at least, early indicators are in, and based on 22 cases, some of which required a great deal of manual editing, the program is returning estimates in line with expectations.&lt;/p&gt;
&lt;p&gt;Backing up, as I trip a little over my excitement, IBI VizEdit is an Rshiny application I created to help our lab process and edit heart rate data. We used a photoplethysmogram to measure changes in light absorption in a local capillary bed, in this case the child’s fingertip. As blood flows through the capillary bed in sync with the beating heart, the amount of light absorbed by the underlying tissue varies, particularly within certain wavelengths. With this knowledge and a sufficiently high sampling rate (we used 2000 Hz), you can readily record heart beats using a relatively low-cost sensor.&lt;/p&gt;
&lt;p&gt;Individual heart rate data contains a surprising amount of information that can be used to predict an individual’s cognitive and affective states as well as predict global mental and physical health outcomes. Paired with specific tasks, we can get a sense of how effective the autonomic nervous system is at regulating internal states that are designed to potentiate certain response patterns (i.e., fleeing, fighting, freezing, behaviors).&lt;/p&gt;
&lt;p&gt;Currently, there are no open-source tools available to researchers interested in measures of heart-rate variability obtained via photoplethysmography. That is why I created IBI VizEdit. It is still very much in its first lifespan, and it is primarily designed to work with our lab’s files in ways that optimize output for what we plan to do with our data. I eventually plan to adapt the program to be more general in its input to allow researchers to upload files of just about any basic data format (e.g., .dat, .txt, csv, etc.).&lt;/p&gt;
&lt;p&gt;For now, I am just happy to be getting off on the right foot. And here are (again to be stressed) the &lt;em&gt;preliminary&lt;/em&gt; results of an analysis of 22 edited cases. Participants in the study experienced three conditions during a baseline laboratory visit. The first is a child-appropriate Sesame Street video, which the child sees three times. In addition to the videos children also experienced two stressors: the appearance of a clown and the recording of an introduction video. The sixth task (fourth in its presentation) was a social attention task in which children learned about fictitious children and their interests.&lt;/p&gt;
&lt;p&gt;Now if the program is working, estimates based on its output should show that respiratory sinus arrhythmia (RSA) is lowest during the Clown and Introduction tasks and highest during the Video and social attention (Kids) tasks.&lt;/p&gt;
&lt;p&gt;So what do we find… First always graph your data. I really like the &lt;a href=&#34;https://micahallen.org/2018/03/15/introducing-raincloud-plots/&#34;&gt;sideways raincloud plot (or nose plot)&lt;/a&gt; as an option for plotting. It sort of puts all of your data out there for everyone to see.&lt;/p&gt;
&lt;p&gt;First let’s look at the RSA values by task:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df.m&amp;lt;-reshape2::melt(dat.RSA[,2:7])

g1&amp;lt;-ggplot(data = df.m, aes(y = value, x = variable, fill=variable)) +
  geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8) +
  geom_point(aes(y = value, color = variable), 
             position = position_jitter(width = .15), 
             size = .5, alpha = 0.8) +
  geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5) +
  expand_limits(x = 5.25) +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  scale_fill_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  theme_bw() +
  raincloud_theme+
  xlab(&amp;#39;&amp;#39;)+ylab(&amp;#39;RSA&amp;#39;)+
  guides(fill=FALSE, color=FALSE)+
  ggtitle(&amp;#39;Distribution of Respiratory Sinus Arrhythmia as a Function of Task&amp;#39;)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-08-16-it-s-alive-first-evidence-that-ibi-vizedit-works_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So a good amount of spread, but even with only 22 cases we can start to see the expected pattern. The medians for the videos and the social attention taks (Kids) are higher than the median RSA values for the two distressing tasks.&lt;/p&gt;
&lt;p&gt;I would expect heart period (the inverse of heart rate - think the time, in seconds, between successive beats) to show a similar pattern.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df.m&amp;lt;-reshape2::melt(dat.HP[,2:7])

g2&amp;lt;-ggplot(data = df.m, aes(y = value, x = variable, fill=variable)) +
  geom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8) +
  geom_point(aes(y = value, color = variable), 
             position = position_jitter(width = .15), 
             size = .5, alpha = 0.8) +
  geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5) +
  expand_limits(x = 5.25) +
  scale_color_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  scale_fill_brewer(palette = &amp;quot;Dark2&amp;quot;) +
  theme_bw() +
  raincloud_theme+
  xlab(&amp;#39;&amp;#39;)+ylab(&amp;#39;HP&amp;#39;)+
  guides(fill=FALSE, color=FALSE)+
  ggtitle(&amp;#39;Distribution of Heart Period as a Function of Task&amp;#39;)

g2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-08-16-it-s-alive-first-evidence-that-ibi-vizedit-works_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lo and behold largely the same pattern. This is good for me so far. So let’s see if it passes a strict inferential test. Do the two distressing tasks each reliably depress heart rate variability (in the form of respiratory sinus arrhythmia) and average heart period?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#--
fit.RSA&amp;lt;-lmer(RSA~1+Clown+Video2+Kids+Intro+Video3+(1|File), 
                data=dat.RSA.long)
summary(fit.RSA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: RSA ~ 1 + Clown + Video2 + Kids + Intro + Video3 + (1 | File)
##    Data: dat.RSA.long
## 
## REML criterion at convergence: 208.6
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.13204 -0.68778 -0.00622  0.52209  2.77991 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  File     (Intercept) 0.4428   0.6655  
##  Residual             0.1801   0.4244  
## Number of obs: 125, groups:  File, 22
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  6.18134    0.17278  35.777
## Clown       -0.30111    0.14211  -2.119
## Video2       0.03093    0.13383   0.231
## Kids        -0.10134    0.13383  -0.757
## Intro       -0.38044    0.13383  -2.843
## Video3      -0.08134    0.13383  -0.608
## 
## Correlation of Fixed Effects:
##        (Intr) Clown  Video2 Kids   Intro 
## Clown  -0.399                            
## Video2 -0.420  0.515                     
## Kids   -0.420  0.515  0.543              
## Intro  -0.420  0.515  0.543  0.543       
## Video3 -0.420  0.515  0.543  0.543  0.543&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Getting bootstrapped CIs for model results
boot.RSA&amp;lt;-bootMer(fit.RSA, fixef, nsim=5000)
print(sjstats::boot_ci(boot.RSA))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           term   conf.low   conf.high
## 1 X.Intercept.  5.8459129  6.52132541
## 2        Clown -0.5795623 -0.02974797
## 3       Video2 -0.2321143  0.28924592
## 4         Kids -0.3673733  0.15766268
## 5        Intro -0.6464905 -0.11757507
## 6       Video3 -0.3422758  0.17603913&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the estimated t-scores, and my prefered method - the boot-strapped confidence interval, we see much the same story. RSA estimates were reliably lower during the clown and video introduction task relative to the first video (there were no other significant differences).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#--
fit.HP&amp;lt;-lmer(HP~1+Clown+Video2+Kids+Intro+Video3+(1|File), 
                data=dat.HP.long)
summary(fit.HP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: HP ~ 1 + Clown + Video2 + Kids + Intro + Video3 + (1 | File)
##    Data: dat.HP.long
## 
## REML criterion at convergence: -489.2
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7645 -0.5346  0.0340  0.5869  3.5147 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  File     (Intercept) 0.002568 0.05068 
##  Residual             0.000554 0.02354 
## Number of obs: 130, groups:  File, 22
## 
## Fixed effects:
##               Estimate Std. Error t value
## (Intercept)  0.6061055  0.0120385  50.347
## Clown       -0.0240009  0.0073053  -3.285
## Video2      -0.0049964  0.0073053  -0.684
## Kids        -0.0008464  0.0073053  -0.116
## Intro       -0.0296282  0.0073053  -4.056
## Video3      -0.0089237  0.0073053  -1.222
## 
## Correlation of Fixed Effects:
##        (Intr) Clown  Video2 Kids   Intro 
## Clown  -0.320                            
## Video2 -0.320  0.528                     
## Kids   -0.320  0.528  0.528              
## Intro  -0.320  0.528  0.528  0.528       
## Video3 -0.320  0.528  0.528  0.528  0.528&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Getting bootstrapped CIs for model results
boot.HP&amp;lt;-bootMer(fit.HP, fixef, nsim=5000)
print(sjstats::boot_ci(boot.HP))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           term    conf.low    conf.high
## 1 X.Intercept.  0.58255685  0.629886751
## 2        Clown -0.03840314 -0.009302852
## 3       Video2 -0.01904878  0.009356290
## 4         Kids -0.01530506  0.013353923
## 5        Intro -0.04414100 -0.015208981
## 6       Video3 -0.02334688  0.005379632&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the pattern was replicated with heart period.&lt;/p&gt;
&lt;p&gt;I definitely do not want to oversell these results. They are just a good sign is all as I continue to finalize this program and its features.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RMarkdown is the Most Powerful Codebook Maker You Can Find for Your Datasets</title>
      <link>https://mgb-research.netlify.com/post/rmarkdown-is-the-most-powerful-codebook-maker-you-can-find-for-your-datasets/</link>
      <pubDate>Tue, 07 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/rmarkdown-is-the-most-powerful-codebook-maker-you-can-find-for-your-datasets/</guid>
      <description>&lt;p&gt;You worked hard to collect, clean, and process your data. Now your dataset is finally ready for that final step. That put a bow on it and send it to all your colleagues phase. You are at that moment when you release a deep sigh of relief at the end of the hard (sometimes underappreciated) work of data wrangling. Now you have to make something that communicates all that you have done. A dataset is often only as good as its documentation. After all, it is rarely &lt;em&gt;only&lt;/em&gt; you, the person hunched over his/her computer for the past 2 weeks learning every nook and cranny of the data, who is going to need understand and explore the data’s contents. You can have different people ask you a million and one questions about where everything is and how it was all coded.&lt;/p&gt;
&lt;p&gt;Or…&lt;/p&gt;
&lt;p&gt;You could create a manual that is complete, easy to you use, readily tweakable, completely reproducible, and 100% shareable. Having made a couple of manuals in my time as a former camp director and current Ph.D. candidate, I appreciate well-put-together informational resources that I can consult to quickly find the answer I want.&lt;/p&gt;
&lt;p&gt;By integrating R’s powerful coding, graphing, and analytic abilities directly with LaTex, Rmarkdown lets you create a completely comprehensive record of everything you have done to your data (any errors can be easily tracked back to the code) while also producing a practically useful and aesthetically pleasing (at least to my eye) manual that can be shared with your research group. To see an example of its power check out an early draft of the manual I created for our current project (these data are raw, may contain as yet undiscovered inaccuracies and are not for public use). See the manual &lt;a href=&#34;https://mgb-research.netlify.com/pdf/parent_qualtrics.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interaction Plots with Continuous Moderators in R</title>
      <link>https://mgb-research.netlify.com/post/interaction-plots-with-continuous-moderators-in-r/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/interaction-plots-with-continuous-moderators-in-r/</guid>
      <description>&lt;p&gt;Long ago (the first half of my grad school life), I created a model for a manuscript I submitted. The paper was focused on adolescents’ appraisals of their relationships with their mothers, fathers, and best friends. Specifically, I wanted to test whether the association between different motivations for social withdrawal (i.e., removing oneself from social activities and interactions) and internalizing symptoms varied as a function of perceived support in any one (or all three) of these relationships.&lt;/p&gt;
&lt;p&gt;It is and was a modest study, with some flaws (notably the fact I only had self-report measures from the adolescents). If you want more context for the rest of this post, you can read the paper &lt;a href=&#34;https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/&#34;&gt;here&lt;/a&gt;. These data come from the &lt;em&gt;Friendship Project&lt;/em&gt; and were collected in the early-to-mid 2000s, a fact that I wish I had included in the manuscript in retrospect. My blog is as good a place as any to call out my past transparency shortcomings I suppose.&lt;/p&gt;
&lt;p&gt;To understand the data and models, here is some additional information:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;I had grade 8 measures of relationship quality, shyness, preference for solitude, and anxiety/depression&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I had grade 9 measures of anxiety and depression&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The analyses were based on a saturated path model in &lt;code&gt;lavaan&lt;/code&gt; - an R package for creating and testing structural equation models.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using &lt;code&gt;lavaan&lt;/code&gt; (&lt;strong&gt;la&lt;/strong&gt;tent &lt;strong&gt;va&lt;/strong&gt;riable &lt;strong&gt;an&lt;/strong&gt;alysis), I created three latent variables that represented outcomes of interest in grade 9: anxiety, depression, and general negative affect (I refer to this as dispositional negativity in the manuscript). To start, here is the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
mod8&amp;lt;-&amp;#39;
int=~cdi9ngmd+cdi9inpr+cdi9inef+cdi9anhe+cdi9ngse+masc9tr+masc9sma+masc9per+masc9ac+masc9hr+masc9pf
dep=~cdi9ngmd+cdi9inpr+cdi9inef+cdi9anhe+cdi9ngse
anx=~masc9tr+masc9sma+masc9per+masc9ac+masc9hr+masc9pf
int~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt
dep~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt+frsptXpfs+frsptXshy
anx~ysr8anxd+sex1+c.pfs+c.shy+
c.mospt+mopstXpfs+mopstXshy+
c.faspt+
c.frspt
#Covariances
int~~0*dep
int~~0*anx
anx~~0*dep
#Added covariances 
masc9hr ~~ masc9pf
cdi9inpr ~~ masc9per
cdi9inef ~~  masc9ac 
&amp;#39;

fit8&amp;lt;-sem(mod8, data=dat, std.lv = T, estimator = &amp;#39;MLR&amp;#39;)
summary(fit8, fit.measures=T, standardized=T, rsquare=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-3 ended normally after 66 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         65
## 
##                                                   Used       Total
##   Number of observations                           188         330
## 
##   Estimator                                         ML      Robust
##   Model Fit Test Statistic                     226.515     221.684
##   Degrees of freedom                               122         122
##   P-value (Chi-square)                           0.000       0.000
##   Scaling correction factor                                  1.022
##     for the Yuan-Bentler correction (Mplus variant)
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             1392.959    1307.712
##   Degrees of freedom                               176         176
##   P-value                                        0.000       0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.914       0.912
##   Tucker-Lewis Index (TLI)                       0.876       0.873
## 
##   Robust Comparative Fit Index (CFI)                         0.916
##   Robust Tucker-Lewis Index (TLI)                            0.878
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3975.960   -3975.960
##   Scaling correction factor                                  1.222
##     for the MLR correction
##   Loglikelihood unrestricted model (H1)      -3862.702   -3862.702
##   Scaling correction factor                                  1.091
##     for the MLR correction
## 
##   Number of free parameters                         65          65
##   Akaike (AIC)                                8081.919    8081.919
##   Bayesian (BIC)                              8292.288    8292.288
##   Sample-size adjusted Bayesian (BIC)         8086.402    8086.402
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.068       0.066
##   90 Percent Confidence Interval          0.054  0.081       0.052  0.079
##   P-value RMSEA &amp;lt;= 0.05                          0.020       0.030
## 
##   Robust RMSEA                                               0.067
##   90 Percent Confidence Interval                             0.052  0.080
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.049       0.049
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                   Robust.huber.white
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   int =~                                                                
##     cdi9ngmd          0.730    0.200    3.641    0.000    0.888    0.441
##     cdi9inpr          0.203    0.088    2.318    0.020    0.247    0.290
##     cdi9inef          0.490    0.166    2.944    0.003    0.597    0.380
##     cdi9anhe          1.237    0.207    5.985    0.000    1.506    0.637
##     cdi9ngse          0.577    0.134    4.308    0.000    0.702    0.470
##     masc9tr           1.573    0.248    6.345    0.000    1.915    0.650
##     masc9sma          1.447    0.218    6.645    0.000    1.761    0.679
##     masc9per         -0.733    0.299   -2.454    0.014   -0.892   -0.368
##     masc9ac          -0.644    0.373   -1.728    0.084   -0.784   -0.237
##     masc9hr           1.117    0.309    3.616    0.000    1.360    0.386
##     masc9pf           0.837    0.252    3.323    0.001    1.019    0.381
##   dep =~                                                                
##     cdi9ngmd          1.180    0.183    6.445    0.000    1.298    0.645
##     cdi9inpr          0.396    0.064    6.151    0.000    0.436    0.511
##     cdi9inef          0.789    0.146    5.402    0.000    0.868    0.553
##     cdi9anhe          1.185    0.184    6.442    0.000    1.304    0.551
##     cdi9ngse          0.971    0.141    6.876    0.000    1.068    0.714
##   anx =~                                                                
##     masc9tr           0.999    0.290    3.449    0.001    1.248    0.423
##     masc9sma          0.749    0.245    3.052    0.002    0.935    0.361
##     masc9per          1.381    0.241    5.730    0.000    1.725    0.711
##     masc9ac           2.168    0.276    7.861    0.000    2.708    0.817
##     masc9hr           1.296    0.262    4.955    0.000    1.618    0.459
##     masc9pf           1.129    0.208    5.431    0.000    1.410    0.527
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   int ~                                                                 
##     ysr8anxd          0.121    0.042    2.919    0.004    0.100    0.368
##     sex1              0.071    0.254    0.280    0.780    0.058    0.029
##     c.pfs            -0.097    0.163   -0.593    0.553   -0.080   -0.064
##     c.shy             0.408    0.183    2.227    0.026    0.335    0.213
##     c.mospt          -0.337    0.186   -1.814    0.070   -0.277   -0.153
##     mopstXpfs        -0.219    0.211   -1.039    0.299   -0.180   -0.089
##     mopstXshy        -0.437    0.279   -1.568    0.117   -0.359   -0.128
##     c.faspt           0.041    0.165    0.251    0.802    0.034    0.023
##     c.frspt          -0.176    0.210   -0.838    0.402   -0.145   -0.081
##   dep ~                                                                 
##     ysr8anxd          0.037    0.039    0.952    0.341    0.034    0.126
##     sex1              0.085    0.221    0.386    0.700    0.077    0.039
##     c.pfs            -0.039    0.136   -0.286    0.775   -0.035   -0.028
##     c.shy            -0.186    0.180   -1.035    0.301   -0.169   -0.107
##     c.mospt          -0.212    0.182   -1.168    0.243   -0.193   -0.107
##     mopstXpfs         0.211    0.250    0.843    0.399    0.191    0.094
##     mopstXshy        -0.178    0.255   -0.698    0.485   -0.162   -0.058
##     c.faspt          -0.219    0.124   -1.768    0.077   -0.199   -0.137
##     c.frspt           0.029    0.193    0.148    0.883    0.026    0.014
##     frsptXpfs        -0.414    0.213   -1.944    0.052   -0.377   -0.257
##     frsptXshy         0.847    0.268    3.159    0.002    0.770    0.282
##   anx ~                                                                 
##     ysr8anxd          0.122    0.034    3.562    0.000    0.098    0.362
##     sex1              0.391    0.220    1.780    0.075    0.313    0.156
##     c.pfs             0.321    0.137    2.348    0.019    0.257    0.207
##     c.shy             0.411    0.188    2.187    0.029    0.329    0.209
##     c.mospt           0.172    0.191    0.901    0.368    0.138    0.076
##     mopstXpfs         0.110    0.217    0.508    0.612    0.088    0.043
##     mopstXshy        -0.112    0.269   -0.418    0.676   -0.090   -0.032
##     c.faspt           0.102    0.142    0.716    0.474    0.082    0.056
##     c.frspt           0.108    0.191    0.567    0.570    0.087    0.048
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .int ~~                                                                
##    .dep               0.000                               0.000    0.000
##    .anx               0.000                               0.000    0.000
##  .dep ~~                                                                
##    .anx               0.000                               0.000    0.000
##  .masc9hr ~~                                                            
##    .masc9pf           1.917    0.512    3.746    0.000    1.917    0.392
##  .cdi9inpr ~~                                                           
##    .masc9per         -0.351    0.129   -2.723    0.006   -0.351   -0.305
##  .cdi9inef ~~                                                           
##    .masc9ac          -0.311    0.219   -1.417    0.156   -0.311   -0.135
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .cdi9ngmd          1.493    0.270    5.524    0.000    1.493    0.369
##    .cdi9inpr          0.469    0.097    4.822    0.000    0.469    0.644
##    .cdi9inef          1.320    0.187    7.075    0.000    1.320    0.535
##    .cdi9anhe          1.483    0.265    5.592    0.000    1.483    0.265
##    .cdi9ngse          0.547    0.123    4.450    0.000    0.547    0.245
##    .masc9tr           2.346    0.392    5.985    0.000    2.346    0.270
##    .masc9sma          1.977    0.373    5.293    0.000    1.977    0.294
##    .masc9per          2.831    0.714    3.965    0.000    2.831    0.481
##    .masc9ac           4.023    1.250    3.217    0.001    4.023    0.367
##    .masc9hr           6.933    0.895    7.747    0.000    6.933    0.558
##    .masc9pf           3.458    0.471    7.345    0.000    3.458    0.483
##    .int               1.000                               0.675    0.675
##    .dep               1.000                               0.826    0.826
##    .anx               1.000                               0.641    0.641
## 
## R-Square:
##                    Estimate
##     cdi9ngmd          0.631
##     cdi9inpr          0.356
##     cdi9inef          0.465
##     cdi9anhe          0.735
##     cdi9ngse          0.755
##     masc9tr           0.730
##     masc9sma          0.706
##     masc9per          0.519
##     masc9ac           0.633
##     masc9hr           0.442
##     masc9pf           0.517
##     int               0.325
##     dep               0.174
##     anx               0.359&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there is a whole script file of assumption checking and model evaluation that goes along with this (that is how I got to model 8). I have just included the final model here for simplicity’s sake.&lt;/p&gt;
&lt;p&gt;One thing that frustrates me when we report on interactions between variables is that we often pick static, potentially arbitrary values to probe simple slopes. A common pair of values is +/- 1 &lt;em&gt;SD&lt;/em&gt;. Sometimes researchers will also examine simple slopes at the mean of the sample as well. Probing simple slopes at static values also influences how researchers present their findings visually. Typically, we get either a pair of bars or lines to use to help us better evaluate the nature of the detected interaction.&lt;/p&gt;
&lt;p&gt;Here is where my gripe starts to eek in. Why are we probing simple slopes at static (often arbitrary) values? When our moderators are continuous it seems to me the better approach is to evaluate and interpret the effect of the moderator on the association between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; along the entire continuum of plausible moderator values.&lt;/p&gt;
&lt;p&gt;To do this, I need to extract some information from my model first. I’ll need the point estimates and their variances invovled in defining the moderation (pro-tip, I am using the &lt;code&gt;parm[,1:3]&lt;/code&gt; line to figure out the correct numbering for the parameters in the model):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parm&amp;lt;-as.data.frame(parameterEstimates(fit8))
parm[,1:3]

COV&amp;lt;-vcov(fit8)
COVd&amp;lt;-as.data.frame(COV)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay so now that I have the information, I can grab the relevant values as follows. (You don’t need to grab things from the model using R objects - you can type in values manually here instead should you wish).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&amp;lt;-parm[26,4]#slope for SHY
b3&amp;lt;-parm[29,4]#slope for SHY x Maternal Support
s11&amp;lt;-COV[26,26]#variance for SHY
s13&amp;lt;-COV[26,29]#covariance for SHY and SHY x Maternal Support
s33&amp;lt;-COV[29,29]#variance for SHY x MS Parameter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then need to create a vector of values that I am going to use for plotting. Since the long-standing convention has been to use standardized values of the moderator for probing simple slopes, I keep with that tradition in the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd1&amp;lt;-c(-2, -1.75, -1.5, -1.25, -1, -.75, -.5, -.25, 0,
       .25,.5,.75,1,1.25,1.5,1.75,2)

sd2&amp;lt;-sd1*sd(dat$c.mospt)

#Formula for standard error of simple slopes can be found in most regression textbooks 
se&amp;lt;-sqrt(s11+2*(sd2)*s13+(sd2)*(sd2)*s33)
se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.3605632 0.3259049 0.2928709 0.2620761 0.2344049 0.2110890 0.1937077
##  [8] 0.1839510 0.1830423 0.1911077 0.2071014 0.2293710 0.2562856 0.2865392
## [15] 0.3191838 0.3535578 0.3892032&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&amp;lt;-b1+b3*(sd2)

b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.94764891  1.75514194  1.56263497  1.37012800  1.17762103
##  [6]  0.98511406  0.79260710  0.60010013  0.40759316  0.21508619
## [11]  0.02257922 -0.16992775 -0.36243472 -0.55494169 -0.74744865
## [16] -0.93995562 -1.13246259&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;UB&amp;lt;-b1+se*qnorm(.975)
LB&amp;lt;-b1+se*qnorm(.025)

DF.temp&amp;lt;-data.frame(b1, sd1, UB, LB)

library(ggplot2)
g1&amp;lt;-ggplot(DF.temp)+
  geom_ribbon(aes(ymin=LB, ymax=UB, x=sd1), alpha=.7)+
  geom_line(aes(y=b1, x=sd1))+
  geom_hline(aes(yintercept=0), lty=&amp;#39;dashed&amp;#39;)+
  xlab(&amp;#39;Standardized Maternal Support Scores&amp;#39;)+
  ylab(&amp;#39;Simple Slope for Shyness-DN Association&amp;#39;)+
  ggtitle(&amp;quot;Shyness Predicting Dispositional Negativity as a Function of Maternal Support&amp;quot;)


g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-07-12-interaction-plots-with-continuous-moderators-in-r_files/figure-html/graph2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila! To me, the greatest value of this approach is that I can see exactly where 0 is included in the 95% confidence band and where the value falls outside the confidence band. For instance, in the plot above, I can easily see that the model would only predict a significant and positive association between shyness and dispositional negativity for adolescents with lower levels of self-reported maternal relationship quality.&lt;/p&gt;
&lt;p&gt;Note that you may have to change the y-axis scaling to get the plot to display correctly. Otherwise, you now never have to ever ever plot the interaction between two continuous variables using a pair of static variables.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data: Complete Work Flow - Part 1 of 3</title>
      <link>https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/</guid>
      <description>&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview:&lt;/h1&gt;
&lt;p&gt;This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for missingness. The third and final post will be a walkthrough of the final models and their interpretation - including a comparison of the same models using listwise deletion (which is bad unless missingness is small or definitely, 100% completely at random).&lt;/p&gt;
&lt;div id=&#34;data-brief-description&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Brief Description:&lt;/h2&gt;
&lt;p&gt;The data in question come from a study that involved a one-week ecological momentary assessment (EMA) protocol. For seven consecutive days, participants (&lt;em&gt;N&lt;/em&gt;=127) responded to 10 prompts delivered at pseudo-random times. The timing of EMA probes was built around class schedules during the day (hence pseudo-random). More detail about the sample and procedures can be found &lt;a href=&#34;https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As might be expected, some prompts were not responded to or were responded to outside the response time window (which was 30 minutes in this case). In the present data set, what I cared about was predicting global mood using a measure of trait negativity (sometimes referred to as neuroticism, negative emotionality, or dispositional negativity).&lt;/p&gt;
&lt;p&gt;The problem is that I believe that there are certain factors about individuals that make them less likely to complete all of their surveys. Technically speaking, I believe that the data are likely missing at random (that is that missingness is conditional on some set of predictors/covariates but not on the outcome measure).&lt;/p&gt;
&lt;p&gt;There is a debate over whether researchers should impute or simply use full information maximum likelihood to address missingness in cases like the one represented by this current set of data. All things being equal, properly specified models using either approach will result in essentially the same result. Full information maximum likelihood (FIML) estimation is convenient, when it can be used, as it is often seamlessly intergrated into the modeling sofware.&lt;/p&gt;
&lt;p&gt;The seamlessness of its integration can, however, sometimes lead to bad modeling practices. Being a little out of sight and a little out of mind, relying on FIML to resolve the effects of missing data on biasing estimates can be just a hair too easy. On the user side it is: set up the model, click run, and wait, filled with the confidence that comes from being repeatedly told that FIML can be used to deal with missing data, so long as the technical condition of missingness at random is met.&lt;/p&gt;
&lt;p&gt;When imputing, probing for predictors of missingness is built into the process. It is a step that cannot be skipped as you are essentially building a model with other variables in your data set to predict missing scores. For my money, imputation makes you consider more thoughtful choices regarding modeling missingness. With FIML, it can be a little too easy to perform an analysis with little to no examination of the reasons for missingness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1---understanding-and-exploring-missingness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1 - Understanding and Exploring Missingness&lt;/h2&gt;
&lt;p&gt;Before anything else we need to understand a little about the missingness in the data. For starters, how much missingness are we talking about here? That is relatively simple to answer. But even before that, a few pieces of information that may help readers understand this data set a bit better in the form of some variable names and definitions:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ID&lt;/code&gt;: ID variable&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EMA Variables&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;c.Worst&lt;/code&gt;: Individually mean-centered ratings of how &lt;em&gt;unpleasant&lt;/em&gt; the worst event was that occurred in the past hour&lt;/p&gt;
&lt;p&gt;&lt;code&gt;c.Best&lt;/code&gt;: Individually mean-centered ratings of how &lt;em&gt;pleasant&lt;/em&gt; the best event was that occurred in the past hour&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NegAff&lt;/code&gt;: Three-item composite (scores can range from 1 to 5) assessing momentary negative affect - mostly anxious affect (i.e., &lt;em&gt;anxious&lt;/em&gt;, &lt;em&gt;worried&lt;/em&gt;, &lt;em&gt;nervous&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PosAff&lt;/code&gt;: Three-item composite (scores can range from 1 to 5) assessing momentary positive affect - mostly cheerful affect (i.e., &lt;em&gt;cheerful&lt;/em&gt;, &lt;em&gt;happy&lt;/em&gt;, &lt;em&gt;joyful&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Individual Variables:&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;c.DN&lt;/code&gt;: Mean-centered dispositional negativity scores (combination of Neuroticism from Big Five and additional items that tap the anxious facet of neuroticism)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BFI_O&lt;/code&gt;: Big Five Openness Factor&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BFI_C&lt;/code&gt;: Big Five Conscientiousness Factor&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BFI_E&lt;/code&gt;: Big Five Extraversion Factor&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dep&lt;/code&gt;: IDAS general depression score&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Miscellaneous:&lt;/strong&gt;&lt;br /&gt;
&lt;code&gt;IMP&lt;/code&gt;: 0 is used to mark these data as “original” (will be useful after imputation)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;miss&lt;/code&gt;: 1 if missing EMA measures, 0 if not (more on this later)&lt;/p&gt;
&lt;p&gt;In this study design, missingness is possible in any one of the EMA variables, though in practice if one of the four scores in question were missing, all scores for that period were typically missing (only 2 instances when this was not the case - see figure below). To understand missingness in this data set a little better, let’s take a closer look, now that we have a better understanding of what every variable represents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mice::md.pattern(dat.study1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow_files/figure-html/missing-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      ID c.DN BFI_O BFI_C BFI_E BFI_A Dep Gender IMP c.Worst c.Best NegAff
## 7003  1    1     1     1     1     1   1      1   1       1      1      1
## 2     1    1     1     1     1     1   1      1   1       1      1      1
## 2     1    1     1     1     1     1   1      1   1       1      1      0
## 1883  1    1     1     1     1     1   1      1   1       0      0      0
##       0    0     0     0     0     0   0      0   0    1883   1883   1885
##      PosAff     
## 7003      1    0
## 2         0    1
## 2         1    1
## 1883      0    4
##        1885 7536&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative way to look at these data is to use the &lt;code&gt;VIM&lt;/code&gt; package in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VIM::aggr(
  dat.study1[
    c(
      &amp;#39;c.Worst&amp;#39;, 
      &amp;#39;c.Best&amp;#39;,
      &amp;#39;NegAff&amp;#39;,
      &amp;#39;PosAff&amp;#39;
      )
    ]
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow_files/figure-html/VIM-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So now I can see that I am missing about 20% of data related to momentary events and mood. The next question is whether any of the individual-level variables available to me are related to missingness. First, I am going to need a indicator of missingness.&lt;/p&gt;
&lt;p&gt;Let’s begin by getting a missingness variable in a new missing data set. Grabbing just the comlumns that I need from the original:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat.miss&amp;lt;-data.frame(
  dat.study1[,c(1,4,7:12)], 
  miss = ifelse(!is.na(dat.study1$PosAff), &amp;quot;Complete&amp;quot;, &amp;quot;Missing&amp;quot;)
)

table(dat.miss$miss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Complete  Missing 
##     7005     1885&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a proportion of .212 (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\frac{1885}{8890} = .212\)&lt;/span&gt;), it looks as if I have successfully created my missingness variable. Now, it is time to plot. Let`s start with the 4 BFI factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=BFI_O, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;BFI Openess Scores (1-5)&amp;#39;)

g2&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=BFI_C, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;BFI Conscientiousness Scores (1-5)&amp;#39;)

g3&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=BFI_E, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;BFI Extraversion Scores (1-5)&amp;#39;)


g4&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=BFI_A, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;BFI Agreeableness Scores (1-5)&amp;#39;)

gridExtra::grid.arrange(g1, g2, g3, g4,
                        nrow=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow_files/figure-html/plotmiss-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is not much here that would suggest that missingness was related to these personality scores. The distributions look pretty similar. This is an empirical question, though and it is possible, therefore, to determine if any of these measures predict the likelihood of missingness. I am also not considering the nested nature of the data here, just plotting in an absolute sense. To address the issue on more empirical grounds then, I can use the &lt;code&gt;lmer&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.miss&amp;lt;-glmer(
  miss~1+BFI_O+BFI_C+BFI_E+BFI_A+(1|ID), 
  data=dat.miss, 
  family = &amp;#39;binomial&amp;#39;
  )

fit.boot&amp;lt;-bootMer(fit.miss, 
                  FUN = fixef, 
                  nsim=5000, 
                  parallel=&amp;#39;multicore&amp;#39;, 
                  ncpus = 10
                  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit.boot, type=&amp;#39;perc&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bootstrap percent confidence intervals
## 
##                   2.5 %      97.5 %
## (Intercept) -1.94877015  0.19115816
## BFI_O        0.01754566  0.37859218
## BFI_C       -0.39542260 -0.01822857
## BFI_E       -0.05313720  0.18344948
## BFI_A       -0.37821070  0.01239628&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to this model, it seems as though the BFI Openness factor and the BFI Conscientiousness factor both predict the likelihood of failing to complete an assessment (i.e., 0 not included in the interval).&lt;/p&gt;
&lt;p&gt;Let’s also look at our measures of dispositional negativity and general depression scores as potential variables of interest for the imputation model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g5&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=c.DN, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;Centered DN Scores&amp;#39;)

g6&amp;lt;-ggplot(
  data = dat.miss, 
  aes(x=Dep, group=miss, color=miss)
  )+
  geom_freqpoly(stat = &amp;#39;density&amp;#39;)+
  theme(legend.title = element_blank())+
  ylab(&amp;#39;&amp;#39;)+
  xlab(&amp;#39;IDAS Depression Scores&amp;#39;)

gridExtra::grid.arrange(g5, g6,
                        nrow=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-07-05-bayesian-multilevel-model-with-missing-data-complete-work-flow_files/figure-html/plotmiss2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a little bit of a right shift in the distribution of DN scores when split by missing vs. complete. Since the two variables (depression and dispostional negativity scores) are also pretty strongly correlated (introducing a good deal of multicollinearity in the model), I am just going to use the dispositional negativity variable as it is most similar to the other personality variables examined above (also it is going in my model anyway). I also want to check to make sure that gender is not a factor in completion rates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.miss2&amp;lt;-glmer(
  miss~1+c.DN+Gender+(1|ID), 
  data=dat.miss, 
  family = &amp;#39;binomial&amp;#39;, 
  )

fit.boot2&amp;lt;-bootMer(fit.miss2, 
                   FUN = fixef, 
                   nsim=5000, 
                   parallel=&amp;#39;multicore&amp;#39;, 
                   ncpus = 10
                   )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit.boot2, type = &amp;#39;perc&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bootstrap percent confidence intervals
## 
##                  2.5 %      97.5 %
## (Intercept) -1.5177761 -1.16884330
## c.DN        -0.1188116  0.14345555
## Gender      -0.4123946  0.08398782&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is a no to DN and a no to Gender. At this stage, assuming these are the only relevant variables I have access to (in actuality there are more to consider in the complete data set - this is just a toy problem), I have identified the variables I will need to include when creating my imputation model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Okay to wrap it up for post 1 of 3… Missing data is a problem when analyzing data sets, even relatively large ones. We know from decades of research that missingness can bias both estimates and their standard errors (depending on the reasons for missingness). Two techniques, FIML and multiple imputation have been used over the years to address the problems caused by missing data, when the technical condition of “missing at random” is met.&lt;/p&gt;
&lt;p&gt;FIML is great and wonderful, but it can lead to bad practices owing to its ease of use (and often seamless integration with certain software tools). Multiple imputation can take longer and is a more invovled technique, but it forces you to think about the missingness more directly. An additional benefit is that multiple imputation will work with generally any type of statistical model, FIML by comparison requires maximum likelihood - an estimation that works for many but not all models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generalization of an early intervention for inhibited preschoolers to the classroom setting</title>
      <link>https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/barstead_etal_2018_jcfs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Withdrawing from the peer group</title>
      <link>https://mgb-research.netlify.com/publication/rubin_etal_2018_handbook/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/rubin_etal_2018_handbook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>&lt;p&gt;Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. Having already collected data (i.e., having a fixed sample size), the goal of the power analysis was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;) for a secondary research question that was not part of the original proposal (we added collection of a set of variables partway through the data collection period).&lt;/p&gt;
&lt;p&gt;We collected measures of these variables at three time points, approximately evenly spaced apart, and, for the purposes of these analyses, I decided to treat the data as if they were collected at precisely the same equally spaced interval for all participants. Though this is not technically true, it is sufficiently true for the purposes of these analyses. Modifying the code to take into account the actual difference in time between individual assessments is entirely possible and potentially important depending on your measures and design.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; In short, I need a reasonable population model for my data. Ideally, this model is grounded in both theory and empirical findings.&lt;/p&gt;
&lt;p&gt;To create a reasonable population model, even for a relatively simple analysis (the present working example qualifies as such), I need to think through what I know about the treatment effects and the target population. For instance, we know that data in the present case were obatined from a selected sample of children, who were eligible to participate if their scores on a &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1046/j.1467-8624.2003.00645.x&#34;&gt;measure of early childhood anxiety risk&lt;/a&gt; exceeded the 85th percentile.&lt;/p&gt;
&lt;p&gt;This knowledge provides useful information when considering population priors. For instance, I should expect the obtained sample to be drawn from a population elevated in anxiety and other, related, symptoms of emotional and/or behavioral maladaptation at time 1.&lt;/p&gt;
&lt;p&gt;This is useful until I need to consider what I mean by “elevated,” at least insofar as how I will define it numerically in the model. To address this definitional issue, it is helpful to adopt a scale (sometimes a standardized scale is a particularly good option given the easy and obvious interpretation of scores).&lt;/p&gt;
&lt;p&gt;For the present set of analyses, I am going to attempt to place my outcome measure on a pseudo-&lt;em&gt;T&lt;/em&gt;-score (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\mu \approx 50, \sigma \approx 10\)&lt;/span&gt;), that is approximately normally distributed in the population (note that I am referring to a clinical &lt;em&gt;T&lt;/em&gt;-score not a &lt;em&gt;t&lt;/em&gt; distribution). In a Bayesian sense, I am setting a prior; namely that I believe the obtained sample was randomly drawn from a population of children with elevated scores on a measure of maladaptation.&lt;/p&gt;
&lt;p&gt;So far I have only settled on a starting point (i.e., the intercept), but I have a number of parameters I need to consider specfying, and most importantly a number of parameters about which I am somewhat uncertain. To see what other parameters I need to consider, it is perhaps useful to review the simple linear growth model (specified below using &lt;a href=&#34;https://books.google.com/books/about/Hierarchical_Linear_Models.html?id=uyCV0CNGDLQC&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;Raudenbush &amp;amp; Bryk’s&lt;/a&gt; notation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Level 1 Equation:&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y_{ti} = \pi_{0i} + \pi_{1i}(Time) + e_{ti}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The level 1 equation includes the coefficient &lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt; which represents the average predicted value for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(Time=0\)&lt;/span&gt;. For this reason researchers typically center their intercepts at a meaningful value. In the present analyses &lt;span class=&#34;math inline&#34;&gt;\(Time\)&lt;/span&gt; will coded such that &lt;span class=&#34;math inline&#34;&gt;\(T_1 = 0, T_2 = 1,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_3 = 2\)&lt;/span&gt;. With this specification, the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\pi_{0i}\)&lt;/span&gt;}) represents the predicted value for an outcome (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;) prior to the start of the intervention (note that I am also including a random effect in this model specification - more on that below).&lt;/p&gt;
&lt;p&gt;Average change over time is represented in the model by &lt;span class=&#34;math inline&#34;&gt;\(\pi_{1i}\)&lt;/span&gt;. Any one case, however, likely deviates to some degree from the average model of change. These deviations are sometimes referred to as random effects. In the context of a growth model, they represent the degree to an individual case’s slope (or intercept) deviates from the estimated average. Using Raudenbush &amp;amp; Bryk’s (2002) notation, these random effects are defined below, where the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;’s represent the fixed effects and the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;’s represent individual deviations from the estimated fixed effects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Level 2 Equations:&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\pi_{0i} = \beta_{00} + r_{0i}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\pi_{1i} = \beta_{10} + r_{1i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a quick toy example, let’s see what putting priors into practice actually means. Say I have 10 cases measured repeatedly over 5 equally spaced time points. Over the course of the entire data collection window, I expect that I will see an average decrease of -2.5 units in the outcome measure. Additionally, I expect that while most participants will exhibit negaive change overall, it is possible that some cases will experience positive change - I want to make sure that my priors allow for this possibility in practice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(5499)

N&amp;lt;-10
Time.vals&amp;lt;-rep(0:4, times=N)  #5 time points
ID&amp;lt;-rep(1:N, each=5)
pred.y&amp;lt;-vector()
Time&amp;lt;-0:4

#Hyperparamters - fixed effects for slope and intercept
b_00&amp;lt;-50
b_10&amp;lt;--0.5
  for(n in 1:N){
    #Level 1 error
    e_ti&amp;lt;-rnorm(5, 0, 3)
    
    #Level 2 sources of error
    r_0i&amp;lt;-rnorm(1, 0, 5)
    r_1i&amp;lt;-rnorm(1, 0, 1.5)
    
    #Level 1 coefficients 
    pi_0i&amp;lt;-b_00+r_0i
    pi_1i&amp;lt;-b_10+r_1i
    
    #Outcome incoporating level 1 error
    pred.y&amp;lt;-c(pred.y, pi_0i+pi_1i*Time+e_ti)
  }


DF.temp&amp;lt;-data.frame(pred.y, ID, Time.vals)
DF.temp$ID&amp;lt;-as.factor(DF.temp$ID) #a little easier to work with a grouping factor here. 
g1&amp;lt;-ggplot(data=DF.temp)+
  geom_line(aes(x=Time.vals, y=pred.y, group = ID, color=ID), lty=&amp;#39;dashed&amp;#39;, alpha=.75)+
  geom_smooth(aes(x=Time.vals, y=pred.y, group=ID, color=ID), method = &amp;#39;lm&amp;#39;, se=F)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/samp_growth-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the plot above, let’s see if I got what I wanted (note the dashed lines are the raw data for each case). While most cases did in fact decline on average, there are two cases for which scores on the outcome measure increased. Of course, I would need to inspect a larger number of randomly generated datasets (according to my population model) to know for sure. I recommend plotting 10-15 randomly generated data sets and ensuring that the average change and individual heterogeneity of change approximate your expectations.&lt;/p&gt;
&lt;p&gt;Whenever conducting a power analysis, to the degree possible, it is important to ensure that the assumptions you are building into that analysis are meaningfully grounded in some way - typically tethering a rationale to theory, empirical reports, or both.&lt;/p&gt;
&lt;p&gt;Though true, this framework encourages thinking at the level of the overall effect. We often turn to meta-analyses or other reviews that have attempt to describe or define the population-level effect. Less often do these sorts of reviews speak to individual variability from mean estimates.&lt;/p&gt;
&lt;p&gt;The net result is that we sometimes struggle to define our expectations about variability. This is where plotting the data is so valuable in my mind. I can quickly see whether or not individual trajectories are departing wildly from what is reasonable. If so, I can tweak certain aspects of the model and re-inspect until I am satisfied with the results.&lt;/p&gt;
&lt;p&gt;We always stress plotting your real data. Same goes for the made-up stuff too.&lt;/p&gt;
&lt;p&gt;Once I have sufficiently tuned up my population model, the next trick is to simulate a sufficiently large number of data sets to estimate power for my proposed model of change. Briefly, I will note that I am not really addressing the possibility that there is a meaningful correlation between individual random effects. I’ll save that for a future post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Solution:&lt;/strong&gt; Now that I have something of a framework created, I can apply it to my problem in a more direct fashion. I should state at the outset that this simulation is going to take some time. That is because I have chosen to evaluate significance using coverage of &lt;code&gt;0&lt;/code&gt; by a 95% boostrapped confidence interval. I tend to prefer this over say a p-value generated by the &lt;code&gt;lmer()&lt;/code&gt; function using the {lmerTest} library. This may be overkill, but it is the way I would typically assess whether coefficients in the model meaningfully differ from 0, so it is the approach I will be using to assess power as well.&lt;/p&gt;
&lt;p&gt;My goal with the analysis presented below is to assess the power of the model to detect significant negative linear change with the expectation that the overall effect in the population is relatively small (I’ll be using &lt;em&gt;Cohen’s&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as a guide for evaluating effect size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The libraries used
library(lme4)
library(merTools)
library(boot)
library(sjstats)
library(ggplot2)

#Specifying fixed effects for model
b_00&amp;lt;-65        #Fixed intercept: Average starting point on a pseudo-t-score scale)
b_10&amp;lt;--1.5      #Fixed slope: Change per unit of time 
Time&amp;lt;-0:2       #Vector of equally-spaced intervals

#Setting up some empty vectors for the simulation to fill in
#------------------------------------------------------------------------------------
#Intercept vectors
b00_Est&amp;lt;-vector()
b00_boot_se&amp;lt;-vector()
b00_boot_LB&amp;lt;-vector()
b00_boot_UB&amp;lt;-vector()
b00_var&amp;lt;-vector()

#Slope vectors
b10_Est&amp;lt;-vector()
b10_boot_se&amp;lt;-vector()
b10_boot_LB&amp;lt;-vector()
b10_boot_UB&amp;lt;-vector()
b10_var&amp;lt;-vector()

#Capturing variability in Y at multiple levels and overall
ICC.vals&amp;lt;-vector()
sd.y&amp;lt;-vector()
CohensD&amp;lt;-vector()
#------------------------------------------------------------------------------------
#Select number of simulations &amp;amp; Sample size
n.sims&amp;lt;-2500  #number of total simulations to run - recommend &amp;gt; 5,000
N&amp;lt;-120          #Sample size 

for(s in 1:n.sims){
  #browser()
  Time.vals&amp;lt;-rep(0:2, times=N)
  IDs&amp;lt;-rep(1:N, each=3)
  pred.y&amp;lt;-vector()
  for(n in 1:N){
    #Level 1 error
    e_ti&amp;lt;-rnorm(3, 0, 5)
    
    #Level 2 sources of error
    r_0i&amp;lt;-rnorm(1, 0, 5)
    r_1i&amp;lt;-rnorm(1, 0, 2.5)
    
    #Level 1 coefficients 
    pi_0i&amp;lt;-b_00+r_0i
    pi_1i&amp;lt;-b_10+r_1i
    
    #Outcome incoporating level 1 error
    pred.y&amp;lt;-c(pred.y, pi_0i+pi_1i*Time+e_ti)
  }
  
  DF&amp;lt;-data.frame(ID=IDs, 
                 Time=Time.vals, 
                 Y=pred.y)
  
  fit.null&amp;lt;-lme4::lmer(Y~1+(1|ID), DF)
  ICC.vals&amp;lt;-c(ICC.vals, as.numeric(sjstats::icc(fit.null)))
  sd.y&amp;lt;-c(sd.y, sd(pred.y))
  CohensD&amp;lt;-c(CohensD, effsize::cohen.d(c(DF$Y[DF$Time==2], DF$Y[DF$Time==0]), f=rep(c(&amp;#39;T3&amp;#39;, &amp;#39;T1&amp;#39;), each=120))$estimate)
  fit.ucgm&amp;lt;-lme4::lmer(Y~1+Time + (1+Time|ID), data=DF)
  
  boot.ucgm&amp;lt;-bootMer(fit.ucgm, FUN=fixef, type = &amp;#39;parametric&amp;#39;,
                     nsim=1000, parallel = &amp;#39;multicore&amp;#39;, ncpus=12)
  
  #obtaining CIs for intercept
  b00_Est&amp;lt;-c(b00_Est, mean(boot.ucgm$t[,1]))
  b00_boot_se&amp;lt;-c(b00_boot_se, sd(boot.ucgm$t[,1]))
  b00_boot_LB&amp;lt;-c(b00_boot_LB, b00_Est[s]+qt(.975, N-1)*b00_boot_se[s])
  b00_boot_UB&amp;lt;-c(b00_boot_UB, b00_Est[s]+qt(.025, N-1)*b00_boot_se[s])

  #obtaining CIs for time slope
  b10_Est&amp;lt;-c(b10_Est, mean(boot.ucgm$t[,2]))
  b10_boot_se&amp;lt;-c(b10_boot_se, sd(boot.ucgm$t[,2]))
  b10_boot_LB&amp;lt;-c(b10_boot_LB, b10_Est[s]+qt(.975, N-1)*b10_boot_se[s])
  b10_boot_UB&amp;lt;-c(b10_boot_UB, b10_Est[s]+qt(.025, N-1)*b10_boot_se[s])

  #Obtaining estimates of variability in slope and intercept
  b00_var&amp;lt;-c(b00_var, as.numeric(VarCorr(fit.ucgm)$ID[1,1]))
  b10_var&amp;lt;-c(b10_var, as.numeric(VarCorr(fit.ucgm)$ID[2,2]))
  print(paste(s, &amp;#39;out of&amp;#39;, n.sims, &amp;#39;simulations&amp;#39;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the simulation has finished, it is time to combine the output and plot. I cannot stress enough how important it is to plot things. Again, I am looking to see whether or not this model has returned reasonable estimates.&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;Pro tip&lt;/em&gt;: run your whole simulation code with a much smaller number of simulations to start - say 30 - inspect the results, then run the full simulation code if everything checks out.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_small&amp;lt;-data.frame(b00_Est=b00_Est, 
                      b00_se=b00_boot_se,
                      b00_var=b00_var, 
                      b00_boot_LB = b10_boot_LB,
                      b00_boot_UB = b10_boot_UB,
                      b10_Est=b10_Est, 
                      b10_se=b10_boot_se, 
                      b10_var=b10_var,
                      b10_boot_LB = b10_boot_LB, 
                      b10_boot_UB = b10_boot_UB,
                      sd.y=sd.y, 
                      CohensD=CohensD,
                      ICC=ICC.vals)

#plotting distributions returned from the simulations
#Selecting only certain columns related to fixed effects 
bayesplot::mcmc_dens(dat_small[,c(1:2,6:7)])+
  ggtitle(&amp;#39;Variances of Slope Estimates - Small Effect Model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/pow_sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about some of the basic properties of the data? Was the average linear effect relatively small (which was the goal of this analysis after all)? What about the intra-class correlations returned by each data set; were they reasonable given the design and expectations based on similar data sets?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesplot::mcmc_dens(dat_small[,12:13])+
  ggtitle(&amp;#39;Model Diagnostics - Small Effect Model&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-06-07-power-analyses-for-an-unconditional-growth-model-using-lmer_files/figure-html/plot_diag-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I would say yes to both. The maximum a posteriori estimate (MAPE) for standardized average change is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(dat_small$CohensD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.3826507&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat_small$CohensD, c(.025, .975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       2.5%      97.5% 
## -0.5780296 -0.1765590&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which to me seems entirely reasonable to classify as a “small” effect power analysis, with the added bonus that you can clearly see this approach reflects uncertainty about the magnitude of change observed in any one sample.&lt;/p&gt;
&lt;p&gt;So how do I get power? It is simple. I am interested in the power to detect negative change at &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;, given the population model I created, in a sample of 120 subjects. Okay so not that simple. Since I saved the upper boundaries of the 95% CIs for my estimates I could add up all of the times the upper boundary for a given sample was less than 0 (an indication that a significant effect was detected), divide that total by the total number of simulations and voila, I’d have a calculation of power.&lt;/p&gt;
&lt;p&gt;Using a binary variable it is even easier…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig&amp;lt;-ifelse(dat_small$b10_boot_UB&amp;lt;0, 1, 0)
mean(sig)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(dat_small$b10_boot_UB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.6303266&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the maximum upper boundary for model-based 95%-bootstrapped CIs for the fixed slope was negative - which is why I get a probability of 1 using the &lt;code&gt;mean(sig)&lt;/code&gt; call. If interested I could use this information to start working my way down iteratively to a minimum effect size detectable at a rate of 80%, given the population model and a sample size of 120.&lt;/p&gt;
&lt;p&gt;Or… Instead of doing all of that trial and error work, I could create a program to do it, one that has its code written more efficiently, and one that does a little more parallelizing. All for a future post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mgb-research.netlify.com/post/gaussian-process-imputation-models/</guid>
      <description>&lt;p&gt;A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.&lt;/p&gt;
&lt;p&gt;The models below will make use of the preloaded &lt;code&gt;AirPassengers&lt;/code&gt; data in R. The data represent the total number of monthly international airline passengers (in thousands) from 1949 to 1960. It is easy to see these data have both a non-stationary mean and a non-stationary variance. There is also a clear periodic component to these data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;#39;AirPassengers&amp;#39;)
plot(AirPassengers)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/AirPassengers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a toy problem, I am going to focus on the application of a Gaussian process model to forecasting future monthly passengers. This is not the only way one could try to solve this prediction problem. I offer it as a means of understanding the potential power that exists in using these sorts of models for prediction and imputation problems involving univariate time series data.&lt;/p&gt;
&lt;p&gt;A few notes about Gaussian process models. To start they are a class of Bayesian models. There are a few &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;Python&lt;/code&gt; packages that allow researchers to use this modeling approach. I have become something of a &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; convert recently, but it is not the only option out there.&lt;/p&gt;
&lt;p&gt;The authoritative text on Gaussian process models was arguably published by &lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/RW.pdf&#34;&gt;Rasumssen &amp;amp; Williams in 2006&lt;/a&gt;, but only recently have computing power and programming languages allowed for a deeper tapping of this methodology’s strengths. For anyone interested in learning more about these models I highly recommend the Rasmussen &amp;amp; Williams (2006) text as a starting point.&lt;/p&gt;
&lt;p&gt;It is worth pointing out that, because Guassian process models rely on Bayesian estimation, parameters either need to be fixed or given a prior distribution. I like that Bayesian analyses really make you think about your priors. It is the statistical equivalent of eating your veggies. You may not always enjoy it, but it will do you good in the long run. Strategies for choosing priors are beyond the purpose of this post. If interested, the following &lt;a href=&#34;https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations&#34;&gt;page on the Stan GitHub repo&lt;/a&gt; provides a brief, but reasonable overview as a starting point.&lt;/p&gt;
&lt;p&gt;First, the data need a bit of prepping to be fed into a Stan program.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Obtaining a numeric vector for time. Maintaining the units of measure at 1 = 1 year
Year&amp;lt;-seq(1949, 1960+11/12, by=1/12)

#converting time-series to a vector
Pass&amp;lt;-as.vector(AirPassengers)

#identifiying number of data points for the &amp;quot;training&amp;quot; data
N1&amp;lt;-length(Year)

#specifying 2-year prediction window. 
year.fore2&amp;lt;-seq(1961, 1962+11/12, by=1/12)
N2&amp;lt;-length(year.fore2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data prepped, I will run the first of two models. The first model relies solely on the squared exponential covariance function (plus error) to define the underlying Gaussian process. The squared exponential function takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;) = \sigma^2 exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_1^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the estimated variance accounted for by the function &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a length scale parameter that governs the decay rate. Smaller estimated values for &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; indicate a faster decay rate in the covariance between two points as a function of time.&lt;/p&gt;
&lt;p&gt;This model, along with its forecasting function are defined in &lt;code&gt;Stan&lt;/code&gt; code below (Adapted from &lt;a href=&#34;http://natelemoine.com/fast-gaussian-process-models-in-stan/&#34;&gt;Nate Lemoine’s code&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real rho1){
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            Sigma[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real r1, 
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, r1);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, r1);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, r1);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; r1;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X,  a1, r1);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ student_t(3,0,1);
    sigma_sq ~ student_t(3,0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, r1, sigma_sq, N1, X, N2, Xp, Y);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model takes just over a minute to run. For those of you who are computational gearheads, here is the hardware I am working with (with a total of 64GB of RAM):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;benchmarkme::get_cpu()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $vendor_id
## [1] &amp;quot;GenuineIntel&amp;quot;
## 
## $model_name
## [1] &amp;quot;Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz&amp;quot;
## 
## $no_of_cores
## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A quick view of summary stats good convergence of estimates across the 6 chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(&amp;#39;a1&amp;#39;,&amp;#39;r1&amp;#39;,&amp;#39;sigma_sq&amp;#39;, &amp;#39;Ypred&amp;#39;)
summary(fit.stan1, pars=pars.to.monitor[-4])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  mean      se_mean           sd        2.5%          25%
## a1        2.064974195 2.117451e-02 1.2312158202 0.739681295  1.272435874
## r1       22.681133287 1.252456e-01 9.0005034579 9.102436133 16.045254783
## sigma_sq  0.003559372 5.819530e-06 0.0004251947 0.002818252  0.003254155
##                   50%          75%        97.5%    n_eff      Rhat
## a1        1.759559393  2.505894675  5.176821352 3380.974 1.0010353
## r1       21.367822936 28.012894270 42.692355288 5164.270 0.9996089
## sigma_sq  0.003524852  0.003823097  0.004468209 5338.262 1.0002208&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and traceplots demonstrate good mixing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan1, pars=c(&amp;#39;a1&amp;#39;, &amp;#39;r1&amp;#39;, &amp;#39;sigma_sq&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the real question is how did the model do?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan1, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not all that great of a prediction to be honest. In this case, the function essentially reduces to a linear regression as there is no place for the periodic nature of the data to be explicitly modeled. This is where the flexibility of Gaussian process models starts to shine as any Gaussian process can be re-expressed as a the sum of an infinite number of Gaussian processes. Here we will add a covariance function that incorporates periodicity. In this case, a period is approximately one year.&lt;/p&gt;
&lt;p&gt;The new periodic covariance function is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[k(t,t&amp;#39;)=\sigma_2^2 exp\Big(-\frac{2sin^2(\pi(t-t&amp;#39;)*1)}{l_2^2}\Big) exp\Big(-\frac{(t-t&amp;#39;)^2}{2l_3^2}\Big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The inclusion of the squared exponential function here simply reduces the ability of the annual features of the data to explain covariation as the interval between two points grows. Here is the &lt;code&gt;Stan&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions{
    //covariance function for main portion of the model
    matrix main_GP(
        int Nx,
        vector x,
        int Ny,
        vector y, 
        real alpha1,
        real alpha2,
        real rho1,
        real rho2,
        real rho3){
                    matrix[Nx, Ny] K1;
                    matrix[Nx, Ny] K2;
                    matrix[Nx, Ny] Sigma;
    
                    //specifying random Gaussian process that governs covariance matrix
                    for(i in 1:Nx){
                        for (j in 1:Ny){
                            K1[i,j] = alpha1*exp(-square(x[i]-y[j])/2/square(rho1));
                        }
                    }
                    
                    //specifying random Gaussian process incorporates heart rate
                    for(i in 1:Nx){
                        for(j in 1:Ny){
                            K2[i, j] = alpha2*exp(-2*square(sin(pi()*fabs(x[i]-y[j])*1))/square(rho2))*
                            exp(-square(x[i]-y[j])/2/square(rho3));
                        }
                    }
                        
                    Sigma = K1+K2;
                    return Sigma;
                }
    //function for posterior calculations
    vector post_pred_rng(
        real a1,
        real a2,
        real r1, 
        real r2,
        real r3,
        real sn,
        int No,
        vector xo,
        int Np, 
        vector xp,
        vector yobs){
                matrix[No,No] Ko;
                matrix[Np,Np] Kp;
                matrix[No,Np] Kop;
                matrix[Np,No] Ko_inv_t;
                vector[Np] mu_p;
                matrix[Np,Np] Tau;
                matrix[Np,Np] L2;
                vector[Np] yp;
    
    //--------------------------------------------------------------------
    //Kernel Multiple GPs for observed data
    Ko = main_GP(No, xo, No, xo, a1, a2, r1, r2, r3);
    for(n in 1:No) Ko[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for predicted data
    Kp = main_GP(Np, xp, Np, xp,  a1, a2, r1, r2,  r3);
    for(n in 1:Np) Kp[n,n] += sn;
        
    //--------------------------------------------------------------------
    //kernel for observed and predicted cross 
    Kop = main_GP(No, xo, Np, xp,  a1, a2, r1, r2, r3);
    
    //--------------------------------------------------------------------
    //Algorithm 2.1 of Rassmussen and Williams... 
    Ko_inv_t = Kop&amp;#39;/Ko;
    mu_p = Ko_inv_t*yobs;
    Tau=Kp-Ko_inv_t*Kop;
    L2 = cholesky_decompose(Tau);
    yp = mu_p + L2*rep_vector(normal_rng(0,1), Np);
    return yp;
    }
}

data { 
    int&amp;lt;lower=1&amp;gt; N1;
    int&amp;lt;lower=1&amp;gt; N2;
    vector[N1] X; 
    vector[N1] Y;
    vector[N2] Xp;
}

transformed data { 
    vector[N1] mu;
    for(n in 1:N1) mu[n] = 0;
}

parameters {
    real&amp;lt;lower=0&amp;gt; a1;
    real&amp;lt;lower=0&amp;gt; a2;
    real&amp;lt;lower=15&amp;gt; r1;      //Set after some preliminary modeling
    real&amp;lt;lower=0&amp;gt; r2;
    real&amp;lt;lower=0&amp;gt; r3;
    real&amp;lt;lower=0&amp;gt; sigma_sq;
}

model{ 
    matrix[N1,N1] Sigma;
    matrix[N1,N1] L_S;
    
    //using GP function from above 
    Sigma = main_GP(N1, X, N1, X, a1, a2, r1, r2, r3);
    for(n in 1:N1) Sigma[n,n] += sigma_sq;
    
    L_S = cholesky_decompose(Sigma);
    Y ~ multi_normal_cholesky(mu, L_S);
    
    //priors for parameters
    a1 ~ normal(2.06,1.23);     //Taken from the first model
    a2 ~ student_t(3,0,1);
    //incorporate minimum and maximum distances - use invgamma
    r1 ~ normal(22.68,9.005);   //Taken from the first model
    r2 ~ student_t(3,0,1);
    r3 ~ student_t(3,0,1);  
    sigma_sq ~ normal(0,1);
}

generated quantities {
    vector[N2] Ypred = post_pred_rng(a1, a2, r1, r2, r3, sigma_sq, N1, X, N2, Xp, Y);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model took about 6 minutes to run.&lt;/p&gt;
&lt;p&gt;Unfortunately, the sampling algorithm that generates the Bayesian estimates is not parallelizable. Until &lt;code&gt;Stan&lt;/code&gt; and &lt;code&gt;rstan&lt;/code&gt; can run using graphics chips’ architecture (which has been buzzed about around the &lt;a href=&#34;http://discourse.mc-stan.org/t/stan-on-the-gpu/326&#34;&gt;Stan ether&lt;/a&gt;), model run time is going to be the biggest downside. Still, 6 minutes is not that long to wait if the model performs well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars.to.monitor&amp;lt;-c(paste0(&amp;#39;a&amp;#39;, 1:2), paste0(&amp;#39;r&amp;#39;, 1:3), &amp;#39;Ypred&amp;#39;)
summary(fit.stan2, pars=pars.to.monitor[-6])$summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           mean      se_mean          sd        2.5%          25%
## a1  2.54983942 0.0134372272  0.92993236  1.03551274  1.842231419
## a2  0.01028192 0.0007652084  0.02499935  0.00195483  0.003809465
## r1 29.72788580 0.0860626598  5.77288905 19.05829543 25.751944510
## r2  0.71435573 0.0024883370  0.12979765  0.49982165  0.627554970
## r3 21.16693790 0.7029880494 10.71983939  2.13531511 14.836950073
##             50%        75%       97.5%     n_eff     Rhat
## a1  2.464693706  3.1470023  4.60109271 4789.4253 1.001067
## a2  0.005887629  0.0100722  0.04229047 1067.3281 1.004514
## r1 29.510979184 33.4589313 41.57756671 4499.4237 1.000575
## r2  0.700328139  0.7850996  1.00378065 2720.9169 1.000556
## r3 19.220095144 25.4484115 45.91201691  232.5309 1.026833&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit.stan2, pars=pars.to.monitor[-6])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ypred&amp;lt;-10^colMeans(extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred)

Ypred.DF&amp;lt;-extract(fit.stan2, pars=&amp;#39;Ypred&amp;#39;)$Ypred
UB&amp;lt;-vector()
LB&amp;lt;-vector()

for(i in 1:N2){
  UB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .975)
  LB[i]&amp;lt;-10^quantile(Ypred.DF[,i], .025)
}

library(ggplot2)
DF.orig&amp;lt;-data.frame(Year=Year, Passengers=Pass)
DF.fore2&amp;lt;-data.frame(Year=year.fore2, Passengers=Ypred, UB=UB, LB=LB)

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5)
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The forecast looks to be in line with what I might expect based on trends leading up to 1962. The results are similar to those obtained using a different forecasting technique (i.e., an ARIMA model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- arima(log10(AirPassengers), c(0, 1, 1),
              seasonal = list(order = c(0, 1, 1), period = 12)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12))
## 
## Coefficients:
##           ma1     sma1
##       -0.4018  -0.5569
## s.e.   0.0896   0.0731
## 
## sigma^2 estimated as 0.0002543:  log likelihood = 353.96,  aic = -701.92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update(fit, method = &amp;quot;CSS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = log10(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
##     1, 1), period = 12), method = &amp;quot;CSS&amp;quot;)
## 
## Coefficients:
##           ma1     sma1
##       -0.3772  -0.5724
## s.e.   0.0883   0.0704
## 
## sigma^2 estimated as 0.0002619:  part log likelihood = 354.32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred &amp;lt;- predict(fit, n.ahead = 24)
tl &amp;lt;- pred$pred - 1.96 * pred$se
tu &amp;lt;- pred$pred + 1.96 * pred$se

ARIMA.for&amp;lt;-data.frame(Year=year.fore2, Passengers=10^pred$pred, UB=10^as.numeric(tu), LB=10^as.numeric(tl))

g1&amp;lt;-ggplot()+
  geom_line(data=DF.orig, aes(x=Year, y=Passengers))+
  geom_ribbon(data=DF.fore2, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;blue&amp;#39;)+
  geom_ribbon(data=ARIMA.for, aes(x=Year, ymin=LB, ymax=UB), alpha=.5, fill=&amp;#39;red&amp;#39;)+
  geom_line(data=DF.fore2, aes(x=Year, y=Passengers, color=&amp;#39;blue&amp;#39;), lwd=1.25)+
  geom_line(data=ARIMA.for, aes(x=Year, y=Passengers, color=&amp;#39;red&amp;#39;), lwd=1.25)+
  coord_cartesian(xlim=c(1960, 1963.25), ylim= c(275,1000))+
  scale_color_manual(name=&amp;#39;Forecast Model&amp;#39;,
                     values = c(&amp;#39;blue&amp;#39;, &amp;#39;red&amp;#39;), 
                     labels = c(&amp;#39;Gaussian Process&amp;#39;, &amp;#39;ARIMA&amp;#39;))
g1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mgb-research.netlify.com/post/2018-05-21-gaussian-process-imputation-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
The two models make fairly similar predictions for 1961 (shaded regions represent respective 95% intervals). In 1962, the ARIMA model is a little more bullish than the Gaussian process model on airline passengers.&lt;/p&gt;
&lt;p&gt;Still, it is impossible to know which of these models is better, a methodological question I may tackle in greater detail when I have some time. The answer is almost certainly “it depends.” For now, the main takeaway is that Gaussian process models may represent a useful approach to the age-old problems of forecasting and imputation, a fact I plan to exploit in some of my signal processing work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dispositional negativity in the wild: Social environment governs momentary emotional experience</title>
      <link>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shackman_etal_2017_emotion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpersonal predictors of stress generation: Is there a super factor?</title>
      <link>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/shih_etal_2017_bjop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neuroticism and conscientiousness as moderators of the relation between social withdrawal and internalizing problems in adolescence</title>
      <link>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/smith_etal_2017_joya/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shyness, preference for solitude and adolescent internalizing: The roles of maternal, paternal, and best-friend support</title>
      <link>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 -0400</pubDate>
      
      <guid>https://mgb-research.netlify.com/publication/barstead_etal_2017_jora/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
