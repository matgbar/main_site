<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Modeling on Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog</title>
    <link>/categories/modeling/</link>
    <description>Recent content in Modeling on Matthew Barstead, Ph.D.: Research, Repository, &amp; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Matthew Barstead</copyright>
    <lastBuildDate>Mon, 03 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/modeling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 2 of 3)</title>
      <link>/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai-part-2-of-3/</guid>
      <description>In the first post in this series, I described the impetus for this trek through statistical modeling, machine learning and artificial intelligence. I also provided an initial set of comparisons for three different approaches to classification: k-means, k-nearest neighbor, and latent profile analysis (model-based clustering). If you want to check those mini-walkthroughs out click here.
As a reminder, my goal here is to compare and contrast different approaches to data analysis and predictive modeling that are, in my mind, arbitrarily lumped into statistical modeling and machine learing/artificial intelligence categories.</description>
    </item>
    
    <item>
      <title>A Rose by Any Other Name: Statistics, Machine Learning, and AI (Part 1 of 3)</title>
      <link>/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-rose-by-any-other-name-statistics-machine-learning-and-ai/</guid>
      <description>I was recently interviewing for a job and a recruiter asked me if I wanted to enhance aspects of my machine learning background on my resume before she passed it on for the next round of reviews. I resisted the urge to chide her in the moment by pointing out the flawed distinction between statistics and machine learning, an unnecessary admonishment that would have been to no one’s benefit. The modal outcome would have been me sounding as if I was speaking with all of the arrogance, jackassery, and superiority that a recently minted Ph.</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data Complete Workflow (Part 2 of 3)</title>
      <link>/post/bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-multilevel-model-with-missing-data-complete-workflow-part-2/</guid>
      <description>Overview:This is the second post in a three-part blog series I am putting together. If you have not read the first post in this series, you may want to go back and check it out. In this post, I will focus on running and evaluating the imputation model itself, having identified the appropriate covariates that help account for missingness in the first post.
Data Brief Description:The data in question come from a study that involved a one-week ecological momentary assessment (EMA) protocol.</description>
    </item>
    
    <item>
      <title>Bayesian Multilevel Model with Missing Data: Complete Work Flow - Part 1 of 3</title>
      <link>/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-multilevel-model-with-missing-data-complete-work-flow/</guid>
      <description>Overview:This is the first post in a three-part blog series I am putting together. The focus of this initial post is effective exploration of the reasons for missingness in a particular set of data. The second post in the series will focus on running and evaluating the imputation model itself after having identified the appropriate covariates that help account for missingness. The third and final post will be a walkthrough of the final models and their interpretation - including a comparison of the same models using listwise deletion (which is bad unless missingness is small or definitely, 100% completely at random).</description>
    </item>
    
    <item>
      <title>Power Analyses for an Unconditional Growth Model using {lmer}</title>
      <link>/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/power-analyses-for-an-unconditional-growth-model-using-lmer/</guid>
      <description>## Warning: package &amp;#39;sjstats&amp;#39; was built under R version 3.5.2Recently, I was asked to knock together a quick power analysis for a linear growth model with approximately 120 subjects. Having already collected data (i.e., having a fixed sample size), the goal of the power analysis was to explore whether a sample of 120 subjects would be sufficient to detect significant linear change (\(\alpha = .05\)) for a secondary research question that was not part of the original proposal (we added collection of a set of variables partway through the data collection period).</description>
    </item>
    
    <item>
      <title>Gaussian Process Imputation/Forecast Models</title>
      <link>/post/gaussian-process-imputation-models/</link>
      <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gaussian-process-imputation-models/</guid>
      <description>## Warning: package &amp;#39;StanHeaders&amp;#39; was built under R version 3.5.2A well-established set of problems emerges when attempting to analyze non-stationary univariate time series (i.e., the signal’s mean and/or variance changes over time). A common approach is to impose some stationarity on the data so that certain modeling techniques can provide allow a research to make some predictions (e.g., ARIMA models). The selection of the appropriate assumptions to make when forcing a time series into stationarity is difficult to automate in many circumstances, requiring that a researcher evaluate competing models.</description>
    </item>
    
  </channel>
</rss>